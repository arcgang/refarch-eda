{"componentChunkName":"component---src-pages-technology-kafka-connect-index-mdx","path":"/technology/kafka-connect/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect","description":"Kafka Connect"},"relativePagePath":"/technology/kafka-connect/index.mdx","titleType":"append","MdxNode":{"id":"142caa9e-3e4f-5b85-b28c-9cc11ee37e15","children":[],"parent":"ee24c705-6de5-53fb-a81a-0bcfe8db608b","internal":{"content":"---\ntitle: Kafka Connect\ndescription: Kafka Connect\n---\n\n[Kafka connect](https://kafka.apache.org/documentation/#connect) is an open source component \nfor easily integrate external systems with Kafka. It works with any Kafka product like IBM Event Streams, Strimzi, AMQ Streams, Confluent. \nIt uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.\n\n![Kafka component](../images/kafka-components.png)\n\nThe general concepts are detailed in the [IBM Event streams product documentation](https://ibm.github.io/event-streams/connecting/connectors/), and [Robin Moffatt's video](https://talks.rmoff.net/DQkDj3). Here is a quick summary:\n\n* **Connector** represents a logical job to move data from / to kafka  to / from external systems. A lot of [existing connectors](https://ibm.github.io/event-streams/connectors/), [Apache Camel Kafka connectors](https://camel.apache.org/camel-kafka-connector/1.0.x/index.html) can be reused, or you can [implement your own](https://kafka.apache.org/documentation/#connect_development).\n* **Workers** are JVMs running the connectors. For production deployment workers run in cluster or \"distributed mode\", and leverage the Kafka consumer group management protocol to scale tasks horizontally.\n* **Tasks**: each worker coordinates a set of tasks to copy data. In distributed mode, task states are saved in Kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.\n* **REST API** to configure the connectors and monitors the tasks.\n\nThe following figure illustrates a classical 'distributed' deployment of a Kafka Connect cluster. \nWorkers are the running processes to execute connectors and tasks.  \nEach Connector is responsible for defining and updating a set of Tasks that actually copy the data. Tasks are threads in a JVM. \nFor fault tolerance and offset management, Kafka Connect uses Kafka topics (suffix name as `-offsets, -config, -status`) to persist its states.\nWhen a connector is first submitted to the cluster, the workers rebalance the full set of connectors\n in the cluster with their tasks so that each worker has approximately the same amount of work. \n\n![Connectors and tasks](./images/connector-tasks.png)\n\n* Connector and tasks are not guaranteed to run on the same instance in the cluster, \nespecially if you have multiple tasks and multiple instances in your kafka connect cluster.\n* The connector may be configured to add `Converters` (code used to translate data between Connect and the system sending or receiving data), \nand `Transforms`: simple logic to alter each message produced by or sent to a connector.\n\nConnector keeps state into three topics, which may be created when the connectors start are:\n\n* **connect-configs**: This topic stores the connector and task configurations.\n* **connect-offsets**: This topic stores offsets for Kafka Connect.\n* **connect-status**: This topic stores status updates of connectors and tasks.\n\n\n## Characteristics\n\n* Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example.\n* Support streaming and batch.\n* Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster.\n* Copy data, externalizing transformation in other framework.\n* Kafka Connect defines three models: data model, worker model and connector model. Worker model allows Kafka Connect to scale the application.\nKafka Connect cluster can serve multiple applications and so may be organized as a service.\n\n## Connector cluster configuration\n\nThe following configurations are important to review:\n\n* `group.id`: one per connect cluster. It is ised by source connectors only.\n* `heartbeat.interval.ms`: The expected time between heartbeats to the group coordinator when using Kafka’s group management facilities.\n\n## Fault tolerance\n\nWhen a worker fails: \n\n![](./images/fault-1.png)\n\nTasks allocated in the failed worker are reallocated to existing workers, and the task's state, read offsets, source record mapping to offset are reloaded from the different topics.\n\n![](./images/fault-2.png)\n\nBoth figure above are illustrating a MongoDB sink connector.\n\n## MQ Source connector\n\nThe [source code is in this repo](https://github.com/ibm-messaging/kafka-connect-mq-source) and uses JMS as protocol to integrate with MQ.\nWhen the connector encounters a message that it cannot process, it stops rather than throwing the message away. \nThe MQ source connector does not currently make much use of message keys. It is possible to use CorrelationID as a key\nby defining MQ source `mq.record.builder.key.header` property:\n\n```\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.converters.ByteArrayConverter\n    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\n    mq.connection.mode: client\n    mq.message.body.jms: true\n    mq.record.builder.key.header: JMSCorrelationID\n```\n\nThe record builder helps to transform the input message to a kafka record, using or not a schema.\n\nAlways keep the coherence between body.jms, record builder and data converter. \n\nThe MQ source task starts a unique JMS Reader that will read n messages from the queue. The `poll() function \nreturns the list of MQ source records, and will commit to JMS if the number of message read match the batch size or\nif there is no more records. Once the Kafka Producer gets acknowledge that records are received by Brokers then use callback on the source\ntask to commit MQ transaction for example. \n\nAny producer configuration can be modified in the source connector configuration:\n\n```yaml\nproducer.override.acks: 1\n```\n\n## Installation\n\nThe  Kafka connect framework fits well into a kubernetes deployment. \nIn 2021 we have different options for that deployment: the [Strimzi Kafka connect operator](https://strimzi.io/docs/0.22.0/#kafka-connect-str), \nIBM [Event Streams Connector](https://ibm.github.io/event-streams/connecting/setting-up-connectors/),\nRed Hat [AMQ Streams (2021.Q3) connector](https://access.redhat.com/documentation/en-us/red_hat_amq/2021.q3/html/using_amq_streams_on_openshift/assembly-deployment-configuration-str#assembly-kafka-connect-str)\nor one of the [Confluent connector](https://www.confluent.io/hub/).\n\n### IBM Event Streams Cloud Pak for Integration\n\nIf you are using IBM Event Streams 2021.x on Cloud Pak for Integration, the connectors setup is part of the user admin console toolbox:\n\n![Event Streams connector](../images/es-connectors.png)\n\n*Deploying connectors against an IBM Event Streams cluster, you need to have a Kafka user with Manager role, to be able to create topic, produce and consume messages for all topics.*\n\nAs an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker \nimage which needs to be updated with the connector jars and redeployed to kubernetes cluster \nor to other environment. With IBM Event Streams on Openshift, the toolbox includes a \nkafka connect environment packaging, that defines a Dockerfile and configuration files \nto build your own image with the connectors jar files you need. The configuration files \ndefines the properties to connect to Event Streams kafka brokers using API keys and SASL.\n\nThe following [public IBM messaging github account](https://github.com/ibm-messaging) includes \nsupported, open sourced, connectors (search for `connector`).\n\nHere is the [list of supported connectors](https://ibm.github.io/event-streams/connectors/) for IBM Event Streams.\n\nEvent Stream Kafka connector use custom resource definition defined by Strimzi. So configuration for Strimzi works for\nEvent Streams.\n\nNormally you define one Kafka connect cluster, with a custom docker image which has all the necessary\njars file for any connector you want to use. Then you configure each connector so they can start\nprocessing events or producing events.\nA Kafka connect cluster is identified with a group.id and then it saves its states in topics. The example\nbelow are for the configuration in cluster, also named distributed.\n\n```yaml\n  config:\n    group.id: connect-cluster\n    offset.storage.topic: connect-cluster-offsets\n    config.storage.topic: connect-cluster-configs\n    status.storage.topic: connect-cluster-status\n```\n\nThe [real-time inventory gitops repository](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops) includes a MQ Source connector to push message to Kafka. It uses\nArgoCD to maintain states of Kafka Cluster, topics, users, and Kafka connector.\n\nOnce the connector pods are running we need to start the connector tasks. \n\n### Strimzi\n\n[KafkaConnector resources](https://strimzi.io/docs/operators/latest/using.html#proc-kafka-connect-config-str) allow you to create and manage connector instances for Kafka Connect in a Kubernetes-native way.\nTo manage connectors, you can use the Kafka Connect REST API, or use KafkaConnector custom resources.\nIn case of GitOps methodology we will define connector cluster and connector instance as yamls.\nConnector configuration is passed to Kafka Connect as part of an HTTP request and stored within Kafka itself.\n\n## Further Readings\n\n* [Apache Kafka connect documentation](https://kafka.apache.org/documentation/#connect)\n* [Confluent Connector Documentation](https://docs.confluent.io/current/connect/index.html)\n* [IBM Event Streams Connectors](https://ibm.github.io/event-streams/connecting/connectors/) \n* [List of supported connectors by Event Streams](https://ibm.github.io/event-streams/connectors/)\n* [MongoDB Connector for Apache Kafka](https://github.com/mongodb/mongo-kafka)\n","type":"Mdx","contentDigest":"78bcf7bf5f6e5d83e24a7ff6dbbcb0c3","owner":"gatsby-plugin-mdx","counter":949},"frontmatter":{"title":"Kafka Connect","description":"Kafka Connect"},"exports":{},"rawBody":"---\ntitle: Kafka Connect\ndescription: Kafka Connect\n---\n\n[Kafka connect](https://kafka.apache.org/documentation/#connect) is an open source component \nfor easily integrate external systems with Kafka. It works with any Kafka product like IBM Event Streams, Strimzi, AMQ Streams, Confluent. \nIt uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.\n\n![Kafka component](../images/kafka-components.png)\n\nThe general concepts are detailed in the [IBM Event streams product documentation](https://ibm.github.io/event-streams/connecting/connectors/), and [Robin Moffatt's video](https://talks.rmoff.net/DQkDj3). Here is a quick summary:\n\n* **Connector** represents a logical job to move data from / to kafka  to / from external systems. A lot of [existing connectors](https://ibm.github.io/event-streams/connectors/), [Apache Camel Kafka connectors](https://camel.apache.org/camel-kafka-connector/1.0.x/index.html) can be reused, or you can [implement your own](https://kafka.apache.org/documentation/#connect_development).\n* **Workers** are JVMs running the connectors. For production deployment workers run in cluster or \"distributed mode\", and leverage the Kafka consumer group management protocol to scale tasks horizontally.\n* **Tasks**: each worker coordinates a set of tasks to copy data. In distributed mode, task states are saved in Kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.\n* **REST API** to configure the connectors and monitors the tasks.\n\nThe following figure illustrates a classical 'distributed' deployment of a Kafka Connect cluster. \nWorkers are the running processes to execute connectors and tasks.  \nEach Connector is responsible for defining and updating a set of Tasks that actually copy the data. Tasks are threads in a JVM. \nFor fault tolerance and offset management, Kafka Connect uses Kafka topics (suffix name as `-offsets, -config, -status`) to persist its states.\nWhen a connector is first submitted to the cluster, the workers rebalance the full set of connectors\n in the cluster with their tasks so that each worker has approximately the same amount of work. \n\n![Connectors and tasks](./images/connector-tasks.png)\n\n* Connector and tasks are not guaranteed to run on the same instance in the cluster, \nespecially if you have multiple tasks and multiple instances in your kafka connect cluster.\n* The connector may be configured to add `Converters` (code used to translate data between Connect and the system sending or receiving data), \nand `Transforms`: simple logic to alter each message produced by or sent to a connector.\n\nConnector keeps state into three topics, which may be created when the connectors start are:\n\n* **connect-configs**: This topic stores the connector and task configurations.\n* **connect-offsets**: This topic stores offsets for Kafka Connect.\n* **connect-status**: This topic stores status updates of connectors and tasks.\n\n\n## Characteristics\n\n* Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example.\n* Support streaming and batch.\n* Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster.\n* Copy data, externalizing transformation in other framework.\n* Kafka Connect defines three models: data model, worker model and connector model. Worker model allows Kafka Connect to scale the application.\nKafka Connect cluster can serve multiple applications and so may be organized as a service.\n\n## Connector cluster configuration\n\nThe following configurations are important to review:\n\n* `group.id`: one per connect cluster. It is ised by source connectors only.\n* `heartbeat.interval.ms`: The expected time between heartbeats to the group coordinator when using Kafka’s group management facilities.\n\n## Fault tolerance\n\nWhen a worker fails: \n\n![](./images/fault-1.png)\n\nTasks allocated in the failed worker are reallocated to existing workers, and the task's state, read offsets, source record mapping to offset are reloaded from the different topics.\n\n![](./images/fault-2.png)\n\nBoth figure above are illustrating a MongoDB sink connector.\n\n## MQ Source connector\n\nThe [source code is in this repo](https://github.com/ibm-messaging/kafka-connect-mq-source) and uses JMS as protocol to integrate with MQ.\nWhen the connector encounters a message that it cannot process, it stops rather than throwing the message away. \nThe MQ source connector does not currently make much use of message keys. It is possible to use CorrelationID as a key\nby defining MQ source `mq.record.builder.key.header` property:\n\n```\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.converters.ByteArrayConverter\n    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\n    mq.connection.mode: client\n    mq.message.body.jms: true\n    mq.record.builder.key.header: JMSCorrelationID\n```\n\nThe record builder helps to transform the input message to a kafka record, using or not a schema.\n\nAlways keep the coherence between body.jms, record builder and data converter. \n\nThe MQ source task starts a unique JMS Reader that will read n messages from the queue. The `poll() function \nreturns the list of MQ source records, and will commit to JMS if the number of message read match the batch size or\nif there is no more records. Once the Kafka Producer gets acknowledge that records are received by Brokers then use callback on the source\ntask to commit MQ transaction for example. \n\nAny producer configuration can be modified in the source connector configuration:\n\n```yaml\nproducer.override.acks: 1\n```\n\n## Installation\n\nThe  Kafka connect framework fits well into a kubernetes deployment. \nIn 2021 we have different options for that deployment: the [Strimzi Kafka connect operator](https://strimzi.io/docs/0.22.0/#kafka-connect-str), \nIBM [Event Streams Connector](https://ibm.github.io/event-streams/connecting/setting-up-connectors/),\nRed Hat [AMQ Streams (2021.Q3) connector](https://access.redhat.com/documentation/en-us/red_hat_amq/2021.q3/html/using_amq_streams_on_openshift/assembly-deployment-configuration-str#assembly-kafka-connect-str)\nor one of the [Confluent connector](https://www.confluent.io/hub/).\n\n### IBM Event Streams Cloud Pak for Integration\n\nIf you are using IBM Event Streams 2021.x on Cloud Pak for Integration, the connectors setup is part of the user admin console toolbox:\n\n![Event Streams connector](../images/es-connectors.png)\n\n*Deploying connectors against an IBM Event Streams cluster, you need to have a Kafka user with Manager role, to be able to create topic, produce and consume messages for all topics.*\n\nAs an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker \nimage which needs to be updated with the connector jars and redeployed to kubernetes cluster \nor to other environment. With IBM Event Streams on Openshift, the toolbox includes a \nkafka connect environment packaging, that defines a Dockerfile and configuration files \nto build your own image with the connectors jar files you need. The configuration files \ndefines the properties to connect to Event Streams kafka brokers using API keys and SASL.\n\nThe following [public IBM messaging github account](https://github.com/ibm-messaging) includes \nsupported, open sourced, connectors (search for `connector`).\n\nHere is the [list of supported connectors](https://ibm.github.io/event-streams/connectors/) for IBM Event Streams.\n\nEvent Stream Kafka connector use custom resource definition defined by Strimzi. So configuration for Strimzi works for\nEvent Streams.\n\nNormally you define one Kafka connect cluster, with a custom docker image which has all the necessary\njars file for any connector you want to use. Then you configure each connector so they can start\nprocessing events or producing events.\nA Kafka connect cluster is identified with a group.id and then it saves its states in topics. The example\nbelow are for the configuration in cluster, also named distributed.\n\n```yaml\n  config:\n    group.id: connect-cluster\n    offset.storage.topic: connect-cluster-offsets\n    config.storage.topic: connect-cluster-configs\n    status.storage.topic: connect-cluster-status\n```\n\nThe [real-time inventory gitops repository](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops) includes a MQ Source connector to push message to Kafka. It uses\nArgoCD to maintain states of Kafka Cluster, topics, users, and Kafka connector.\n\nOnce the connector pods are running we need to start the connector tasks. \n\n### Strimzi\n\n[KafkaConnector resources](https://strimzi.io/docs/operators/latest/using.html#proc-kafka-connect-config-str) allow you to create and manage connector instances for Kafka Connect in a Kubernetes-native way.\nTo manage connectors, you can use the Kafka Connect REST API, or use KafkaConnector custom resources.\nIn case of GitOps methodology we will define connector cluster and connector instance as yamls.\nConnector configuration is passed to Kafka Connect as part of an HTTP request and stored within Kafka itself.\n\n## Further Readings\n\n* [Apache Kafka connect documentation](https://kafka.apache.org/documentation/#connect)\n* [Confluent Connector Documentation](https://docs.confluent.io/current/connect/index.html)\n* [IBM Event Streams Connectors](https://ibm.github.io/event-streams/connecting/connectors/) \n* [List of supported connectors by Event Streams](https://ibm.github.io/event-streams/connectors/)\n* [MongoDB Connector for Apache Kafka](https://github.com/mongodb/mongo-kafka)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/kafka-connect/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}