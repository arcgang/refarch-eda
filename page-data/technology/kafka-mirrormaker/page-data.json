{"componentChunkName":"component---src-pages-technology-kafka-mirrormaker-index-mdx","path":"/technology/kafka-mirrormaker/","result":{"pageContext":{"frontmatter":{"title":"Kafka Mirror Maker 2","description":"Kafka Mirror Maker 2"},"relativePagePath":"/technology/kafka-mirrormaker/index.mdx","titleType":"append","MdxNode":{"id":"6a0a26a8-1ae5-556d-b1f1-194d5080a1e2","children":[],"parent":"5daff97d-73d5-559c-8cb8-e95f0e85aa65","internal":{"content":"---\ntitle: Kafka Mirror Maker 2\ndescription: Kafka Mirror Maker 2\n---\n\nThis section introduces **Mirror Maker 2.0**, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. \nMirror Maker 2.0 was defined as part of the Kafka Improvement Process - [KIP 382](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0) and can be used\nfor disaster recovery (active / passive) or for more complex topology with 3 data centers to support always on.\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Mirror Maker 2 components</AnchorLink>\n  <AnchorLink>Deployment examples</AnchorLink>\n  <AnchorLink>Replication considerations</AnchorLink>\n  <AnchorLink> MM2 topology</AnchorLink>\n  <AnchorLink>Capacity planning</AnchorLink>\n  <AnchorLink>Version migration</AnchorLink>\n  <AnchorLink>Resources</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nWe recommend to start by reading the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/georeplication/about/) on geo-replication to understand the main concepts.\nSome core principles we use in this article:\n\n* We have two data centers in different region and each region has multiple availability zones\n* OpenShift clusters are defined in both region and spread among the three data center. For a better view of the golden topology for OpenShift [see this diagram](https://github.com/ibm-cloud-architecture/eda-tech-academy/blob/main/docs/demo/images/es-golden-topo.png)  with master, worker nodes deployment.\n* At least two Event Streams / Kafka clusters are defined, one as `source` in the active region and one `target` for disaster recovery or passive region.\n* Source cluster has producer and consumer applications deployed in the same OpenShift Cluster or deployed on VMs and accessing the Kafka brokers via network load balancer.\n* Producer, consumer or streaming applications deployed within OpenShift use the `bootstrap URL` to kafka broker via internal service definition. Something like `es-prod-kafka-bootstrap.ibm-eventstreams.svc`.\nWith such configuration their setting will be the same on the target cluster.\n* The target cluster has the mirror maker cluster which is based on the Kafka connect framework.\n\nThe following diagram illustrates those principles:\n\n![](./images/principles.png)\n\nWhen zooming into what need to be replicated, we can see source topics from the blue cluster to target topics on the green cluster. \nThis configuration is for disaster recovery, with a active - passive model, where only the left side has active applications producing and consuming records from Kafka Topics.\n\n ![1](./images/mm2-dr.png)\n\nAs the mirroring is over longer internet distance, then expect some latency in the data mirroring.\n\nWe can extend this deployment by using Mirror Maker 2 to replicate data over multiple clusters with a more active - active deployment which the following diagram illustrates the concepts for an \"always-on\" deployment:\n\n ![2](./images/mm2-multi-cluster.png)\n\nThis model can also being used between cloud providers.\n\nIn active - active mode the clusters get data injected in local cluster and replicated data injected from remote cluster. \nThe topic names are prefixed with the original cluster name. In the figure below, the cluster on the right has green local producers and consumers, \ntopics are replicated to the left, the blue cluster. Same for blue topic from the left to the right.\n\n ![3](./images/mm2-act-act.png)\n\nConsumers on both sides are getting data from the 'order' topics (local and replicated) to get a complete view of all the orders created on both sides. \n\nThe following diagram zooms into a classical Web based solution design where mobile or web apps are going to a web tier to serve single page application, static content, and APIs.\n\n![](./images/classic-n-tier.png)\n\nThen a set of microservices implement the business logic, some of those services are event-driven, so they produce and consumer events from topics. When\nactive-active replication is in place it, means the same topology is deployed in another data center and data from the same topic (business entity) arrive\nto the replicated topic. The service can save the record in its own database and cache. (The service Tier is not detailed with the expected replicas, neither the application load balancer displays routes to other data center)\n\nIf there is a failure on one of the side of the data replication, the data are transparently available. A read model query will return the good result on both side.\n\nIn replication, data in topic, topic states and metadata are replicated.\n\nIBM Event Streams release 10.0 is supporting Mirror Maker 2 as part of the [geo-replication feature](https://ibm.github.io/event-streams/georeplication/about/).\n\n## Mirror Maker 2 components\n\n[Mirror maker 2.0](https://strimzi.io/docs/operators/in-development/configuring.html#assembly-deployment-configuration-kafka-mirror-maker-str) is the solution to replicate data in topics from one Kafka cluster to another. It uses the [Kafka Connect](../kafka-connect/) framework to simplify configuration, parallel execution and horizontal scaling.\n\nThe figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.\n\n![Kafka Connect](../images/mm-k-connect.png)\n\nMirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. It runs in standalone mode, which can be used for development and test purpose, or in distributed mode (cluster) for production deployment. With distribution mode, MirrorMaker 2.0 creates the following topics on the cluster it is connected to (See later the property `connectCluster`):\n\n* **...-configs.source.internal**: This topic is used to store the connector and task configuration.\n* **...-offsets.source.internal**: This topic is used to store offsets for Kafka Connect.\n* **...-status.source.internal**: This topic is used to store status updates of connectors and tasks.\n* **source.heartbeats**: to check that the remote cluster is available and the clusters are connected\n* **source.checkpoints.internal*: MirrorCheckpointConnector tracks and maps offsets for specified consumer groups using an offset sync topic and checkpoint topic.\n\nA typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. \nHere is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and \nSASL authentication protocol.  The IBM Event Streams instance runs on the Cloud.\n\n```properties\nclusters=source, target\nsource.bootstrap.servers=${KAFKA_SOURCE_BROKERS}\ntarget.bootstrap.servers=${KAFKA_TARGET_BROKERS}\ntarget.security.protocol=SASL_SSL\ntarget.ssl.protocol=TLSv1.2\ntarget.ssl.endpoint.identification.algorithm=https\ntarget.sasl.mechanism=PLAIN\ntarget.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY};\n# enable and configure individual replication flows\nsource->target.enabled=true\nsource->target.topics=products\ntasks.max=10\n```\n\n* Topics to be replicated are configured via a _whitelist_ that may include regular expression for pattern matching on the topic name. So if you use naming convention for your topic, you could do fine grained selection of the replicated topic. \nIt is possible to specify topics you do not want to replicate via the _blacklist_ property.\n* White listed topics are set with the `source->target.topics` attribute of the replication flow and uses [Java regular expression](https://www.vogella.com/tutorials/JavaRegularExpressions/article.html) syntax.\n* The default blacklisted topics are Kafka internal topic:\n\n```properties\nblacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas]\n```\n\nWe can also define the _blacklist_ with the properties: `topics.blacklist`. Comma-separated lists and Java Regular Expressions are supported.\n\nInternally, `MirrorSourceConnector` and `MirrorCheckpointConnector` will create multiple Kafka tasks (up to the value of `tasks.max` property), and `MirrorHeartbeatConnector` creates an additional task. `MirrorSourceConnector` will have one task per topic-partition combination to replicate, while `MirrorCheckpointConnector` will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the `assign()` API, so there is no consumer group used while fetching data from source topics. There is no call to `commit()` either; rebalancing occurs only when there is a new topic created that matches the _whitelist_ pattern.\n\nMirror Maker 2 can run on VM, bare metal or within containers deployed on kubernetes cluster.\n\n### Why replicating?\n\nThe classical needs for replication between clusters can be listed as:\n\n* Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The [following article](https://cloud.ibm.com/docs/EventStreams?topic=EventStreams-disaster_recovery_scenario) goes over those principals.\n* Active-active cluster mirroring for inter services communication: consumers and producers are on both sides and consume or produce to their local cluster.\n* Moving data to a read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view.\n* GDPR compliance to isolate data in country and geography\n* Hybrid cloud operations to share data between on-premise cluster and managed service clusters.\n\n## Deployment examples\n\nWe encourage you to go over our [Mirror maker 2 labs](/use-cases/kafka-mm2/) which addresses different replication scenarios. The `Connect` column defines where the Mirror Maker 2 runs.\n\n\n| Scenario | Source                 | Target                 | Connect | Lab |\n|-------------|------------------------|------------------------|:-------:|:---:|\n| Getting Started | | | | | [In tech academy](https://ibm-cloud-architecture.github.io/eda-tech-academy/getting-started/mm2/) | \n| Lab 1  | Event Streams on Cloud  | Local Kafka | Local on localhost   | [Kafka Mirror Maker 2 - Lab 1](/use-cases/kafka-mm2/lab-1/)|\n| Lab 2 | Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud | On OCP | OCP | [Kafka Mirror Maker 2 - Lab 2](/use-cases/kafka-mm2/lab-2/) |\n\n\n## Replication considerations\n\n### Topic metadata replication\n\nIt is possible to disable the topic metadata replication. We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created by the stream topology semantic, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare.\nBy synchronizing configuration properties, the need for rebalancing is reduced.\n\nWhen doing manual configuration, even if the initial topic configuration is duplicated, any dynamic changes to the topic properties are not going \nto be automatically propagated and the administrator needs to change the target topic. If the throughput on \nthe source topic has increased and the number of partition was increased to support the load, then the target cluster will not have the same \ndownstream capability which may lead to overloading (disk space or memory capacity). With a GitOps approach, this risk is mitigated as topic definition in\nthe GitOps repository could be propagated to the target and source cluster mostly at the same time. \n\nAlso if the consumer of a partition is expecting to process the events in order within the partition, then changing the number of partitions \nbetween source and target will make the ordering not valid any more.\n\nIf the replication factor are set differently between the two clusters then the availability guaranty of the replicated data may be impacted \nand bad settings with broker failure will lead to data lost.\n\nFinally, it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates\n a backlog in the pipeline and increases the end to end latency observed by the downstream application.\n\n### Naming convention\n\nMirror maker 2 sets the prefix for the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid \ninfinite loop when doing bi-directional mirroring. At the consumer side the `subscribe()` function supports regular expression for topic name. So a code like:\n\n```java\nkafkaConsumer.subscribe(\"^.*accounts\")\n```\n\nwill listen to all the topics in the cluster having cluster name prefixed topics and the local `accounts` topic. \nThis could be useful when we want to aggregate data from different data centers / clusters.\n\n### Offset management\n\nMirror maker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters \nand the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its records, it gets the offsets \nin the partition the records were created.\n\nIn the diagram below we have a source topic A/partition 1 with the last write offset done by a producer to be  5, and the last read committed offset by the consumer assigned to partition 1 being 3. \nThe last replicated offset 3 is mapped as downstream offset 12 in the target partition. Offset numbers do not match between replicated partitions.\nSo if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment. \nThis information is saved in the `checkpoint` topic.\n\n![6](./images/mm2-offset-mgt.png)\n\nOffset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. \nFor example, the normal behavior is to increase the offset by one 2,3,4,5,6,7, which is mapped to 12,13,14,15,16,... on target cluster. \nIf the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the `offset-synch` topic.\n\nThe `checkpoint` and `offset_synch` topics enable replication to be fully restored from the correct offset position on failover. \nOn the following diagram, once the cluster source is down, the consumers on the target cluster are restarted, and they will start from the last committed \noffset of the source, which was offset 3 that is in fact offset 12 on target replicated topic. No record skipped.\n\n![7](./images/mm2-offset-mgt-2.png)\n\n### Record duplication\n\nExactly-once delivery is difficult to achieve in distributed system. In the case of Kafka, producer, brokers, and consumers are working together to ensure \nonly one message is processed end to end. With coding practice and configuration settings, within a unique cluster, Kafka can guarantee exactly once processing. \nNo duplicated records between producer and broker, and committed reads, on consumer side, are not reprocessed in case of consumer restarts.\n\nBut for cross cluster replications, the semantic is based on at least once approach. Duplicates can happen when the mirror maker source task stops \nbefore committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates. \nThe following diagram illustrate this case, record offset 26 on target topic is a duplicate of record 25.\n\n![8](./images/mm2-dup.png)\n\nAlso Mirror Maker 2 is a generic topic consumer, it will not participate to the \"read-committed\" process, if the topic includes duplicate messages \nit will propagate them to the target.\n\nIn the future MM2 will be able to support exactly once by using the `checkpoint` topic on the target cluster to keep the state of the committed offset \nfrom the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part \nof the same transaction.\n\n## MM2 topology\n\nIn this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi-tenancy. The simplest approach is \nto use one Mirror Maker instance per family of topics: the classification of family of topic can be anything, from line of business, to team, to application. \nSuppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application. \nThe configuration will define the groupId to match the application name for example.\n\nThe following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three Mirror Maker 2 \ninstances mirroring the different topics with name starting with `topic-name-A*, topic-name-B*, topic-name-C*,` respectively.\n\n![9](./images/mm2-topology.png)\n\nEach connect instance is a JVM workers that replicate the topic/partitions and has different group.id.\n\nFor Bi-directional replication for the same topic name, Mirror Maker 2 will use the cluster name as prefix. With MM2, we do not need to have 2 MM2 clusters \nbut only one and bidirectional definitions. The following example is showing the configuration for a MM2 bidirectional settings, with `accounts` topic to be \nreplicated on both cluster:\n\n```\napiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaMirrorMaker2\n...\n mirrors:\n  - sourceCluster: \"event-streams-wdc\"\n    targetCluster: \"kafka-on-premise\"\n    ...\n    topicsPattern: \"accounts,orders\"\n  - sourceCluster: \"kafka-on-premise\"\n    targetCluster: \"event-streams-wdc\"\n    ...\n    topicsPattern: \"accounts,customers\"\n```\n\n\n### Consumer coding\n\nWe recommend to review the [producer implementation best practices](/technology/kafka-producers/) and the [consumer considerations](/technology/kafka-consumers).\n\n## Capacity planning\n\n\nFor platform sizing, the main metric to assess, is the number of partitions to replicate. The number of partitions and number of brokers are somehow connected as \ngetting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster \nand each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size.\n\nTo address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. \nWe can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. \nThe parameters `max.tasks` specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated. \nEach task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controller. \nWith Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller. \nSo the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing.\n\nThe task processing is stateless, consume - produce wait for acknowledge,  commit offset. In this case, the CPU and network performance are key. \nFor platform tuning activity, we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start \nto scale horizontally by adding more Mirror Maker 2 instances. If the network at the server level is the bottleneck, then adding more servers will help. \nKafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughput as with small \nmessage the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation.\n\nThe parameters to consider for sizing are the following:\n\n| Parameter | Description |Impact|\n| --- | --- | --- |\n| Number of topic/ partition | Each task processes one partition | For pure parallel processing max.tasks is set around the number of CPU |\n| Record size | Size of the message in each partition in average | Memory usage and Throughput: the # of records/s decrease when size increase, while MB/s throughput increases in logarithmic|\n| Expected input throughput | The producer writing to the source topic throughput | Be sure the consumers inside MM2 absorb the demand |\n| Network latency | | This is where positioning MM2 close to the target cluster may help improve latency|\n\n## Version migration\n\nOnce the Mirror Maker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves.  If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers.\nIf a new worker starts work, a rebalance ensures it takes over some work from the existing workers.\n\nUsing the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the  'consumer' task to partition assignment.\n\nWe have presented a similar approach in [this section](/technology/kafka-mirrormaker/vm-provisioning), where we tested that each instance of MirrorMaker 2 could assume the replication.  First we will stop the Node 1 instance, upgrade it to the latest version, then start it again.  Then we’ll repeat the same procedure on Node 2.  We’ll continue to watch the Consumer VM window to note that replication should not stop at any point.\n\n![10](./images/mm2-v2v-1.png)\n\nWe’ve now upgraded to Kafka 2.5 including the latest MirrorMaker 2.  Meanwhile, replication was uninterrupted due to the second instance of MirrorMaker 2:\n\n![11](./images/mm2-v2v-2.png)\n\nNow we’ll restart the Node 1 instance of MirrorMaker 2, stop the Node 2 instance, we can still see replication occurring on the upgraded Node 1 instance of MirrorMaker 2.\n\n![12](./images/mm2-v2v-3.png)\n\nWe upgrade Node 2’s instance of MirrorMaker 2 exactly as on Node 1, and start it again, and once again, replication is still going.\n\n\n![13](./images/mm2-v2v-4.png)\n\nWhen using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough.\n\nBe sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version.\n\n## Resources\n\n* [IBM Event Streams product documentation](https://ibm.github.io/event-streams/georeplication/about/)\n* [IBM Event Streams managed service - Disaster recovery example scenario](https://cloud.ibm.com/docs/EventStreams?topic=EventStreams-disaster_recovery_scenario)\n* [Strimzi configuration for Mirror Maker 2](https://strimzi.io/docs/operators/latest/configuring.html#assembly-deployment-configuration-kafka-mirror-maker-str)\n* [Getting started with Mirror Maker 2 - Tech Academy](https://ibm-cloud-architecture.github.io/eda-tech-academy/getting-started/mm2/)\n* [Using MirrorMaker2 from Dale Lane](https://dalelane.co.uk/blog/?p=4074)\n","type":"Mdx","contentDigest":"c5c6de30239a15a35ef03e8403d444bc","owner":"gatsby-plugin-mdx","counter":966},"frontmatter":{"title":"Kafka Mirror Maker 2","description":"Kafka Mirror Maker 2"},"exports":{},"rawBody":"---\ntitle: Kafka Mirror Maker 2\ndescription: Kafka Mirror Maker 2\n---\n\nThis section introduces **Mirror Maker 2.0**, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. \nMirror Maker 2.0 was defined as part of the Kafka Improvement Process - [KIP 382](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0) and can be used\nfor disaster recovery (active / passive) or for more complex topology with 3 data centers to support always on.\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Mirror Maker 2 components</AnchorLink>\n  <AnchorLink>Deployment examples</AnchorLink>\n  <AnchorLink>Replication considerations</AnchorLink>\n  <AnchorLink> MM2 topology</AnchorLink>\n  <AnchorLink>Capacity planning</AnchorLink>\n  <AnchorLink>Version migration</AnchorLink>\n  <AnchorLink>Resources</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nWe recommend to start by reading the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/georeplication/about/) on geo-replication to understand the main concepts.\nSome core principles we use in this article:\n\n* We have two data centers in different region and each region has multiple availability zones\n* OpenShift clusters are defined in both region and spread among the three data center. For a better view of the golden topology for OpenShift [see this diagram](https://github.com/ibm-cloud-architecture/eda-tech-academy/blob/main/docs/demo/images/es-golden-topo.png)  with master, worker nodes deployment.\n* At least two Event Streams / Kafka clusters are defined, one as `source` in the active region and one `target` for disaster recovery or passive region.\n* Source cluster has producer and consumer applications deployed in the same OpenShift Cluster or deployed on VMs and accessing the Kafka brokers via network load balancer.\n* Producer, consumer or streaming applications deployed within OpenShift use the `bootstrap URL` to kafka broker via internal service definition. Something like `es-prod-kafka-bootstrap.ibm-eventstreams.svc`.\nWith such configuration their setting will be the same on the target cluster.\n* The target cluster has the mirror maker cluster which is based on the Kafka connect framework.\n\nThe following diagram illustrates those principles:\n\n![](./images/principles.png)\n\nWhen zooming into what need to be replicated, we can see source topics from the blue cluster to target topics on the green cluster. \nThis configuration is for disaster recovery, with a active - passive model, where only the left side has active applications producing and consuming records from Kafka Topics.\n\n ![1](./images/mm2-dr.png)\n\nAs the mirroring is over longer internet distance, then expect some latency in the data mirroring.\n\nWe can extend this deployment by using Mirror Maker 2 to replicate data over multiple clusters with a more active - active deployment which the following diagram illustrates the concepts for an \"always-on\" deployment:\n\n ![2](./images/mm2-multi-cluster.png)\n\nThis model can also being used between cloud providers.\n\nIn active - active mode the clusters get data injected in local cluster and replicated data injected from remote cluster. \nThe topic names are prefixed with the original cluster name. In the figure below, the cluster on the right has green local producers and consumers, \ntopics are replicated to the left, the blue cluster. Same for blue topic from the left to the right.\n\n ![3](./images/mm2-act-act.png)\n\nConsumers on both sides are getting data from the 'order' topics (local and replicated) to get a complete view of all the orders created on both sides. \n\nThe following diagram zooms into a classical Web based solution design where mobile or web apps are going to a web tier to serve single page application, static content, and APIs.\n\n![](./images/classic-n-tier.png)\n\nThen a set of microservices implement the business logic, some of those services are event-driven, so they produce and consumer events from topics. When\nactive-active replication is in place it, means the same topology is deployed in another data center and data from the same topic (business entity) arrive\nto the replicated topic. The service can save the record in its own database and cache. (The service Tier is not detailed with the expected replicas, neither the application load balancer displays routes to other data center)\n\nIf there is a failure on one of the side of the data replication, the data are transparently available. A read model query will return the good result on both side.\n\nIn replication, data in topic, topic states and metadata are replicated.\n\nIBM Event Streams release 10.0 is supporting Mirror Maker 2 as part of the [geo-replication feature](https://ibm.github.io/event-streams/georeplication/about/).\n\n## Mirror Maker 2 components\n\n[Mirror maker 2.0](https://strimzi.io/docs/operators/in-development/configuring.html#assembly-deployment-configuration-kafka-mirror-maker-str) is the solution to replicate data in topics from one Kafka cluster to another. It uses the [Kafka Connect](../kafka-connect/) framework to simplify configuration, parallel execution and horizontal scaling.\n\nThe figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.\n\n![Kafka Connect](../images/mm-k-connect.png)\n\nMirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. It runs in standalone mode, which can be used for development and test purpose, or in distributed mode (cluster) for production deployment. With distribution mode, MirrorMaker 2.0 creates the following topics on the cluster it is connected to (See later the property `connectCluster`):\n\n* **...-configs.source.internal**: This topic is used to store the connector and task configuration.\n* **...-offsets.source.internal**: This topic is used to store offsets for Kafka Connect.\n* **...-status.source.internal**: This topic is used to store status updates of connectors and tasks.\n* **source.heartbeats**: to check that the remote cluster is available and the clusters are connected\n* **source.checkpoints.internal*: MirrorCheckpointConnector tracks and maps offsets for specified consumer groups using an offset sync topic and checkpoint topic.\n\nA typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. \nHere is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and \nSASL authentication protocol.  The IBM Event Streams instance runs on the Cloud.\n\n```properties\nclusters=source, target\nsource.bootstrap.servers=${KAFKA_SOURCE_BROKERS}\ntarget.bootstrap.servers=${KAFKA_TARGET_BROKERS}\ntarget.security.protocol=SASL_SSL\ntarget.ssl.protocol=TLSv1.2\ntarget.ssl.endpoint.identification.algorithm=https\ntarget.sasl.mechanism=PLAIN\ntarget.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY};\n# enable and configure individual replication flows\nsource->target.enabled=true\nsource->target.topics=products\ntasks.max=10\n```\n\n* Topics to be replicated are configured via a _whitelist_ that may include regular expression for pattern matching on the topic name. So if you use naming convention for your topic, you could do fine grained selection of the replicated topic. \nIt is possible to specify topics you do not want to replicate via the _blacklist_ property.\n* White listed topics are set with the `source->target.topics` attribute of the replication flow and uses [Java regular expression](https://www.vogella.com/tutorials/JavaRegularExpressions/article.html) syntax.\n* The default blacklisted topics are Kafka internal topic:\n\n```properties\nblacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas]\n```\n\nWe can also define the _blacklist_ with the properties: `topics.blacklist`. Comma-separated lists and Java Regular Expressions are supported.\n\nInternally, `MirrorSourceConnector` and `MirrorCheckpointConnector` will create multiple Kafka tasks (up to the value of `tasks.max` property), and `MirrorHeartbeatConnector` creates an additional task. `MirrorSourceConnector` will have one task per topic-partition combination to replicate, while `MirrorCheckpointConnector` will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the `assign()` API, so there is no consumer group used while fetching data from source topics. There is no call to `commit()` either; rebalancing occurs only when there is a new topic created that matches the _whitelist_ pattern.\n\nMirror Maker 2 can run on VM, bare metal or within containers deployed on kubernetes cluster.\n\n### Why replicating?\n\nThe classical needs for replication between clusters can be listed as:\n\n* Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The [following article](https://cloud.ibm.com/docs/EventStreams?topic=EventStreams-disaster_recovery_scenario) goes over those principals.\n* Active-active cluster mirroring for inter services communication: consumers and producers are on both sides and consume or produce to their local cluster.\n* Moving data to a read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view.\n* GDPR compliance to isolate data in country and geography\n* Hybrid cloud operations to share data between on-premise cluster and managed service clusters.\n\n## Deployment examples\n\nWe encourage you to go over our [Mirror maker 2 labs](/use-cases/kafka-mm2/) which addresses different replication scenarios. The `Connect` column defines where the Mirror Maker 2 runs.\n\n\n| Scenario | Source                 | Target                 | Connect | Lab |\n|-------------|------------------------|------------------------|:-------:|:---:|\n| Getting Started | | | | | [In tech academy](https://ibm-cloud-architecture.github.io/eda-tech-academy/getting-started/mm2/) | \n| Lab 1  | Event Streams on Cloud  | Local Kafka | Local on localhost   | [Kafka Mirror Maker 2 - Lab 1](/use-cases/kafka-mm2/lab-1/)|\n| Lab 2 | Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud | On OCP | OCP | [Kafka Mirror Maker 2 - Lab 2](/use-cases/kafka-mm2/lab-2/) |\n\n\n## Replication considerations\n\n### Topic metadata replication\n\nIt is possible to disable the topic metadata replication. We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created by the stream topology semantic, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare.\nBy synchronizing configuration properties, the need for rebalancing is reduced.\n\nWhen doing manual configuration, even if the initial topic configuration is duplicated, any dynamic changes to the topic properties are not going \nto be automatically propagated and the administrator needs to change the target topic. If the throughput on \nthe source topic has increased and the number of partition was increased to support the load, then the target cluster will not have the same \ndownstream capability which may lead to overloading (disk space or memory capacity). With a GitOps approach, this risk is mitigated as topic definition in\nthe GitOps repository could be propagated to the target and source cluster mostly at the same time. \n\nAlso if the consumer of a partition is expecting to process the events in order within the partition, then changing the number of partitions \nbetween source and target will make the ordering not valid any more.\n\nIf the replication factor are set differently between the two clusters then the availability guaranty of the replicated data may be impacted \nand bad settings with broker failure will lead to data lost.\n\nFinally, it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates\n a backlog in the pipeline and increases the end to end latency observed by the downstream application.\n\n### Naming convention\n\nMirror maker 2 sets the prefix for the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid \ninfinite loop when doing bi-directional mirroring. At the consumer side the `subscribe()` function supports regular expression for topic name. So a code like:\n\n```java\nkafkaConsumer.subscribe(\"^.*accounts\")\n```\n\nwill listen to all the topics in the cluster having cluster name prefixed topics and the local `accounts` topic. \nThis could be useful when we want to aggregate data from different data centers / clusters.\n\n### Offset management\n\nMirror maker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters \nand the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its records, it gets the offsets \nin the partition the records were created.\n\nIn the diagram below we have a source topic A/partition 1 with the last write offset done by a producer to be  5, and the last read committed offset by the consumer assigned to partition 1 being 3. \nThe last replicated offset 3 is mapped as downstream offset 12 in the target partition. Offset numbers do not match between replicated partitions.\nSo if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment. \nThis information is saved in the `checkpoint` topic.\n\n![6](./images/mm2-offset-mgt.png)\n\nOffset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. \nFor example, the normal behavior is to increase the offset by one 2,3,4,5,6,7, which is mapped to 12,13,14,15,16,... on target cluster. \nIf the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the `offset-synch` topic.\n\nThe `checkpoint` and `offset_synch` topics enable replication to be fully restored from the correct offset position on failover. \nOn the following diagram, once the cluster source is down, the consumers on the target cluster are restarted, and they will start from the last committed \noffset of the source, which was offset 3 that is in fact offset 12 on target replicated topic. No record skipped.\n\n![7](./images/mm2-offset-mgt-2.png)\n\n### Record duplication\n\nExactly-once delivery is difficult to achieve in distributed system. In the case of Kafka, producer, brokers, and consumers are working together to ensure \nonly one message is processed end to end. With coding practice and configuration settings, within a unique cluster, Kafka can guarantee exactly once processing. \nNo duplicated records between producer and broker, and committed reads, on consumer side, are not reprocessed in case of consumer restarts.\n\nBut for cross cluster replications, the semantic is based on at least once approach. Duplicates can happen when the mirror maker source task stops \nbefore committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates. \nThe following diagram illustrate this case, record offset 26 on target topic is a duplicate of record 25.\n\n![8](./images/mm2-dup.png)\n\nAlso Mirror Maker 2 is a generic topic consumer, it will not participate to the \"read-committed\" process, if the topic includes duplicate messages \nit will propagate them to the target.\n\nIn the future MM2 will be able to support exactly once by using the `checkpoint` topic on the target cluster to keep the state of the committed offset \nfrom the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part \nof the same transaction.\n\n## MM2 topology\n\nIn this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi-tenancy. The simplest approach is \nto use one Mirror Maker instance per family of topics: the classification of family of topic can be anything, from line of business, to team, to application. \nSuppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application. \nThe configuration will define the groupId to match the application name for example.\n\nThe following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three Mirror Maker 2 \ninstances mirroring the different topics with name starting with `topic-name-A*, topic-name-B*, topic-name-C*,` respectively.\n\n![9](./images/mm2-topology.png)\n\nEach connect instance is a JVM workers that replicate the topic/partitions and has different group.id.\n\nFor Bi-directional replication for the same topic name, Mirror Maker 2 will use the cluster name as prefix. With MM2, we do not need to have 2 MM2 clusters \nbut only one and bidirectional definitions. The following example is showing the configuration for a MM2 bidirectional settings, with `accounts` topic to be \nreplicated on both cluster:\n\n```\napiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaMirrorMaker2\n...\n mirrors:\n  - sourceCluster: \"event-streams-wdc\"\n    targetCluster: \"kafka-on-premise\"\n    ...\n    topicsPattern: \"accounts,orders\"\n  - sourceCluster: \"kafka-on-premise\"\n    targetCluster: \"event-streams-wdc\"\n    ...\n    topicsPattern: \"accounts,customers\"\n```\n\n\n### Consumer coding\n\nWe recommend to review the [producer implementation best practices](/technology/kafka-producers/) and the [consumer considerations](/technology/kafka-consumers).\n\n## Capacity planning\n\n\nFor platform sizing, the main metric to assess, is the number of partitions to replicate. The number of partitions and number of brokers are somehow connected as \ngetting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster \nand each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size.\n\nTo address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. \nWe can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. \nThe parameters `max.tasks` specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated. \nEach task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controller. \nWith Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller. \nSo the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing.\n\nThe task processing is stateless, consume - produce wait for acknowledge,  commit offset. In this case, the CPU and network performance are key. \nFor platform tuning activity, we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start \nto scale horizontally by adding more Mirror Maker 2 instances. If the network at the server level is the bottleneck, then adding more servers will help. \nKafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughput as with small \nmessage the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation.\n\nThe parameters to consider for sizing are the following:\n\n| Parameter | Description |Impact|\n| --- | --- | --- |\n| Number of topic/ partition | Each task processes one partition | For pure parallel processing max.tasks is set around the number of CPU |\n| Record size | Size of the message in each partition in average | Memory usage and Throughput: the # of records/s decrease when size increase, while MB/s throughput increases in logarithmic|\n| Expected input throughput | The producer writing to the source topic throughput | Be sure the consumers inside MM2 absorb the demand |\n| Network latency | | This is where positioning MM2 close to the target cluster may help improve latency|\n\n## Version migration\n\nOnce the Mirror Maker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves.  If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers.\nIf a new worker starts work, a rebalance ensures it takes over some work from the existing workers.\n\nUsing the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the  'consumer' task to partition assignment.\n\nWe have presented a similar approach in [this section](/technology/kafka-mirrormaker/vm-provisioning), where we tested that each instance of MirrorMaker 2 could assume the replication.  First we will stop the Node 1 instance, upgrade it to the latest version, then start it again.  Then we’ll repeat the same procedure on Node 2.  We’ll continue to watch the Consumer VM window to note that replication should not stop at any point.\n\n![10](./images/mm2-v2v-1.png)\n\nWe’ve now upgraded to Kafka 2.5 including the latest MirrorMaker 2.  Meanwhile, replication was uninterrupted due to the second instance of MirrorMaker 2:\n\n![11](./images/mm2-v2v-2.png)\n\nNow we’ll restart the Node 1 instance of MirrorMaker 2, stop the Node 2 instance, we can still see replication occurring on the upgraded Node 1 instance of MirrorMaker 2.\n\n![12](./images/mm2-v2v-3.png)\n\nWe upgrade Node 2’s instance of MirrorMaker 2 exactly as on Node 1, and start it again, and once again, replication is still going.\n\n\n![13](./images/mm2-v2v-4.png)\n\nWhen using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough.\n\nBe sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version.\n\n## Resources\n\n* [IBM Event Streams product documentation](https://ibm.github.io/event-streams/georeplication/about/)\n* [IBM Event Streams managed service - Disaster recovery example scenario](https://cloud.ibm.com/docs/EventStreams?topic=EventStreams-disaster_recovery_scenario)\n* [Strimzi configuration for Mirror Maker 2](https://strimzi.io/docs/operators/latest/configuring.html#assembly-deployment-configuration-kafka-mirror-maker-str)\n* [Getting started with Mirror Maker 2 - Tech Academy](https://ibm-cloud-architecture.github.io/eda-tech-academy/getting-started/mm2/)\n* [Using MirrorMaker2 from Dale Lane](https://dalelane.co.uk/blog/?p=4074)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/kafka-mirrormaker/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}