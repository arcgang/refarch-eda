{"componentChunkName":"component---src-pages-technology-event-streams-es-cp-4-i-index-mdx","path":"/technology/event-streams/es-cp4i/","result":{"pageContext":{"frontmatter":{"title":"Event Streams within Cloud Pak for Integration","description":"Hands on lab to deploy Event Streams on OpenShift"},"relativePagePath":"/technology/event-streams/es-cp4i/index.mdx","titleType":"append","MdxNode":{"id":"20bedb2f-0722-52ad-b2f8-2a3cc4dc0f38","children":[],"parent":"c7b7dcbf-e0d7-5efb-b812-ac3c1d876cbb","internal":{"content":"---\ntitle: Event Streams within Cloud Pak for Integration\ndescription: Hands on lab to deploy Event Streams on OpenShift\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 10/03/2022 </strong>\n</InlineNotification>\n\nIn this tutorial you will learn how to install Event Streams on OpenShift, using a the Administration Console, or using CLI.\nWe propose two installation tutorials: \n\n* one using the OpenShift Admin console, then the Event Streams console to create Kafka topics and use the Starter Application,\nto validate the installation.\n* one using `oc CLI` from your terminal or from Cloud Shell Terminal.\n\n<AnchorLinks>\n  <AnchorLink>Demonstrate Event Streams from A to Z</AnchorLink>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>Install Event Streams Using OpenShift Console</AnchorLink>\n  <AnchorLink>Run starter application</AnchorLink>\n  <AnchorLink>Install Event Streams Using CLIs</AnchorLink>\n  <AnchorLink>Common operations to perform on cluster</AnchorLink>\n</AnchorLinks>\n\n\n\nThe Kafka Cluster configuration is for a development or staging environment with no persistence.\n\nOnce the installation is successful, the final pods running in the `eventstreams` project are:\n\n ```sh\ndev-entity-operator-69c6b7cf87-bfxbw   2/2     Running   0          43s\ndev-kafka-0                            1/1     Running   0          2m\ndev-kafka-1                            1/1     Running   0          2m\ndev-kafka-2                            1/1     Running   0          2m\ndev-zookeeper-0                        1/1     Running   0          2m46s\ndev-zookeeper-1                        1/1     Running   0          3h23m\ndev-zookeeper-2                        1/1     Running   0          3h23m\n ```\n\n**Updated October 07/2021 - Operator Release 2.4 - Product Release 10.4 - Kafka 2.8**\n\nWe recommend to read the following [\"structuring your deployment\" chapter from product documentation](https://www.ibm.com/docs/en/cloud-paks/cp-integration/2021.3?topic=installation-structuring-your-deployment)\nto give you more insight of the things to consider for deployment. \n\nIn this tutorial we select to deploy Event Streams in one namespace, and the Operator to monitor multiple namespaces.\n\n## Prerequisites\n\n* Get access to an OpenShift Cluster.\n* Get `oc ` CLI from OpenShift Admin Console\n\n### Install Cloudctl CLIs \n\n* Install IBM Cloud Pak CLI: Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). \nThis CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak. In order to install it, execute the following commands in your IBM Cloud Shell:\n\n  * Download the IBM Cloud Pak CLI (example below is for Mac but  other downloads are [available here](https://github.com/IBM/cloud-pak-cli/releases/tag/v3.12.0)): \n\n  ```sh\n  curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-darwin-amd64.tar.gz -o cloudctl.tar.gz\n  ```\n  * Untar it\n\n  ```sh\n  tar -xvf cloudctl.tar.gz\n  ```\n\n  * Rename it as cloudctl and move to a folder within your PATH:\n\n  ```sh\n  mv cloudctl-darwin-amd64 cloudctl\n  ```\n\n  * Make sure your IBM Cloud Pak CLI is in the path: `which cloudctl`\n  * Make sure your IBM Cloud Pak CLI works: `cloudctl help`\n\n  ![shell2](./images/shell2-v10.png)\n\n\nSee also the product documentation for [prerequisites details](https://ibm.github.io/event-streams/installing/prerequisites/)\n\n### Install IBM catalog\n\nUse the EDA Gitops project with the different operator subscriptions and IBM catalog to define\nthe IBM Operator catalog.\n\n```sh\n# List existing catalog\noc get catalogsource -n openshift-marketplace\n# Verifi ibm-operator-catalog is present in the list\n# if not add IBM catalog \noc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-gitops-catalog/main/ibm-catalog/catalog_source.yaml\n```\n\n### Get IBM Software entitlement key and create a secret\n\n* Obtain [IBM license entitlement key](https://github.com/IBM/cloudpak-gitops/blob/main/docs/install.md#obtain-an-entitlement-key).\n* Verify you can access the IBM Image repository with\n\n    ```sh\n    docker login cp.icr.io --username cp --password <key-copied>\n    ```\n\n* Create a OpenShift secret in the `openshift-operator` project \n\n    ```sh\n    # Get on good project\n    oc project openshift-operators\n    # Verify if secret exists\n    oc describe secret ibm-entitlement-key\n    # Create if needed\n    oc create secret docker-registry ibm-entitlement-key \\\n        --docker-username=cp \\\n        --docker-server=cp.icr.io \\\n        --namespace=eventstreams \\\n        --docker-password=your_entitlement_key \n    ```\n\n> Attention\n  This secret needs to be present in any namespace where an Event Streams cluster will be deployed.\n\n### Install IBM foundational services\n\nWhen using security with IAM and other services, Event Streams needs IBM foundations services. It will install it\nautomatically if you install the operator at the cluster level. \nIn case you need to control the deployment and if *Cloud Pak for Integration* is not installed yet, you need to install the IBM foundational services operator\nfor that, follow these [simple instructions](https://www.ibm.com/docs/en/cpfs?topic=311-installing-foundational-services-by-using-console).\n\nCreate the same ibm-entitlement-key in the `ibm-common-services`  namespace.\n\n```sh\n# verify operator installed\noc get operators -n ibm-common-services\n# -> response:\nibm-common-service-operator.openshift-operators    \nibm-namespace-scope-operator.ibm-common-services   \nibm-odlm.ibm-common-services                       \n```\n\nThose are the deployments the foundational services are creating:\n\n```\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\nibm-common-service-webhook             1/1     1            1           68m\nibm-namespace-scope-operator           1/1     1            1           69m\noperand-deployment-lifecycle-manager   1/1     1            1           68m\nsecretshare                            1/1     1            1           68m\n```\n\nRecalls that operands are what operators manage. \n\n---\n\n## Install Event Streams Using OpenShift Console\n\n* As Cluster Administrator, create a project using Home > Projects, and create button, and enter `eventstreams` as project name.\n\n    ![](./images/create-project.png)\n\nThe goal is to create a shareable Kafka cluster. \n\n* As Administrator, use `Openshift console -> Operators > OperatorHub` to search for `Event Streams` operator, then install the operator by selecting\nthe `All namespaces on the cluster`, the version v2.4. The Operator will monitor all namespaces.\n\n  ![](./images/es-operator-cfg.png)\n\n  The installation of this operator will also include pre-requisites operators like `Cloud Pak foundational services (3.11)`\n  \n* Once the Operator is ready, go to the `Installed Operator` under the `eventstreams` project\n\n ![](./images/es-operator-home.png)\n\n* Create an Event Streams cluster instance using the operator user interface:\n\n ![](./images/create-es-instance.png)\n\n  Enter the minimum information for a development cluster, like name, license,.. \n\n  ![](./images/es-top-form.png)\n\n  Or go to the `Yaml view` and select one of the proposed Sample:\n  \n  ![](./images/es-yaml-view.png)\n  \n  The `Development` configuration should be enough to get you started. For production\n  deployment [see this article](/technology/event-streams/es-cp4i/es-production/).\n  \n  Do not forget to set `spec.license.accept` to true.\n\n  Before creating the instance, we recommend you read [the security summary](/technology/security/) \n  so you can relate the authentication configured by default with what application\n  needs to use to connect to the Kafka cluster. For example the following declaration stipulates\n  to use TLS authentication for internal communication, and SCRAM for external connection.\n\n    ```yaml\n        listeners:\n          plain:\n            port: 9092\n            type: internal\n            tls: false\n          external:\n            authentication:\n              type: scram-sha-512\n            type: route\n          tls:\n            authentication:\n              type: tls\n    ```\n\nThe first deployment of Event Streams, there may be creation of new deployments into the `ibm-common-services` if\nthose services were not present before. Here is the updated list of `ibm-common-services` deployments:\n\n  ```sh\n  NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\n  auth-idp                               1/1     1            1           27m\n  auth-pap                               1/1     1            1           32m\n  auth-pdp                               1/1     1            1           32m\n  cert-manager-cainjector                1/1     1            1           31m\n  cert-manager-controller                1/1     1            1           31m\n  cert-manager-webhook                   1/1     1            1           30m\n  common-web-ui                          1/1     1            1           20m\n  configmap-watcher                      1/1     1            1           31m\n  default-http-backend                   1/1     1            1           31m\n  iam-policy-controller                  1/1     1            1           32m\n  ibm-cert-manager-operator              1/1     1            1           32m\n  ibm-common-service-webhook             1/1     1            1           3h12m\n  ibm-commonui-operator                  1/1     1            1           32m\n  ibm-iam-operator                       1/1     1            1           33m\n  ibm-ingress-nginx-operator             1/1     1            1           32m\n  ibm-management-ingress-operator        1/1     1            1           33m\n  ibm-mongodb-operator                   1/1     1            1           32m\n  ibm-monitoring-grafana                 1/1     1            1           32m\n  ibm-monitoring-grafana-operator        1/1     1            1           33m\n  ibm-namespace-scope-operator           1/1     1            1           3h12m\n  ibm-platform-api-operator              1/1     1            1           31m\n  management-ingress                     1/1     1            1           23m\n  nginx-ingress-controller               1/1     1            1           31m\n  oidcclient-watcher                     1/1     1            1           32m\n  operand-deployment-lifecycle-manager   1/1     1            1           3h11m\n  platform-api                           1/1     1            1           31m\n  secret-watcher                         1/1     1            1           32m\n  secretshare                            1/1     1            1           3h12m\n  ```\n\nIt could take few minutes to get these pods up and running.\n\nThe result of this cluster creation should looks like: \n\n![](./images/es-created.png)\n\nwith the list of pod as illustrated at the top of this article.\n\nSee also [Cloud Pak for integration documentation](https://www.ibm.com/docs/en/cloud-paks/cp-integration/2021.3?topic=installing-overview-installation) for other\ndeployment considerations.\n\n### Adding users and teams \n\nYou need to be a platform administrator to create users and teams in IAM.\n\nTo get the admin user credential, use the following command:\n\n```sh\noc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' -n ibm-common-services | base64 && echo \"\"\n```\n\n### Log into Event Streams\n\nYou can get the User Interface end point by doing the following command.\n\n```sh\noc get route dev-ibm-es-ui -o jsonpath='{.spec.host}'\n```\n\n> Change `dev` prefix with the name of your Kafka cluster you defined earlier.\n\nOr using the Admin Console \n\n* Select the Event Streams Operator in the project where you install it\n* Select Click on the IBM Event Streams Operator and then on the `Event Streams` option listed at the top bar\n1. Click on the IBM Event Streams cluster instance you want to access to its console and see the `Admin UI` attribute that displays the route to this IBM Event Streams instance's console.\n  ![log](./images/admin-ui-url.png)\n1. Click on the route link and enter your IBM Event Streams credentials.\n\nHere is the Console home page:\n\n![](./images/es-console.png)\n\n\n### Create Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift. \nThe example is to define a topic named INBOUND with 1 partition and a replica set to 3.\n\n1. Log into your IBM Event Streams instance through the UI as explained in the previous section.\n1. Click on the Topics option on the navigation bar on the left. \n1. In the topics page, click on the `Create topic` blue button on the top right corner\n\n  ![Create Topic](./images/create-topic.png)\n\n1. Provide a name for your topic.\n\n  ![Topic Name](./images/inbound-topic-name.png)\n\n1. Leave Partitions at 1.\n\n  ![Partition](./images/partitions.png)\n\n1. Depending on how long you want messages to persist you can change this.\n\n  ![Message Retention](./images/message-retention.png)\n\n1. You can leave Replication Factor at the default 3.\n\n  ![Replication](./images/replicas.png)\n\n1. Click Create.\n1. Make sure the topic has been created by navigating to the topics section on the IBM Event Streams user inteface you can find an option for in the left hand side menu bar. \n\n## Run starter application\n\n[See separate note](/technology/event-streams/starter-app)\n\n--- \n\n## Install Event Streams Using CLIs\n\nThis section is an alternate of using OpenShift Console. We are using our GitOps catalog repository which\ndefines the different operators and scripts we can use to install Event Streams and other services\nvia scripts. All can be automatized with ArgoCD / OpenShift GitOps.\n\n* Create a project to host Event Streams cluster:\n\n  ```shell\n  oc new-project eventstreams\n  ```\n* Clone our eda-gitops-catalog project\n\n  ```sh\n  git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git\n  ```\n* Create the `ibm-entitlement-key` secret in this project.\n\n  ```sh\n  oc create secret docker-registry ibm-entitlement-key \\\n        --docker-username=cp \\\n        --docker-server=cp.icr.io \\\n        --namespace=eventstreams \\\n        --docker-password=your_entitlement_key \n  ```\n\n* Install Event Streams Operator subscriptions\n\n  ```sh\n  oc apply -k cp4i-operators/event-streams/operator/overlays/v2.4/\n  ```\n\n* Install one Event Streams instance: Instances of Event Streams can be created after the Event Streams operator is installed. \nYou can use te OpenShift console or our predefined cluster definition:\n\n  ```shell\n  oc apply -k cp4i-operators/event-streams/instances/dev/\n  ```\n\n  If you want to do the same thing for a production cluster\n\n  ```shell\n  oc apply -k cp4i-operators/event-streams/instances/prod-small/\n  ```\n---\n\n## Common operations to perform on cluster\n\nWe list here a set of common operations to perform on top of Event Streams Cluster.\n\n### Download ES CLI plugin\n\nFrom Event Streams Console, go to the \"Find more in the Toolbox\" tile, and then select\n`IBM Event Streams command-line interface`, then select your target operating system:\n\n![](./images/es-cli-download.png)\n\nInitialize the Event Streams CLI plugin (make sure you provide the namespace where your IBM Event Streams instance is installed on as the command will fail if you dont have cluster wide admin permissions)\n\n  ```shell\n  $ cloudctl es init -n eventstreams\n                                                  \n  IBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox.gse-ocp.net   \n  Namespace:                                     integration   \n  Name:                                          kafka   \n  IBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox.gse-ocp.net   \n  Event Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox.gse-ocp.net   \n  Event Streams API status:                      OK   \n  Event Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox.gse-ocp.net   \n  Apicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox.gse-ocp.net   \n  Event Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox.gse-ocp.net:443   \n  OK\n  ``` \n\n### Access to Event Streams Console using CLI\n\nIn order to log into IBM Event Streams console through the CLI, \nwe are going to use the `oc` OpenShift CLI, the `cloudctl` Cloud Pak CLI and \nthe `es` Cloud Pak CLI plugin. \n\nWe assume you are already logged into your OpenShift cluster.\n\n1. Get your Cloud Pak Console route (you may need cluster wide admin permissions to do so as the Cloud Pak Console is usually installed in the `ibm-common-services` namespace by the cluster admins)\n\n  ```shell\n  $ oc get routes -n ibm-common-services | grep console\n  cp-console                       cp-console.apps.eda-sandbox.gse-ocp.net                                                  icp-management-ingress           https      reencrypt/Redirect     None\n  ```\n\n1. Log into IBM Event Streams using the Cloud Pak console route from the previous step:\n\n  ```shell\n  $ cloudctl login -a https://cp-console.apps.eda-sandbox.gse-ocp.net --skip-ssl-validation\n\n  Username> user50\n\n  Password>\n  Authenticating...\n  OK\n\n  Targeted account mycluster Account\n\n  Enter a namespace > integration\n  Targeted namespace integration\n\n  Configuring kubectl ...\n  Property \"clusters.mycluster\" unset.\n  Property \"users.mycluster-user\" unset.\n  Property \"contexts.mycluster-context\" unset.\n  Cluster \"mycluster\" set.\n  User \"mycluster-user\" set.\n  Context \"mycluster-context\" created.\n  Switched to context \"mycluster-context\".\n  OK\n\n  Configuring helm: /Users/user/.helm\n  OK\n  ```\n\n### Create Topic with es CLI\n\n1. Create the topic with the desi specification.\n\n   ```shell\n   cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n   ```\n\n1. Make sure the topic has been created by listing the topics.\n\n  ```shell\n  $ cloudctl es topics\n  Topic name   \n  INBOUND   \n  OK\n  ```\n\n### Get Kafka Bootstrap Url\n\nFor an application to access the Kafka Broker, we need to get the bootstrap URL.\n\n#### UI\n\nYou can find your IBM Event Streams Kakfa bootstrap url if you log into the IBM Event\n Streams user interface, and click on the `Connect to this cluster` option displayed\n  on the dashboard. This will display a menu where you will see a url to the left \n  of the `Generate SCAM credentials` button. Make sure that you are on \n  the **External Connection**.\n\n![Connect to this Cluster bootstrap](./images/esv10-connect-to-cluster-bootstrap.png)\n\n#### CLI\n\nYou can find your IBM Event Streams and Kakfa bootstrap url when you init the \nIBM Event Streams Cloud Pak CLI plugin on the last line:\n\n```shell\n$ cloudctl es init -n eventstreams\n                                                  \nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox-delta.gse-ocp.net   \nNamespace:                                     integration   \nName:                                          kafka   \nIBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API status:                      OK   \nEvent Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net   \nApicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443   \nOK\n``` \n  \n### Generate SCRAM Service Credentials\n\nFor an application to connect to an Event Streams instance through the secured \nexternal listener, it needs SCRAM credentials to act as service credentials\nto authenticate the application. We also need TLS certificate to encrypt the\ncommunication with the brokers.\n\n#### UI\n\n1. Log into the IBM Event Streams user interface as explained previously in this readme and click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu. Make sure that you are on the **External Connection**.\n\n  ![Connect to this Cluster](./images/esv10-connect-to-cluster.png)\n\n1. Click on the `Generate SCRAM credentials` button.\n\n  ![Generate SCRAM1 ](./images/esv10-generate-scram.png)\n\n1. Introduce a name for your credentials and choose the option that better suits the needs of your applications (this will create RBAC permissions for you credentials so that a service credentials can do only what it needs to do). For this demo, select `Produce messages, consume messages and create topics and schemas` last option.\n\n  ![Generate SCRAM 2](./images/esv10-scram-1.png)\n\n1. Decide whether your service credentials need to have the ability to access all topics or certain topics only. For this demo, select `All Topics` and then click Next.\n\n  ![Generate SCRAM 3](./images/esv10-scram-2.png)\n\n1. Decide whether your service credentials need to have the ability to access all consumer groups or certain specific consumer groups only. For this demo, select `All Consumer Groups` and click Next.\n\n  ![Generate SCRAM 4](./images/esv10-scram-3.png)\n\n1. Decide whether your service credentials need to have the ability to access all transactional IDs or certain specific transactional IDs only. For this demo, select `All transaction IDs` and click on `Generate credentials`.\n\n  ![Generate SCRAM 5](./images/esv10-scram-4.png)\n\n1. **Take note** of the set of credentials displayed on screen. \nYou will need to provide your applications with these in order to get authenticated \nand authorized with your IBM Event Streams instance.\n\n  ![Generate SCRAM 6](./images/esv10-scram-5.png)\n\n1. If you did not take note of your SCRAM credentials or you forgot these, the above will create a `KafkaUser` object in OpenShift that is interpreted by the IBM Event Streams Operator. You can see this `KafkaUser` if you go to the OpenShift console, click on `Operators --> Installed Operators` on the right hand side menu, then click on the `IBM Event Streams` operator and finally click on `Kafka Users` at the top bar menu.\n\n  ![Retrieve SCRAM 1](./images/esv10-retrieve-scram-1.png)\n\n1. If you click on your `Kafka User`, you will see what is the Kubernetes secret behind holding your SCRAM credentials details.\n\n  ![Retrieve SCRAM 2](./images/esv10-retrieve-scram-2.png)\n\n1. Click on that secret and you will be able to see again your `SCRAM password` (your `SCRAM username` is the same name as the `Kafka User` created or the secret holding your `SCRAM password`) \n\n  ![Retrieve SCRAM 3](./images/esv10-retrieve-scram-3.png)\n\n\n#### CLI\n\n1. Log into your IBM Event Streams instance through the CLI.\n\n1. Create your SCRAM service credentials with the following command (adjust the topics, consumer groups, transaction IDs, etc permissions your SCRAM service credentials should have in order to satisfy your application requirements):\n\n  ```shell\n  $ cloudctl es kafka-user-create --name test-credentials-cli --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n  \n  KafkaUser name         Authentication   Authorization   Username                                                Secret   \n  test-credentials-cli   scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret   \n  \n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  \n  Created KafkaUser test-credentials-cli.\n  OK\n  ```\n\n1. List the `KafkaUser` objects to make sure yours has been created:\n\n  ```shell\n  $ cloudctl es kafka-users\n  KafkaUser name                    Authentication   Authorization   \n  test-credentials                  scram-sha-512    simple   \n  test-credentials-cli              scram-sha-512    simple   \n  OK\n  ```\n\n1. To retrieve your credentials execute the following command:\n\n  ```shell\n  $ cloudctl es kafka-user test-credentials-cli \n  KafkaUser name         Authentication   Authorization   Username               Secret   \n  test-credentials-cli   scram-sha-512    simple          test-credentials-cli   test-credentials-cli   \n\n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  OK\n  ```\n\n1. Above you can see your `SCRAM username` under `Username` and the secret holding your `SCRAM password` under `Secret`. In order to retrieve the password, execute the following command:\n\n  ```shell\n  $ oc get secret test-credentials-cli -o jsonpath='{.data.password}' | base64 --decode \n  \n  *******\n  ```\n\n**NEXT:** For more information about how to connect to your cluste, read the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/getting-started/connecting/)\n\n### Get Event Streams TLS Certificates\n\nIn this section we are going to see how to download the TLS certificats to securely connect to our IBM Event Streams instance.\n\n#### UI\n\n1. Log into the IBM Event Streams console user interface as explained before in this readme.\n\n1. Click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu where you will see a `Certificates` section:\n\n  ![PKCS12 Truststore](./images/esv10-pkcs12.png)\n\n1. Depending on what language your application is written into, you will need a `PKCS12 certificate` or a `PEM certificate`. Click on `Download certificate` for any of the options you need. If it is the `PKCS12 certificate` bear in mind it comes with a password for the truststore. You don't need to write this down as it will display any time you click on `Download certificate` button.\n\n#### CLI\n\n1. Log into IBM Event Streams through the CLI as already explained before in this readme.\n\n1. To retrieve the `PKCS12 certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format p12\n  Trustore password is ********\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.p12.\n  OK\n  ```\n\n1. To retrieve the `PEM certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format pem\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.pem.\n  OK\n  ```\n\n## Demonstrate Event Streams from A to Z\n\n[See separate note](/technology/event-streams/demo-a-z/)","type":"Mdx","contentDigest":"5d5c7e17e8bf1e82edc4a726773ce276","owner":"gatsby-plugin-mdx","counter":978},"frontmatter":{"title":"Event Streams within Cloud Pak for Integration","description":"Hands on lab to deploy Event Streams on OpenShift"},"exports":{},"rawBody":"---\ntitle: Event Streams within Cloud Pak for Integration\ndescription: Hands on lab to deploy Event Streams on OpenShift\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 10/03/2022 </strong>\n</InlineNotification>\n\nIn this tutorial you will learn how to install Event Streams on OpenShift, using a the Administration Console, or using CLI.\nWe propose two installation tutorials: \n\n* one using the OpenShift Admin console, then the Event Streams console to create Kafka topics and use the Starter Application,\nto validate the installation.\n* one using `oc CLI` from your terminal or from Cloud Shell Terminal.\n\n<AnchorLinks>\n  <AnchorLink>Demonstrate Event Streams from A to Z</AnchorLink>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>Install Event Streams Using OpenShift Console</AnchorLink>\n  <AnchorLink>Run starter application</AnchorLink>\n  <AnchorLink>Install Event Streams Using CLIs</AnchorLink>\n  <AnchorLink>Common operations to perform on cluster</AnchorLink>\n</AnchorLinks>\n\n\n\nThe Kafka Cluster configuration is for a development or staging environment with no persistence.\n\nOnce the installation is successful, the final pods running in the `eventstreams` project are:\n\n ```sh\ndev-entity-operator-69c6b7cf87-bfxbw   2/2     Running   0          43s\ndev-kafka-0                            1/1     Running   0          2m\ndev-kafka-1                            1/1     Running   0          2m\ndev-kafka-2                            1/1     Running   0          2m\ndev-zookeeper-0                        1/1     Running   0          2m46s\ndev-zookeeper-1                        1/1     Running   0          3h23m\ndev-zookeeper-2                        1/1     Running   0          3h23m\n ```\n\n**Updated October 07/2021 - Operator Release 2.4 - Product Release 10.4 - Kafka 2.8**\n\nWe recommend to read the following [\"structuring your deployment\" chapter from product documentation](https://www.ibm.com/docs/en/cloud-paks/cp-integration/2021.3?topic=installation-structuring-your-deployment)\nto give you more insight of the things to consider for deployment. \n\nIn this tutorial we select to deploy Event Streams in one namespace, and the Operator to monitor multiple namespaces.\n\n## Prerequisites\n\n* Get access to an OpenShift Cluster.\n* Get `oc ` CLI from OpenShift Admin Console\n\n### Install Cloudctl CLIs \n\n* Install IBM Cloud Pak CLI: Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). \nThis CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak. In order to install it, execute the following commands in your IBM Cloud Shell:\n\n  * Download the IBM Cloud Pak CLI (example below is for Mac but  other downloads are [available here](https://github.com/IBM/cloud-pak-cli/releases/tag/v3.12.0)): \n\n  ```sh\n  curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-darwin-amd64.tar.gz -o cloudctl.tar.gz\n  ```\n  * Untar it\n\n  ```sh\n  tar -xvf cloudctl.tar.gz\n  ```\n\n  * Rename it as cloudctl and move to a folder within your PATH:\n\n  ```sh\n  mv cloudctl-darwin-amd64 cloudctl\n  ```\n\n  * Make sure your IBM Cloud Pak CLI is in the path: `which cloudctl`\n  * Make sure your IBM Cloud Pak CLI works: `cloudctl help`\n\n  ![shell2](./images/shell2-v10.png)\n\n\nSee also the product documentation for [prerequisites details](https://ibm.github.io/event-streams/installing/prerequisites/)\n\n### Install IBM catalog\n\nUse the EDA Gitops project with the different operator subscriptions and IBM catalog to define\nthe IBM Operator catalog.\n\n```sh\n# List existing catalog\noc get catalogsource -n openshift-marketplace\n# Verifi ibm-operator-catalog is present in the list\n# if not add IBM catalog \noc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-gitops-catalog/main/ibm-catalog/catalog_source.yaml\n```\n\n### Get IBM Software entitlement key and create a secret\n\n* Obtain [IBM license entitlement key](https://github.com/IBM/cloudpak-gitops/blob/main/docs/install.md#obtain-an-entitlement-key).\n* Verify you can access the IBM Image repository with\n\n    ```sh\n    docker login cp.icr.io --username cp --password <key-copied>\n    ```\n\n* Create a OpenShift secret in the `openshift-operator` project \n\n    ```sh\n    # Get on good project\n    oc project openshift-operators\n    # Verify if secret exists\n    oc describe secret ibm-entitlement-key\n    # Create if needed\n    oc create secret docker-registry ibm-entitlement-key \\\n        --docker-username=cp \\\n        --docker-server=cp.icr.io \\\n        --namespace=eventstreams \\\n        --docker-password=your_entitlement_key \n    ```\n\n> Attention\n  This secret needs to be present in any namespace where an Event Streams cluster will be deployed.\n\n### Install IBM foundational services\n\nWhen using security with IAM and other services, Event Streams needs IBM foundations services. It will install it\nautomatically if you install the operator at the cluster level. \nIn case you need to control the deployment and if *Cloud Pak for Integration* is not installed yet, you need to install the IBM foundational services operator\nfor that, follow these [simple instructions](https://www.ibm.com/docs/en/cpfs?topic=311-installing-foundational-services-by-using-console).\n\nCreate the same ibm-entitlement-key in the `ibm-common-services`  namespace.\n\n```sh\n# verify operator installed\noc get operators -n ibm-common-services\n# -> response:\nibm-common-service-operator.openshift-operators    \nibm-namespace-scope-operator.ibm-common-services   \nibm-odlm.ibm-common-services                       \n```\n\nThose are the deployments the foundational services are creating:\n\n```\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\nibm-common-service-webhook             1/1     1            1           68m\nibm-namespace-scope-operator           1/1     1            1           69m\noperand-deployment-lifecycle-manager   1/1     1            1           68m\nsecretshare                            1/1     1            1           68m\n```\n\nRecalls that operands are what operators manage. \n\n---\n\n## Install Event Streams Using OpenShift Console\n\n* As Cluster Administrator, create a project using Home > Projects, and create button, and enter `eventstreams` as project name.\n\n    ![](./images/create-project.png)\n\nThe goal is to create a shareable Kafka cluster. \n\n* As Administrator, use `Openshift console -> Operators > OperatorHub` to search for `Event Streams` operator, then install the operator by selecting\nthe `All namespaces on the cluster`, the version v2.4. The Operator will monitor all namespaces.\n\n  ![](./images/es-operator-cfg.png)\n\n  The installation of this operator will also include pre-requisites operators like `Cloud Pak foundational services (3.11)`\n  \n* Once the Operator is ready, go to the `Installed Operator` under the `eventstreams` project\n\n ![](./images/es-operator-home.png)\n\n* Create an Event Streams cluster instance using the operator user interface:\n\n ![](./images/create-es-instance.png)\n\n  Enter the minimum information for a development cluster, like name, license,.. \n\n  ![](./images/es-top-form.png)\n\n  Or go to the `Yaml view` and select one of the proposed Sample:\n  \n  ![](./images/es-yaml-view.png)\n  \n  The `Development` configuration should be enough to get you started. For production\n  deployment [see this article](/technology/event-streams/es-cp4i/es-production/).\n  \n  Do not forget to set `spec.license.accept` to true.\n\n  Before creating the instance, we recommend you read [the security summary](/technology/security/) \n  so you can relate the authentication configured by default with what application\n  needs to use to connect to the Kafka cluster. For example the following declaration stipulates\n  to use TLS authentication for internal communication, and SCRAM for external connection.\n\n    ```yaml\n        listeners:\n          plain:\n            port: 9092\n            type: internal\n            tls: false\n          external:\n            authentication:\n              type: scram-sha-512\n            type: route\n          tls:\n            authentication:\n              type: tls\n    ```\n\nThe first deployment of Event Streams, there may be creation of new deployments into the `ibm-common-services` if\nthose services were not present before. Here is the updated list of `ibm-common-services` deployments:\n\n  ```sh\n  NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\n  auth-idp                               1/1     1            1           27m\n  auth-pap                               1/1     1            1           32m\n  auth-pdp                               1/1     1            1           32m\n  cert-manager-cainjector                1/1     1            1           31m\n  cert-manager-controller                1/1     1            1           31m\n  cert-manager-webhook                   1/1     1            1           30m\n  common-web-ui                          1/1     1            1           20m\n  configmap-watcher                      1/1     1            1           31m\n  default-http-backend                   1/1     1            1           31m\n  iam-policy-controller                  1/1     1            1           32m\n  ibm-cert-manager-operator              1/1     1            1           32m\n  ibm-common-service-webhook             1/1     1            1           3h12m\n  ibm-commonui-operator                  1/1     1            1           32m\n  ibm-iam-operator                       1/1     1            1           33m\n  ibm-ingress-nginx-operator             1/1     1            1           32m\n  ibm-management-ingress-operator        1/1     1            1           33m\n  ibm-mongodb-operator                   1/1     1            1           32m\n  ibm-monitoring-grafana                 1/1     1            1           32m\n  ibm-monitoring-grafana-operator        1/1     1            1           33m\n  ibm-namespace-scope-operator           1/1     1            1           3h12m\n  ibm-platform-api-operator              1/1     1            1           31m\n  management-ingress                     1/1     1            1           23m\n  nginx-ingress-controller               1/1     1            1           31m\n  oidcclient-watcher                     1/1     1            1           32m\n  operand-deployment-lifecycle-manager   1/1     1            1           3h11m\n  platform-api                           1/1     1            1           31m\n  secret-watcher                         1/1     1            1           32m\n  secretshare                            1/1     1            1           3h12m\n  ```\n\nIt could take few minutes to get these pods up and running.\n\nThe result of this cluster creation should looks like: \n\n![](./images/es-created.png)\n\nwith the list of pod as illustrated at the top of this article.\n\nSee also [Cloud Pak for integration documentation](https://www.ibm.com/docs/en/cloud-paks/cp-integration/2021.3?topic=installing-overview-installation) for other\ndeployment considerations.\n\n### Adding users and teams \n\nYou need to be a platform administrator to create users and teams in IAM.\n\nTo get the admin user credential, use the following command:\n\n```sh\noc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' -n ibm-common-services | base64 && echo \"\"\n```\n\n### Log into Event Streams\n\nYou can get the User Interface end point by doing the following command.\n\n```sh\noc get route dev-ibm-es-ui -o jsonpath='{.spec.host}'\n```\n\n> Change `dev` prefix with the name of your Kafka cluster you defined earlier.\n\nOr using the Admin Console \n\n* Select the Event Streams Operator in the project where you install it\n* Select Click on the IBM Event Streams Operator and then on the `Event Streams` option listed at the top bar\n1. Click on the IBM Event Streams cluster instance you want to access to its console and see the `Admin UI` attribute that displays the route to this IBM Event Streams instance's console.\n  ![log](./images/admin-ui-url.png)\n1. Click on the route link and enter your IBM Event Streams credentials.\n\nHere is the Console home page:\n\n![](./images/es-console.png)\n\n\n### Create Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift. \nThe example is to define a topic named INBOUND with 1 partition and a replica set to 3.\n\n1. Log into your IBM Event Streams instance through the UI as explained in the previous section.\n1. Click on the Topics option on the navigation bar on the left. \n1. In the topics page, click on the `Create topic` blue button on the top right corner\n\n  ![Create Topic](./images/create-topic.png)\n\n1. Provide a name for your topic.\n\n  ![Topic Name](./images/inbound-topic-name.png)\n\n1. Leave Partitions at 1.\n\n  ![Partition](./images/partitions.png)\n\n1. Depending on how long you want messages to persist you can change this.\n\n  ![Message Retention](./images/message-retention.png)\n\n1. You can leave Replication Factor at the default 3.\n\n  ![Replication](./images/replicas.png)\n\n1. Click Create.\n1. Make sure the topic has been created by navigating to the topics section on the IBM Event Streams user inteface you can find an option for in the left hand side menu bar. \n\n## Run starter application\n\n[See separate note](/technology/event-streams/starter-app)\n\n--- \n\n## Install Event Streams Using CLIs\n\nThis section is an alternate of using OpenShift Console. We are using our GitOps catalog repository which\ndefines the different operators and scripts we can use to install Event Streams and other services\nvia scripts. All can be automatized with ArgoCD / OpenShift GitOps.\n\n* Create a project to host Event Streams cluster:\n\n  ```shell\n  oc new-project eventstreams\n  ```\n* Clone our eda-gitops-catalog project\n\n  ```sh\n  git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git\n  ```\n* Create the `ibm-entitlement-key` secret in this project.\n\n  ```sh\n  oc create secret docker-registry ibm-entitlement-key \\\n        --docker-username=cp \\\n        --docker-server=cp.icr.io \\\n        --namespace=eventstreams \\\n        --docker-password=your_entitlement_key \n  ```\n\n* Install Event Streams Operator subscriptions\n\n  ```sh\n  oc apply -k cp4i-operators/event-streams/operator/overlays/v2.4/\n  ```\n\n* Install one Event Streams instance: Instances of Event Streams can be created after the Event Streams operator is installed. \nYou can use te OpenShift console or our predefined cluster definition:\n\n  ```shell\n  oc apply -k cp4i-operators/event-streams/instances/dev/\n  ```\n\n  If you want to do the same thing for a production cluster\n\n  ```shell\n  oc apply -k cp4i-operators/event-streams/instances/prod-small/\n  ```\n---\n\n## Common operations to perform on cluster\n\nWe list here a set of common operations to perform on top of Event Streams Cluster.\n\n### Download ES CLI plugin\n\nFrom Event Streams Console, go to the \"Find more in the Toolbox\" tile, and then select\n`IBM Event Streams command-line interface`, then select your target operating system:\n\n![](./images/es-cli-download.png)\n\nInitialize the Event Streams CLI plugin (make sure you provide the namespace where your IBM Event Streams instance is installed on as the command will fail if you dont have cluster wide admin permissions)\n\n  ```shell\n  $ cloudctl es init -n eventstreams\n                                                  \n  IBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox.gse-ocp.net   \n  Namespace:                                     integration   \n  Name:                                          kafka   \n  IBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox.gse-ocp.net   \n  Event Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox.gse-ocp.net   \n  Event Streams API status:                      OK   \n  Event Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox.gse-ocp.net   \n  Apicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox.gse-ocp.net   \n  Event Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox.gse-ocp.net:443   \n  OK\n  ``` \n\n### Access to Event Streams Console using CLI\n\nIn order to log into IBM Event Streams console through the CLI, \nwe are going to use the `oc` OpenShift CLI, the `cloudctl` Cloud Pak CLI and \nthe `es` Cloud Pak CLI plugin. \n\nWe assume you are already logged into your OpenShift cluster.\n\n1. Get your Cloud Pak Console route (you may need cluster wide admin permissions to do so as the Cloud Pak Console is usually installed in the `ibm-common-services` namespace by the cluster admins)\n\n  ```shell\n  $ oc get routes -n ibm-common-services | grep console\n  cp-console                       cp-console.apps.eda-sandbox.gse-ocp.net                                                  icp-management-ingress           https      reencrypt/Redirect     None\n  ```\n\n1. Log into IBM Event Streams using the Cloud Pak console route from the previous step:\n\n  ```shell\n  $ cloudctl login -a https://cp-console.apps.eda-sandbox.gse-ocp.net --skip-ssl-validation\n\n  Username> user50\n\n  Password>\n  Authenticating...\n  OK\n\n  Targeted account mycluster Account\n\n  Enter a namespace > integration\n  Targeted namespace integration\n\n  Configuring kubectl ...\n  Property \"clusters.mycluster\" unset.\n  Property \"users.mycluster-user\" unset.\n  Property \"contexts.mycluster-context\" unset.\n  Cluster \"mycluster\" set.\n  User \"mycluster-user\" set.\n  Context \"mycluster-context\" created.\n  Switched to context \"mycluster-context\".\n  OK\n\n  Configuring helm: /Users/user/.helm\n  OK\n  ```\n\n### Create Topic with es CLI\n\n1. Create the topic with the desi specification.\n\n   ```shell\n   cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n   ```\n\n1. Make sure the topic has been created by listing the topics.\n\n  ```shell\n  $ cloudctl es topics\n  Topic name   \n  INBOUND   \n  OK\n  ```\n\n### Get Kafka Bootstrap Url\n\nFor an application to access the Kafka Broker, we need to get the bootstrap URL.\n\n#### UI\n\nYou can find your IBM Event Streams Kakfa bootstrap url if you log into the IBM Event\n Streams user interface, and click on the `Connect to this cluster` option displayed\n  on the dashboard. This will display a menu where you will see a url to the left \n  of the `Generate SCAM credentials` button. Make sure that you are on \n  the **External Connection**.\n\n![Connect to this Cluster bootstrap](./images/esv10-connect-to-cluster-bootstrap.png)\n\n#### CLI\n\nYou can find your IBM Event Streams and Kakfa bootstrap url when you init the \nIBM Event Streams Cloud Pak CLI plugin on the last line:\n\n```shell\n$ cloudctl es init -n eventstreams\n                                                  \nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox-delta.gse-ocp.net   \nNamespace:                                     integration   \nName:                                          kafka   \nIBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API status:                      OK   \nEvent Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net   \nApicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443   \nOK\n``` \n  \n### Generate SCRAM Service Credentials\n\nFor an application to connect to an Event Streams instance through the secured \nexternal listener, it needs SCRAM credentials to act as service credentials\nto authenticate the application. We also need TLS certificate to encrypt the\ncommunication with the brokers.\n\n#### UI\n\n1. Log into the IBM Event Streams user interface as explained previously in this readme and click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu. Make sure that you are on the **External Connection**.\n\n  ![Connect to this Cluster](./images/esv10-connect-to-cluster.png)\n\n1. Click on the `Generate SCRAM credentials` button.\n\n  ![Generate SCRAM1 ](./images/esv10-generate-scram.png)\n\n1. Introduce a name for your credentials and choose the option that better suits the needs of your applications (this will create RBAC permissions for you credentials so that a service credentials can do only what it needs to do). For this demo, select `Produce messages, consume messages and create topics and schemas` last option.\n\n  ![Generate SCRAM 2](./images/esv10-scram-1.png)\n\n1. Decide whether your service credentials need to have the ability to access all topics or certain topics only. For this demo, select `All Topics` and then click Next.\n\n  ![Generate SCRAM 3](./images/esv10-scram-2.png)\n\n1. Decide whether your service credentials need to have the ability to access all consumer groups or certain specific consumer groups only. For this demo, select `All Consumer Groups` and click Next.\n\n  ![Generate SCRAM 4](./images/esv10-scram-3.png)\n\n1. Decide whether your service credentials need to have the ability to access all transactional IDs or certain specific transactional IDs only. For this demo, select `All transaction IDs` and click on `Generate credentials`.\n\n  ![Generate SCRAM 5](./images/esv10-scram-4.png)\n\n1. **Take note** of the set of credentials displayed on screen. \nYou will need to provide your applications with these in order to get authenticated \nand authorized with your IBM Event Streams instance.\n\n  ![Generate SCRAM 6](./images/esv10-scram-5.png)\n\n1. If you did not take note of your SCRAM credentials or you forgot these, the above will create a `KafkaUser` object in OpenShift that is interpreted by the IBM Event Streams Operator. You can see this `KafkaUser` if you go to the OpenShift console, click on `Operators --> Installed Operators` on the right hand side menu, then click on the `IBM Event Streams` operator and finally click on `Kafka Users` at the top bar menu.\n\n  ![Retrieve SCRAM 1](./images/esv10-retrieve-scram-1.png)\n\n1. If you click on your `Kafka User`, you will see what is the Kubernetes secret behind holding your SCRAM credentials details.\n\n  ![Retrieve SCRAM 2](./images/esv10-retrieve-scram-2.png)\n\n1. Click on that secret and you will be able to see again your `SCRAM password` (your `SCRAM username` is the same name as the `Kafka User` created or the secret holding your `SCRAM password`) \n\n  ![Retrieve SCRAM 3](./images/esv10-retrieve-scram-3.png)\n\n\n#### CLI\n\n1. Log into your IBM Event Streams instance through the CLI.\n\n1. Create your SCRAM service credentials with the following command (adjust the topics, consumer groups, transaction IDs, etc permissions your SCRAM service credentials should have in order to satisfy your application requirements):\n\n  ```shell\n  $ cloudctl es kafka-user-create --name test-credentials-cli --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n  \n  KafkaUser name         Authentication   Authorization   Username                                                Secret   \n  test-credentials-cli   scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret   \n  \n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  \n  Created KafkaUser test-credentials-cli.\n  OK\n  ```\n\n1. List the `KafkaUser` objects to make sure yours has been created:\n\n  ```shell\n  $ cloudctl es kafka-users\n  KafkaUser name                    Authentication   Authorization   \n  test-credentials                  scram-sha-512    simple   \n  test-credentials-cli              scram-sha-512    simple   \n  OK\n  ```\n\n1. To retrieve your credentials execute the following command:\n\n  ```shell\n  $ cloudctl es kafka-user test-credentials-cli \n  KafkaUser name         Authentication   Authorization   Username               Secret   \n  test-credentials-cli   scram-sha-512    simple          test-credentials-cli   test-credentials-cli   \n\n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  OK\n  ```\n\n1. Above you can see your `SCRAM username` under `Username` and the secret holding your `SCRAM password` under `Secret`. In order to retrieve the password, execute the following command:\n\n  ```shell\n  $ oc get secret test-credentials-cli -o jsonpath='{.data.password}' | base64 --decode \n  \n  *******\n  ```\n\n**NEXT:** For more information about how to connect to your cluste, read the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/getting-started/connecting/)\n\n### Get Event Streams TLS Certificates\n\nIn this section we are going to see how to download the TLS certificats to securely connect to our IBM Event Streams instance.\n\n#### UI\n\n1. Log into the IBM Event Streams console user interface as explained before in this readme.\n\n1. Click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu where you will see a `Certificates` section:\n\n  ![PKCS12 Truststore](./images/esv10-pkcs12.png)\n\n1. Depending on what language your application is written into, you will need a `PKCS12 certificate` or a `PEM certificate`. Click on `Download certificate` for any of the options you need. If it is the `PKCS12 certificate` bear in mind it comes with a password for the truststore. You don't need to write this down as it will display any time you click on `Download certificate` button.\n\n#### CLI\n\n1. Log into IBM Event Streams through the CLI as already explained before in this readme.\n\n1. To retrieve the `PKCS12 certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format p12\n  Trustore password is ********\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.p12.\n  OK\n  ```\n\n1. To retrieve the `PEM certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format pem\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.pem.\n  OK\n  ```\n\n## Demonstrate Event Streams from A to Z\n\n[See separate note](/technology/event-streams/demo-a-z/)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/event-streams/es-cp4i/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}