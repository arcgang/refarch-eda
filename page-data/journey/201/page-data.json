{"componentChunkName":"component---src-pages-journey-201-index-mdx","path":"/journey/201/","result":{"pageContext":{"frontmatter":{"title":"Learning Journey - deeper dive (201 content)","description":"Learning more about Event Streams, Event Driven Solution"},"relativePagePath":"/journey/201/index.mdx","titleType":"append","MdxNode":{"id":"3ec8f4f7-4b17-5788-a8d2-ffa407f4d88f","children":[],"parent":"a5966c9f-5f7d-5562-b8b2-d731afa5cdc3","internal":{"content":"---\ntitle: Learning Journey - deeper dive (201 content)\ndescription: Learning more about Event Streams, Event Driven Solution\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 10/25/2021 - Work In Progress</strong>\n</InlineNotification>\n\nIn this `201` content, you should be able to learn more about Kafka, Event Streams, Messaging, and Event-driven solution.\n\n<AnchorLinks>\n  <AnchorLink>More Kafka</AnchorLink>\n  <AnchorLink>Production deployment - High Availability</AnchorLink>\n  <AnchorLink>Event-driven solution GitOps deployment</AnchorLink>\n  <AnchorLink>Performance considerations</AnchorLink>\n  <AnchorLink>EDA Design patterns</AnchorLink>\n  <AnchorLink>Kafka Connect Framework</AnchorLink>\n  <AnchorLink>Integrate with MQ</AnchorLink>\n  <AnchorLink>Introduction to schema management</AnchorLink>\n  <AnchorLink>AsyncAPI</AnchorLink>\n  <AnchorLink>Debezium change data capture</AnchorLink>\n  <AnchorLink>Mirroring Data</AnchorLink>\n</AnchorLinks>\n\n\n## More Kafka\n\nWe have already covered the Kafka architecture in [this section](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/#kafka-components).\nWhen we deploy Event Streams on Kubernetes, it uses Operator, and it is in fact a wrapper on top of [Strimzi](http://strimzi.io),\nthe open source kafka operator.\n\n[Developer.ibm learning path: Develop production-ready, Apache Kafka apps](https://developer.ibm.com/learningpaths/develop-kafka-apps/)\n\n\n### Strimzi\n\n[Strimzi](https://strimzi.io/) uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. \nWhen the Strimzi Cluster Operator is up and runnning, it starts to watch for certain OpenShift or Kubernetes resources containing the \ndesired Kafka and/or Kafka Connect cluster configuration. \n\n![Strimzi](./images/strimzi.png)\n\nIt supports the following capabilities:\n\n* Deploy Kafka OOS on any OpenShift or k8s platform\n* Support TLS and SCRAM-SHA authentication, and automated certificate management\n* Define operators for cluster, users and topics\n* All resources are defined in yaml file so easily integrated into GitOps\n\nThe Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka \nConnect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator.\nWhen deployed the following commands goes to the Cluster operator:\n\n```shell\n# Get the current cluster list\noc get kafka\n# get the list of topic\noc get kafkatopics\n```\n\n#### Installation on OpenShift\n\nThe Strimzi operators deployment is done in two phases:\n\n* Deploy the main operator via Subscription\n* Deploy one to many instances of the Strimzi CRDs: cluster, users, topics...\n\nFor that we have define subscription and configuration in [this eda-gitops-catalog repo](https://github.com/ibm-cloud-architecture/eda-gitops-catalog). \nSo below are the operations to perform:\n\n```shell\n # clone \n git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git\n # Define subscription\n oc apply -k kafka-strimzi/operator/overlays/stable/\n # The subscription creates an operator pod under the openshift-operators project\n oc get pods -n openshift-operators\n # Create a project e.g. strimzi\n oc new-project strimzi\n # deploy a simple kafka cluster with 3 brokers\n oc apply -k  kafka-strimzi/instance/\n # Verify installation\n oc get pods\n # should get kafka, zookeeper and the entity operator running.\n```\n\nThe [Strimzi documentation](https://strimzi.io/docs/operators/latest/using.html) is very good to present a lot of configuration and tuning practices.\n\n#### Application\n\nAll applications written with Kafka API will work the same way with Strimzi and Event Streams. So developer\ncan use Strimzi images for their local development.\n\n## Production deployment - High Availability\n\nKafka clustering brings availability for message replication and failover, see details in this [high availability section.](/technology/kafka-overview/advance/#high-availability)\nThis chapter presents replicas, in-synch replicas concepts and addresses some broker failure scenarios that are important to understand.\n\nWhen looking how Kafka is deployed on Kubernetes / Openshift it is important to isolate each broker\nto different worker node as illustrated in [this section](/technology/kafka-overview/advance/#high-availability-in-the-context-of-kubernetes-deployment).\n\nIn end-to-end deployment, the high availability will become more of a challenge for the producer and consumer.\nConsumers and producers should better run on separate servers than the brokers nodes. Producer may need\nto address back preasure on their own. Consumers need to have configuration that permit to do not enforce\npartition to consumer reassignment too quickly. Consumer process can fail and restart quickly and get the same partition allocated.\n\n## Event-driven solution GitOps deployment\n\nThe [\"event-driven solution GitOps approach\" article](/use-cases/gitops) goes over how to use OpenShift GitOps to deploy Event Streams, MQ, and \na business solution. You will learn how to bootstrap the GitOps environment and deploy the needed IBM operators then use\ncustom resource to define Event Streams cluster, topics, users...\n\nThe following solution GitOps repositories are illustrating the proposed approach:\n\n* [refarch-kc-gitops](https://github.com/ibm-cloud-architecture/refarch-kc-gitops): For the shipping fresh food overseas solution we have defined. It includes\nthe SAGA choreography pattern implemented with Kafka\n* [eda-kc-gitops](https://github.com/ibm-cloud-architecture/eda-kc-gitops): For the shipping fresh food overseas solution we have defined. It includes\nthe SAGA orchestration pattern implemented with MQ\n* [eda-rt-inventory-gitops](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops) to deploy the demo of real-time inventory\n\n\n## Performance considerations\n\nRead [this dedicated article on performance, resilience, throughput.](/technology/kafka-overview/advance/#performance-considerations)\n\n## EDA Design patterns \n\nEvent-driven solutions are based on a set of design pattern for application design. In \n[this article](/patterns/intro/), you will find the different pattern which\nare used a lot in the field like\n\n* [Event sourcing](/patterns/event-sourcing/): persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events.\n* [Command Query Responsibility Segregation](/patterns/cqrs/): helps to separate queries from commands and help to address queries with cross-microservice boundary.\n* [Saga pattern:](/patterns/saga/) Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable.\n* [Event reprocessing with dead letter](/patterns/dlq/): event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone.\n* [Transactional outbox](/patterns/intro/#transactional-outbox): A service command typically needs to update the database and send messages/events.\nThe approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns)\n\n\n## Kafka Connect Framework\n\nKafka connect is an open source component for easily integrate external systems with Kafka. \nIt works with any Kafka product such as IBM Event Streams, Red Hat AMQ Streams, or Strimzi.\nYou can learn more about it [in this article](/technology/kafka-connect/) and with those labs:\n\n  * [Connect to S3 source and sink](/use-cases/connect-s3/)\n  * [Connect to IBM Cloud Object Storage](/use-cases/connect-cos/)\n  * [Connect to a Database with JDBC Sink](/use-cases/connect-jdbc/)\n  * [Connect to MQ queue as source or Sink](use-cases/connect-mq/)\n  * [Connect to RabbitMQ](/use-cases/connect-rabbitmq/)\n\n## Integrate with MQ\n\nUsing Kafka Connect framework, IBM has a [MQ source connector](https://github.com/ibm-messaging/kafka-connect-mq-source)  \nand [MQ Sink connector](https://github.com/ibm-messaging/kafka-connect-mq-sink) to integrate easily between Event Streams and IBM MQ\nThe [following labs](/use-cases/connect-mq/) will help you learn more about how to use those connectors\nand this [gitops repository](https://github.com/ibm-cloud-architecture/store-mq-gitops) helps you to run a store simulation producing messages to MQ\nqueue, with Kafka Connector injecting those message to Event Streams.\n\nFor Confluent MQ connector lab see [this eda-lab-mq-to-kafka repository](https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka)\n\n## Introduction to schema management\n\nSchema management and schema registry are mandatory for doing production deployment, of any Kafka based solution.\nTo understand the following components read [this note](/technology/avro-schemas/)\n\n![](../../technology/avro-schemas/images/schema-registry.png)\n\n## AsyncAPI\n\n[This article on AsyncAPI management](/patterns/api-mgt/) presents the value of using AsyncAPI in API Connect.\nThis blog from development [What is Event Endpoint Management?](https://community.ibm.com/community/user/integration/blogs/dale-lane1/2021/04/08/what-is-event-endpoint-management)\npresents the methodology for event endpoint management:\n\n* event description including the event schema, the topic, and thte communication protocol\n* event discovery, with centralized management of the API\n* self service to easily try the API, but with secure policies enforcement\n* decoupled by API helps to abtract and change implementation if needed.\n\n## Debezium change data capture\n\nChange data capture is the best way to inject data from a database to Kafka. \n[Debezium](https://debezium.io/) is the Red Hat led open source project in this area. The [IBM InfoSphere Data Replication](https://www.ibm.com/docs/en/idr/11.4.0?topic=replication-cdc-engine-kafka) is a more advanced solution\nfor different sources and to kafka or other middlewares. \n\nOne lab [DB2 debezium](/use-cases/db2-debezium) not fully operational, looking for contributor to complete it.\n\nand an implementation of [Postgresql debezium cdc with outpost pattern](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg) and quarkus.\n\n## Mirroring Data\n\nTo replicate data between Kafka clusters, Mirror Maker 2 is the component to use. It is based on Kafka connector\nand will support a active-passive type of deployment or active active, which is little bit more complex.\n\n![](../../technology/kafka-mirrormaker/images/mm2-dr.png)\n\nSee the detail [Mirror Maker 2 technology summary](/technology/kafka-mirrormaker/)\n\n","type":"Mdx","contentDigest":"fae1c9d47541a66abf3971b706bb2e43","owner":"gatsby-plugin-mdx","counter":909},"frontmatter":{"title":"Learning Journey - deeper dive (201 content)","description":"Learning more about Event Streams, Event Driven Solution"},"exports":{},"rawBody":"---\ntitle: Learning Journey - deeper dive (201 content)\ndescription: Learning more about Event Streams, Event Driven Solution\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 10/25/2021 - Work In Progress</strong>\n</InlineNotification>\n\nIn this `201` content, you should be able to learn more about Kafka, Event Streams, Messaging, and Event-driven solution.\n\n<AnchorLinks>\n  <AnchorLink>More Kafka</AnchorLink>\n  <AnchorLink>Production deployment - High Availability</AnchorLink>\n  <AnchorLink>Event-driven solution GitOps deployment</AnchorLink>\n  <AnchorLink>Performance considerations</AnchorLink>\n  <AnchorLink>EDA Design patterns</AnchorLink>\n  <AnchorLink>Kafka Connect Framework</AnchorLink>\n  <AnchorLink>Integrate with MQ</AnchorLink>\n  <AnchorLink>Introduction to schema management</AnchorLink>\n  <AnchorLink>AsyncAPI</AnchorLink>\n  <AnchorLink>Debezium change data capture</AnchorLink>\n  <AnchorLink>Mirroring Data</AnchorLink>\n</AnchorLinks>\n\n\n## More Kafka\n\nWe have already covered the Kafka architecture in [this section](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/#kafka-components).\nWhen we deploy Event Streams on Kubernetes, it uses Operator, and it is in fact a wrapper on top of [Strimzi](http://strimzi.io),\nthe open source kafka operator.\n\n[Developer.ibm learning path: Develop production-ready, Apache Kafka apps](https://developer.ibm.com/learningpaths/develop-kafka-apps/)\n\n\n### Strimzi\n\n[Strimzi](https://strimzi.io/) uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. \nWhen the Strimzi Cluster Operator is up and runnning, it starts to watch for certain OpenShift or Kubernetes resources containing the \ndesired Kafka and/or Kafka Connect cluster configuration. \n\n![Strimzi](./images/strimzi.png)\n\nIt supports the following capabilities:\n\n* Deploy Kafka OOS on any OpenShift or k8s platform\n* Support TLS and SCRAM-SHA authentication, and automated certificate management\n* Define operators for cluster, users and topics\n* All resources are defined in yaml file so easily integrated into GitOps\n\nThe Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka \nConnect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator.\nWhen deployed the following commands goes to the Cluster operator:\n\n```shell\n# Get the current cluster list\noc get kafka\n# get the list of topic\noc get kafkatopics\n```\n\n#### Installation on OpenShift\n\nThe Strimzi operators deployment is done in two phases:\n\n* Deploy the main operator via Subscription\n* Deploy one to many instances of the Strimzi CRDs: cluster, users, topics...\n\nFor that we have define subscription and configuration in [this eda-gitops-catalog repo](https://github.com/ibm-cloud-architecture/eda-gitops-catalog). \nSo below are the operations to perform:\n\n```shell\n # clone \n git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git\n # Define subscription\n oc apply -k kafka-strimzi/operator/overlays/stable/\n # The subscription creates an operator pod under the openshift-operators project\n oc get pods -n openshift-operators\n # Create a project e.g. strimzi\n oc new-project strimzi\n # deploy a simple kafka cluster with 3 brokers\n oc apply -k  kafka-strimzi/instance/\n # Verify installation\n oc get pods\n # should get kafka, zookeeper and the entity operator running.\n```\n\nThe [Strimzi documentation](https://strimzi.io/docs/operators/latest/using.html) is very good to present a lot of configuration and tuning practices.\n\n#### Application\n\nAll applications written with Kafka API will work the same way with Strimzi and Event Streams. So developer\ncan use Strimzi images for their local development.\n\n## Production deployment - High Availability\n\nKafka clustering brings availability for message replication and failover, see details in this [high availability section.](/technology/kafka-overview/advance/#high-availability)\nThis chapter presents replicas, in-synch replicas concepts and addresses some broker failure scenarios that are important to understand.\n\nWhen looking how Kafka is deployed on Kubernetes / Openshift it is important to isolate each broker\nto different worker node as illustrated in [this section](/technology/kafka-overview/advance/#high-availability-in-the-context-of-kubernetes-deployment).\n\nIn end-to-end deployment, the high availability will become more of a challenge for the producer and consumer.\nConsumers and producers should better run on separate servers than the brokers nodes. Producer may need\nto address back preasure on their own. Consumers need to have configuration that permit to do not enforce\npartition to consumer reassignment too quickly. Consumer process can fail and restart quickly and get the same partition allocated.\n\n## Event-driven solution GitOps deployment\n\nThe [\"event-driven solution GitOps approach\" article](/use-cases/gitops) goes over how to use OpenShift GitOps to deploy Event Streams, MQ, and \na business solution. You will learn how to bootstrap the GitOps environment and deploy the needed IBM operators then use\ncustom resource to define Event Streams cluster, topics, users...\n\nThe following solution GitOps repositories are illustrating the proposed approach:\n\n* [refarch-kc-gitops](https://github.com/ibm-cloud-architecture/refarch-kc-gitops): For the shipping fresh food overseas solution we have defined. It includes\nthe SAGA choreography pattern implemented with Kafka\n* [eda-kc-gitops](https://github.com/ibm-cloud-architecture/eda-kc-gitops): For the shipping fresh food overseas solution we have defined. It includes\nthe SAGA orchestration pattern implemented with MQ\n* [eda-rt-inventory-gitops](https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops) to deploy the demo of real-time inventory\n\n\n## Performance considerations\n\nRead [this dedicated article on performance, resilience, throughput.](/technology/kafka-overview/advance/#performance-considerations)\n\n## EDA Design patterns \n\nEvent-driven solutions are based on a set of design pattern for application design. In \n[this article](/patterns/intro/), you will find the different pattern which\nare used a lot in the field like\n\n* [Event sourcing](/patterns/event-sourcing/): persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events.\n* [Command Query Responsibility Segregation](/patterns/cqrs/): helps to separate queries from commands and help to address queries with cross-microservice boundary.\n* [Saga pattern:](/patterns/saga/) Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable.\n* [Event reprocessing with dead letter](/patterns/dlq/): event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone.\n* [Transactional outbox](/patterns/intro/#transactional-outbox): A service command typically needs to update the database and send messages/events.\nThe approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns)\n\n\n## Kafka Connect Framework\n\nKafka connect is an open source component for easily integrate external systems with Kafka. \nIt works with any Kafka product such as IBM Event Streams, Red Hat AMQ Streams, or Strimzi.\nYou can learn more about it [in this article](/technology/kafka-connect/) and with those labs:\n\n  * [Connect to S3 source and sink](/use-cases/connect-s3/)\n  * [Connect to IBM Cloud Object Storage](/use-cases/connect-cos/)\n  * [Connect to a Database with JDBC Sink](/use-cases/connect-jdbc/)\n  * [Connect to MQ queue as source or Sink](use-cases/connect-mq/)\n  * [Connect to RabbitMQ](/use-cases/connect-rabbitmq/)\n\n## Integrate with MQ\n\nUsing Kafka Connect framework, IBM has a [MQ source connector](https://github.com/ibm-messaging/kafka-connect-mq-source)  \nand [MQ Sink connector](https://github.com/ibm-messaging/kafka-connect-mq-sink) to integrate easily between Event Streams and IBM MQ\nThe [following labs](/use-cases/connect-mq/) will help you learn more about how to use those connectors\nand this [gitops repository](https://github.com/ibm-cloud-architecture/store-mq-gitops) helps you to run a store simulation producing messages to MQ\nqueue, with Kafka Connector injecting those message to Event Streams.\n\nFor Confluent MQ connector lab see [this eda-lab-mq-to-kafka repository](https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka)\n\n## Introduction to schema management\n\nSchema management and schema registry are mandatory for doing production deployment, of any Kafka based solution.\nTo understand the following components read [this note](/technology/avro-schemas/)\n\n![](../../technology/avro-schemas/images/schema-registry.png)\n\n## AsyncAPI\n\n[This article on AsyncAPI management](/patterns/api-mgt/) presents the value of using AsyncAPI in API Connect.\nThis blog from development [What is Event Endpoint Management?](https://community.ibm.com/community/user/integration/blogs/dale-lane1/2021/04/08/what-is-event-endpoint-management)\npresents the methodology for event endpoint management:\n\n* event description including the event schema, the topic, and thte communication protocol\n* event discovery, with centralized management of the API\n* self service to easily try the API, but with secure policies enforcement\n* decoupled by API helps to abtract and change implementation if needed.\n\n## Debezium change data capture\n\nChange data capture is the best way to inject data from a database to Kafka. \n[Debezium](https://debezium.io/) is the Red Hat led open source project in this area. The [IBM InfoSphere Data Replication](https://www.ibm.com/docs/en/idr/11.4.0?topic=replication-cdc-engine-kafka) is a more advanced solution\nfor different sources and to kafka or other middlewares. \n\nOne lab [DB2 debezium](/use-cases/db2-debezium) not fully operational, looking for contributor to complete it.\n\nand an implementation of [Postgresql debezium cdc with outpost pattern](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg) and quarkus.\n\n## Mirroring Data\n\nTo replicate data between Kafka clusters, Mirror Maker 2 is the component to use. It is based on Kafka connector\nand will support a active-passive type of deployment or active active, which is little bit more complex.\n\n![](../../technology/kafka-mirrormaker/images/mm2-dr.png)\n\nSee the detail [Mirror Maker 2 technology summary](/technology/kafka-mirrormaker/)\n\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/journey/201/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}