{"componentChunkName":"component---src-pages-use-cases-connect-cos-index-mdx","path":"/use-cases/connect-cos/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to IBM COS","description":"Apache Kafka to IBM Cloud Object Storage Source Connector usecase"},"relativePagePath":"/use-cases/connect-cos/index.mdx","titleType":"append","MdxNode":{"id":"59a7d688-86ad-5326-a261-a86bcbd504a8","children":[],"parent":"ad3d5018-6410-5bb0-ac3c-ce3730f16020","internal":{"content":"---\ntitle: Kafka Connect to IBM COS\ndescription: Apache Kafka to IBM Cloud Object Storage Source Connector usecase\n---\n\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 02/22/2022</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Introduction</AnchorLink>\n  <AnchorLink>Use Case Guided Tour</AnchorLink>\n  <AnchorLink>Full Demo Narration</AnchorLink>\n  <AnchorLink>Developer Corner</AnchorLink>\n </AnchorLinks>\n\n---\n\n# Introduction\n\nOne of the classical use case for event driven solution based on Kafka is to keep the message for a longer time\nperiod than the retention time of the Kafka topic. This demonstration illustrates how to do so using IBM Event Streams\nand IBM Cloud Object Storage as a service.\n\nWe have created a very simple Quarkus (a super sonic and sub-atomic Kubernetes native framework for Java) application that \nuses MicroProfile Reactive Messaging in order to send a stream of data to our Event Streams/Kafka topic. \n\nThe Kafka Connect cluster includes a IBM Cloud Object Storage Connector to grab messages fomr the topoc and \nplace into an IBM COS Bucket.\n\n![Architecture Diagram](./images/quarkus-to-event-streams-to-cos.png)\n\n### Skill level\n\nThe skill level of this learning path is for a beginner.\n\n### Estimated time to complete\n\nIt will take you approximately 15 minuttes to complete this entire demonstration.\n\n### Scenario Prerequisites\n\n**OpenShift Container Platform Cluster**\n  - This scenario will assume you have a 4.7+ Cluster as we will make use of Operators.\n\n**Cloud Pak for Integration**\n  - Updated with 2021.0.3 release of the Cloud Pak for Integration installed on OpenShift. \nThis story will also assume you have followed the installation instructions for Event Streams\n outlined in [the 2021-3 product documentation](https://ibm.github.io/event-streams/installing/installing/) or used one of our [GitOps repository](https://github.com/ibm-cloud-architecture/eda-gitops-catalog) approach\n to deploy Event Streams Operator.\n\n**Git**\n  - We will need to clone repositories.\n\n**An IBM Cloud Account (free)**\n  - A free (Lite) IBM Cloud Object Storage trial Service account [IBM Cloud Object Storage](https://cloud.ibm.com/catalog/services/cloud-object-storage)\n\n__If you want to modify the code then you need:__\n\n**Java**\n  - Java Development Kit (JDK) v1.11+ (Java 11+)\n\n**Maven**\n  - The scenario uses Maven v3.6.3\n\n**Gradle**\n  - Ideally v4.0+ (Note - the gradle shadowJar command might not work on Java versions newer to Java 8)\n\n**An IDE of your choice**\n  - Visual Studio Code is used in this scenario.\n\n---\n\n# Use Case Guided Tour\n\nThe use case is quite simple, but very common. Topic has configuration to keep messages in the kafka broker disks for a certain time perior or until the log reach a certain size.\nWhich means records will be removed from the brokers over time. Some companies put retention time to a value to keep data foreever. This is possible when the amount\nof data in such topic is not reasonable. But most of the time we need to design solution to move data to longer persistence. S3 buckets is one of such\nlong term persistence as it will bring elasticity, transparent replication and geolocalization. \n\nAn infrastructure engineer needs to prepare the Cloud Object Storage service and organize the bucket strategy to keep data. Then on the OpenShift production\ncluster, Event Streams is deployed and Kafka connector cluster defined. The connector configuration will stipulate the COS connection credentials and\nthe Event Streams connection URL and credentials.\n\nOnce the connector is started all messages in the designated topics will go to the COS bucket. \n\nIBM Event Streams team has explanation of the Cloud Object Storage Sink Kafka connector configuration in [this repository](https://github.com/ibm-messaging/kafka-connect-ibmcos-sink).\n\n---\n\n# Full Demo Narration\n\n## 1- Create an IBM COS Service and COS Bucket\n\nIn this section, we are going to see how to create an IBM Cloud Obeject Storage (IBM COS) Service in your IBM Cloud account and a bucket within your \nIBM COS Service. We assume you already have an IBM Cloud account and, if not, you can sign up for one here at [IBM Cloud](https://cloud.ibm.com).\n\n1. Once you are inside your IBM Cloud account, traverse to the `Catalog` section. In the search type in `IBM Cloud Object Storage`\n\n  ![IBM COS Catalog Search](./images/ibm-cloud-create-cos-service.png)\n\n1. Name your IBM COS Service with something unique. Since this is a free account, we can stick with the `Lite Plan`.\n\n  ![IBM COS Create COS Service](./images/ibm-cloud-create-cos-service-2.png)\n\n1.  Now that the IBM Cloud Object Storage Service is created, create a new bucket. On the `Create Bucket` screen pick `Custom Bucket`.\n\n  ![IBM COS Custom Bucket](./images/ibm-cos-create-bucket.png)\n\n1. When selecting options for the bucket, name your bucket something unique. For `Resiliency` let's select `Regional`. \nFor location select an area from the drop-down that you want. Use `Standard` for `Storage Class`. \nLeave everything else as-is and hit `Create Bucket`.\n\n  ![IBM COS Custom Bucket Settings](./images/ibm-cos-bucket-settings.png)\n\n## 2- Create IBM COS Service Credentials\n\nNow that we have created our IBM Cloud Object Storage Service and bucket, we need to create the Service Credential so that Kafka connector can connect to it.\n\n1. Inside your IBM COS Service, select `Service Credentials` and then click the `New Credential` button.\n\n  ![IBM COS Service Credential](./images/ibm-cos-create-service-cred.png)\n\n1. Name your credential and select `Manager` from the `Role:` drop-down menu and click `Add`.\n\n  ![IBM COS SC Settings](./images/ibm-cos-service-credentials.png)\n\n1. Expand your newly created Service Credential and write down the values for `\"apikey\"` and `\"resource_instance_id\"`. \nYou will need this later in the [Build and Apply IBM COS Sink Connector](#build-and-apply-ibm-cos-sink-connector) section.\n\n  ![Expanded Service Cred](./images/ibm-service-credential-keys.png)\n\n## 3- Create a demo project\n\n\nTo isolate the demonstration in the OpenShift Cluster, we will deploy the demo code, event streams cluster and Kafka Connect in one project.\n\n1. Clone the tutorial project\n\n   ```sh\n   git clone https://github.com/ibm-cloud-architecture/eda-quickstarts\n   ```\n\n1. Create an OpenShift project (k8s namespace) named: `eda-cos`\n\n  ```sh\n  oc apply -k gitOps/env/base\n  ```\n\n## 4- Deploy Event Streams Cluster\n\nIf you do not have a Event Stream cluster already deployed on OpenShift, we propose to deploy one in a demo project, using Event Streams Operator monitoring All Namespaces\n\n1. Using [IBM entitled registry entitlement key](https://www.ibm.com/docs/en/cloud-paks/cp-integration/2020.2?topic=installation-entitled-registry-entitlement-keys) \n define your a secret so deployment process can download IBM Event Streams images:\n\n  ```sh\n  KEY=<yourentitlementkey>\n  oc create secret docker-registry ibm-entitlement-key \\\n      --docker-username=cp \\\n      --docker-password=$KEY \\\n      --docker-server=cp.icr.io \\\n      --namespace=eda-cos\n  ```\n\n1. Deploy Event Streams Cluster.\n  \n  ```sh\n  oc apply -k gitOps/services/es \n  # --> Results\n  eventstreams.eventstreams.ibm.com/dev created\n  kafkatopic.eventstreams.ibm.com/edademo-orders created\n  ```\n\n  It will take sometime to get the cluster created. Monitor with `oc get pod -w`. You should get:\n\n  ```\n  dev-entity-operator-6d7d94f68f-6lk86   3/3    \n  dev-ibm-es-admapi-ffd89fdf-x99lq       1/1    \n  dev-ibm-es-ui-74bf84dc67-qx9kk         2/2    \n  dev-kafka-0                            1/1    \n  dev-zookeeper-0                        1/1\n  ```\n\n  With this deployment there is no external route, only on bootstrap URL: `dev-kafka-bootstrap.eda-cos.svc:9092`. The Kafka listener is using PLAINTEXT connection. So no SSL encryption and no authentication.\n\n1. Deploy the existing application (the image we built is in quay.io/ibmcase) using:\n\n  ```sh\n  oc apply -k gitOps/apps/eda-cos-demo/base/\n  ```\n\n1. Test the deployed app by accessing its route and GET API: \n\n  ```sh\n  HOST=$(oc get routes eda-cos-demo  -o jsonpath='{.spec.host}')\n  curl -X GET http://$HOST/api/v1/version\n  ```\n\n## 5- Deploy Kafka Connector\n\nWe have already built a kafka connect image with the Cloud Object Storage jar and push it as image to quay.io.\n\n1. Deploy the connector cluster by using the KafkaConnect custom resource: \n\n  ```sh\n  oc apply -f gitOps/services/kconnect/kafka-connect.yaml\n  # Verify cluster is ready\n  oc get kafkaconnect\n  ```\n\n## 6- Deploy the COS sink connector\n\n\n\n\n1. Create a new file named `kafka-cos-sink-connector.yaml` and past the following code in it.\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1alpha1\n  kind: KafkaConnector\n  metadata:\n    name: cos-sink-connector\n    labels:\n      eventstreams.ibm.com/cluster: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n  spec:\n    class: com.ibm.eventstreams.connect.cossink.COSSinkConnector\n    tasksMax: 1\n    config:\n      key.converter: org.apache.kafka.connect.storage.StringConverter\n      value.converter: org.apache.kafka.connect.storage.StringConverter\n      topics: TOPIC_NAME\n      cos.api.key: IBM_COS_API_KEY\n      cos.bucket.location: IBM_COS_BUCKET_LOCATION\n      cos.bucket.name: IBM_COS_BUCKET_NAME\n      cos.bucket.resiliency: IBM_COS_RESILIENCY\n      cos.service.crn: \"IBM_COS_CRM\"\n      cos.object.records: 5\n      cos.object.deadline.seconds: 5\n      cos.object.interval.seconds: 5\n  ```\n\n  where\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: is the name you gave previously to your Kakfa Connect cluster.\n   * `TOPIC_NAME`: is the name of the topic you created in IBM Event Streams at the beginning of this lab.\n   * `IBM_COS_API_KEY`: is your IBM Cloud Object Storage service credentials `apikey` value. Review first sections of this lab if you don't remember where and how to find this value.\n   * `IBM_COS_BUCKET_LOCATION`: is your IBM Cloud Object Storage bucket location. Review first sections of this lab if you don't remember where and how to find this value (it usually is in the form of something like `us-east` or `eu-gb` for example).\n   * `IBM_COS_RESILIENCY`: is your IBM Cloud Object Storage resiliency option. Review first sections of this lab if you don't remember where and how to find this value (it should be `regional`).\n   * `IBM_COS_CRM`: is your IBM Cloud Object Storage CRN. Review first sections of this lab if you don't remember where and how to find this value. It usually ends with a double `::` at the end of it. **IMPORTANT:** you might need to retain the double quotation marks here as the CRN has colons in it and may collide with yaml syntax.\n\n1. Apply the yaml which will create a `KafkaConnnector` custom resource behind the scenes and register/set up the IBM COS Sink Connector in your Kafka Connect cluster.\n\n  ```shell\n  oc apply -f kafka-cos-sink-connector.yaml\n  ```\n\n1. The initialization of the connector can take a minute or two. You can check the status of the connector to see if everything connected succesfully.\n\n  ```shell\n  oc describe kafkaconnector cos-sink-connector\n  ```\n\n1. When the IBM COS Sink connector is successfully up and running you should see something similar to the below.\n\n  ![IBM COS Sink Connector success](./images/ibm-cos-sink-connector-success.png)\n\n1. You should also see a new connector being registered if you exec into the Kafka Connect cluster pod and query for the existing connectors again:\n\n  ```shell\n  oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash\n  bash-4.4$ curl localhost:8083/connectors\n  [\"cos-sink-connector\"]\n  ```\n\n\n## 7- Test by sending some records\n\n1. Post 10 orders:\n\n  ```sh\n  curl -X POST  'http://$HOST/api/v1/start'   -H 'accept: application/json' -H 'Content-Type: application/json' -d '10'\n  ```\n\n1. Verify whether the messages from the IBM Event Streams topic are getting propagated to your IBM Cloud Object Storage bucket. \nIf you go to your IBM COS bucket, you should find some files in it. The name of the file inside the bucket has starting offset and ending offset. \nYou can download one of these object files to make sure that the value inside matches the value inside your `INBOUND` topic.\n\n  ![End to End Success](./images/ibm-cos-bucket-success.png)\n\n\n\n--- \n\n# Developer Corner\n\nThe https://github.com/ibm-cloud-architecture/eda-quickstarts.git cos-tutorial folder includes a microprofile reactive messaging producer app\nbasd on quarkus runtime.\n\nYou have two choices for this application, reuse the existing code as-is or start by using quarkus CLI.\n\n* Clone the `eda-quickstarts` repository.\n\n  ```bash\n  git clone  https://github.com/ibm-cloud-architecture/eda-quickstarts.git\n  cd cos-tutorial\n  quarkus dev\n  ```\n\n* If you want to start on your own, then create the Quarkus project.\n\n  ```bash\n  quarkus create app cos-tutorial \n  cd cos-tutorial \n  quarkus ext add  reactive-messaging-kafka, mutiny, openshift\n  ```\n\n  and get inspiration from the [DemoController.java](https://github.com/ibm-cloud-architecture/eda-quickstarts/blob/main/cos-tutorial/src/main/java/ibm/eda/demo/kafka/producer/DemoController.java).\n\n* Review the `DemoController.java` file.\n\n    * The `@Channel` annotation indicates that we're sending to a channel defined with reactive messaging in the application.properties.\n    * The `startDemo()` function generate n orders and send to the channel.\n\n* The reactive messaging is defined in the `application.properties`. The setting are using\nthe a kafka cluster named `dev`, we encourage to keep this name or you need to modify a lot of\nyaml files.\n* See the [COS tutorial README](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/cos-tutorial#readme) \nto run the producer application locally with docker compose or quarkus dev.\n","type":"Mdx","contentDigest":"2134f2dd6d9faf4baa617182e67d2ad8","owner":"gatsby-plugin-mdx","counter":941},"frontmatter":{"title":"Kafka Connect to IBM COS","description":"Apache Kafka to IBM Cloud Object Storage Source Connector usecase"},"exports":{},"rawBody":"---\ntitle: Kafka Connect to IBM COS\ndescription: Apache Kafka to IBM Cloud Object Storage Source Connector usecase\n---\n\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 02/22/2022</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Introduction</AnchorLink>\n  <AnchorLink>Use Case Guided Tour</AnchorLink>\n  <AnchorLink>Full Demo Narration</AnchorLink>\n  <AnchorLink>Developer Corner</AnchorLink>\n </AnchorLinks>\n\n---\n\n# Introduction\n\nOne of the classical use case for event driven solution based on Kafka is to keep the message for a longer time\nperiod than the retention time of the Kafka topic. This demonstration illustrates how to do so using IBM Event Streams\nand IBM Cloud Object Storage as a service.\n\nWe have created a very simple Quarkus (a super sonic and sub-atomic Kubernetes native framework for Java) application that \nuses MicroProfile Reactive Messaging in order to send a stream of data to our Event Streams/Kafka topic. \n\nThe Kafka Connect cluster includes a IBM Cloud Object Storage Connector to grab messages fomr the topoc and \nplace into an IBM COS Bucket.\n\n![Architecture Diagram](./images/quarkus-to-event-streams-to-cos.png)\n\n### Skill level\n\nThe skill level of this learning path is for a beginner.\n\n### Estimated time to complete\n\nIt will take you approximately 15 minuttes to complete this entire demonstration.\n\n### Scenario Prerequisites\n\n**OpenShift Container Platform Cluster**\n  - This scenario will assume you have a 4.7+ Cluster as we will make use of Operators.\n\n**Cloud Pak for Integration**\n  - Updated with 2021.0.3 release of the Cloud Pak for Integration installed on OpenShift. \nThis story will also assume you have followed the installation instructions for Event Streams\n outlined in [the 2021-3 product documentation](https://ibm.github.io/event-streams/installing/installing/) or used one of our [GitOps repository](https://github.com/ibm-cloud-architecture/eda-gitops-catalog) approach\n to deploy Event Streams Operator.\n\n**Git**\n  - We will need to clone repositories.\n\n**An IBM Cloud Account (free)**\n  - A free (Lite) IBM Cloud Object Storage trial Service account [IBM Cloud Object Storage](https://cloud.ibm.com/catalog/services/cloud-object-storage)\n\n__If you want to modify the code then you need:__\n\n**Java**\n  - Java Development Kit (JDK) v1.11+ (Java 11+)\n\n**Maven**\n  - The scenario uses Maven v3.6.3\n\n**Gradle**\n  - Ideally v4.0+ (Note - the gradle shadowJar command might not work on Java versions newer to Java 8)\n\n**An IDE of your choice**\n  - Visual Studio Code is used in this scenario.\n\n---\n\n# Use Case Guided Tour\n\nThe use case is quite simple, but very common. Topic has configuration to keep messages in the kafka broker disks for a certain time perior or until the log reach a certain size.\nWhich means records will be removed from the brokers over time. Some companies put retention time to a value to keep data foreever. This is possible when the amount\nof data in such topic is not reasonable. But most of the time we need to design solution to move data to longer persistence. S3 buckets is one of such\nlong term persistence as it will bring elasticity, transparent replication and geolocalization. \n\nAn infrastructure engineer needs to prepare the Cloud Object Storage service and organize the bucket strategy to keep data. Then on the OpenShift production\ncluster, Event Streams is deployed and Kafka connector cluster defined. The connector configuration will stipulate the COS connection credentials and\nthe Event Streams connection URL and credentials.\n\nOnce the connector is started all messages in the designated topics will go to the COS bucket. \n\nIBM Event Streams team has explanation of the Cloud Object Storage Sink Kafka connector configuration in [this repository](https://github.com/ibm-messaging/kafka-connect-ibmcos-sink).\n\n---\n\n# Full Demo Narration\n\n## 1- Create an IBM COS Service and COS Bucket\n\nIn this section, we are going to see how to create an IBM Cloud Obeject Storage (IBM COS) Service in your IBM Cloud account and a bucket within your \nIBM COS Service. We assume you already have an IBM Cloud account and, if not, you can sign up for one here at [IBM Cloud](https://cloud.ibm.com).\n\n1. Once you are inside your IBM Cloud account, traverse to the `Catalog` section. In the search type in `IBM Cloud Object Storage`\n\n  ![IBM COS Catalog Search](./images/ibm-cloud-create-cos-service.png)\n\n1. Name your IBM COS Service with something unique. Since this is a free account, we can stick with the `Lite Plan`.\n\n  ![IBM COS Create COS Service](./images/ibm-cloud-create-cos-service-2.png)\n\n1.  Now that the IBM Cloud Object Storage Service is created, create a new bucket. On the `Create Bucket` screen pick `Custom Bucket`.\n\n  ![IBM COS Custom Bucket](./images/ibm-cos-create-bucket.png)\n\n1. When selecting options for the bucket, name your bucket something unique. For `Resiliency` let's select `Regional`. \nFor location select an area from the drop-down that you want. Use `Standard` for `Storage Class`. \nLeave everything else as-is and hit `Create Bucket`.\n\n  ![IBM COS Custom Bucket Settings](./images/ibm-cos-bucket-settings.png)\n\n## 2- Create IBM COS Service Credentials\n\nNow that we have created our IBM Cloud Object Storage Service and bucket, we need to create the Service Credential so that Kafka connector can connect to it.\n\n1. Inside your IBM COS Service, select `Service Credentials` and then click the `New Credential` button.\n\n  ![IBM COS Service Credential](./images/ibm-cos-create-service-cred.png)\n\n1. Name your credential and select `Manager` from the `Role:` drop-down menu and click `Add`.\n\n  ![IBM COS SC Settings](./images/ibm-cos-service-credentials.png)\n\n1. Expand your newly created Service Credential and write down the values for `\"apikey\"` and `\"resource_instance_id\"`. \nYou will need this later in the [Build and Apply IBM COS Sink Connector](#build-and-apply-ibm-cos-sink-connector) section.\n\n  ![Expanded Service Cred](./images/ibm-service-credential-keys.png)\n\n## 3- Create a demo project\n\n\nTo isolate the demonstration in the OpenShift Cluster, we will deploy the demo code, event streams cluster and Kafka Connect in one project.\n\n1. Clone the tutorial project\n\n   ```sh\n   git clone https://github.com/ibm-cloud-architecture/eda-quickstarts\n   ```\n\n1. Create an OpenShift project (k8s namespace) named: `eda-cos`\n\n  ```sh\n  oc apply -k gitOps/env/base\n  ```\n\n## 4- Deploy Event Streams Cluster\n\nIf you do not have a Event Stream cluster already deployed on OpenShift, we propose to deploy one in a demo project, using Event Streams Operator monitoring All Namespaces\n\n1. Using [IBM entitled registry entitlement key](https://www.ibm.com/docs/en/cloud-paks/cp-integration/2020.2?topic=installation-entitled-registry-entitlement-keys) \n define your a secret so deployment process can download IBM Event Streams images:\n\n  ```sh\n  KEY=<yourentitlementkey>\n  oc create secret docker-registry ibm-entitlement-key \\\n      --docker-username=cp \\\n      --docker-password=$KEY \\\n      --docker-server=cp.icr.io \\\n      --namespace=eda-cos\n  ```\n\n1. Deploy Event Streams Cluster.\n  \n  ```sh\n  oc apply -k gitOps/services/es \n  # --> Results\n  eventstreams.eventstreams.ibm.com/dev created\n  kafkatopic.eventstreams.ibm.com/edademo-orders created\n  ```\n\n  It will take sometime to get the cluster created. Monitor with `oc get pod -w`. You should get:\n\n  ```\n  dev-entity-operator-6d7d94f68f-6lk86   3/3    \n  dev-ibm-es-admapi-ffd89fdf-x99lq       1/1    \n  dev-ibm-es-ui-74bf84dc67-qx9kk         2/2    \n  dev-kafka-0                            1/1    \n  dev-zookeeper-0                        1/1\n  ```\n\n  With this deployment there is no external route, only on bootstrap URL: `dev-kafka-bootstrap.eda-cos.svc:9092`. The Kafka listener is using PLAINTEXT connection. So no SSL encryption and no authentication.\n\n1. Deploy the existing application (the image we built is in quay.io/ibmcase) using:\n\n  ```sh\n  oc apply -k gitOps/apps/eda-cos-demo/base/\n  ```\n\n1. Test the deployed app by accessing its route and GET API: \n\n  ```sh\n  HOST=$(oc get routes eda-cos-demo  -o jsonpath='{.spec.host}')\n  curl -X GET http://$HOST/api/v1/version\n  ```\n\n## 5- Deploy Kafka Connector\n\nWe have already built a kafka connect image with the Cloud Object Storage jar and push it as image to quay.io.\n\n1. Deploy the connector cluster by using the KafkaConnect custom resource: \n\n  ```sh\n  oc apply -f gitOps/services/kconnect/kafka-connect.yaml\n  # Verify cluster is ready\n  oc get kafkaconnect\n  ```\n\n## 6- Deploy the COS sink connector\n\n\n\n\n1. Create a new file named `kafka-cos-sink-connector.yaml` and past the following code in it.\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1alpha1\n  kind: KafkaConnector\n  metadata:\n    name: cos-sink-connector\n    labels:\n      eventstreams.ibm.com/cluster: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n  spec:\n    class: com.ibm.eventstreams.connect.cossink.COSSinkConnector\n    tasksMax: 1\n    config:\n      key.converter: org.apache.kafka.connect.storage.StringConverter\n      value.converter: org.apache.kafka.connect.storage.StringConverter\n      topics: TOPIC_NAME\n      cos.api.key: IBM_COS_API_KEY\n      cos.bucket.location: IBM_COS_BUCKET_LOCATION\n      cos.bucket.name: IBM_COS_BUCKET_NAME\n      cos.bucket.resiliency: IBM_COS_RESILIENCY\n      cos.service.crn: \"IBM_COS_CRM\"\n      cos.object.records: 5\n      cos.object.deadline.seconds: 5\n      cos.object.interval.seconds: 5\n  ```\n\n  where\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: is the name you gave previously to your Kakfa Connect cluster.\n   * `TOPIC_NAME`: is the name of the topic you created in IBM Event Streams at the beginning of this lab.\n   * `IBM_COS_API_KEY`: is your IBM Cloud Object Storage service credentials `apikey` value. Review first sections of this lab if you don't remember where and how to find this value.\n   * `IBM_COS_BUCKET_LOCATION`: is your IBM Cloud Object Storage bucket location. Review first sections of this lab if you don't remember where and how to find this value (it usually is in the form of something like `us-east` or `eu-gb` for example).\n   * `IBM_COS_RESILIENCY`: is your IBM Cloud Object Storage resiliency option. Review first sections of this lab if you don't remember where and how to find this value (it should be `regional`).\n   * `IBM_COS_CRM`: is your IBM Cloud Object Storage CRN. Review first sections of this lab if you don't remember where and how to find this value. It usually ends with a double `::` at the end of it. **IMPORTANT:** you might need to retain the double quotation marks here as the CRN has colons in it and may collide with yaml syntax.\n\n1. Apply the yaml which will create a `KafkaConnnector` custom resource behind the scenes and register/set up the IBM COS Sink Connector in your Kafka Connect cluster.\n\n  ```shell\n  oc apply -f kafka-cos-sink-connector.yaml\n  ```\n\n1. The initialization of the connector can take a minute or two. You can check the status of the connector to see if everything connected succesfully.\n\n  ```shell\n  oc describe kafkaconnector cos-sink-connector\n  ```\n\n1. When the IBM COS Sink connector is successfully up and running you should see something similar to the below.\n\n  ![IBM COS Sink Connector success](./images/ibm-cos-sink-connector-success.png)\n\n1. You should also see a new connector being registered if you exec into the Kafka Connect cluster pod and query for the existing connectors again:\n\n  ```shell\n  oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash\n  bash-4.4$ curl localhost:8083/connectors\n  [\"cos-sink-connector\"]\n  ```\n\n\n## 7- Test by sending some records\n\n1. Post 10 orders:\n\n  ```sh\n  curl -X POST  'http://$HOST/api/v1/start'   -H 'accept: application/json' -H 'Content-Type: application/json' -d '10'\n  ```\n\n1. Verify whether the messages from the IBM Event Streams topic are getting propagated to your IBM Cloud Object Storage bucket. \nIf you go to your IBM COS bucket, you should find some files in it. The name of the file inside the bucket has starting offset and ending offset. \nYou can download one of these object files to make sure that the value inside matches the value inside your `INBOUND` topic.\n\n  ![End to End Success](./images/ibm-cos-bucket-success.png)\n\n\n\n--- \n\n# Developer Corner\n\nThe https://github.com/ibm-cloud-architecture/eda-quickstarts.git cos-tutorial folder includes a microprofile reactive messaging producer app\nbasd on quarkus runtime.\n\nYou have two choices for this application, reuse the existing code as-is or start by using quarkus CLI.\n\n* Clone the `eda-quickstarts` repository.\n\n  ```bash\n  git clone  https://github.com/ibm-cloud-architecture/eda-quickstarts.git\n  cd cos-tutorial\n  quarkus dev\n  ```\n\n* If you want to start on your own, then create the Quarkus project.\n\n  ```bash\n  quarkus create app cos-tutorial \n  cd cos-tutorial \n  quarkus ext add  reactive-messaging-kafka, mutiny, openshift\n  ```\n\n  and get inspiration from the [DemoController.java](https://github.com/ibm-cloud-architecture/eda-quickstarts/blob/main/cos-tutorial/src/main/java/ibm/eda/demo/kafka/producer/DemoController.java).\n\n* Review the `DemoController.java` file.\n\n    * The `@Channel` annotation indicates that we're sending to a channel defined with reactive messaging in the application.properties.\n    * The `startDemo()` function generate n orders and send to the channel.\n\n* The reactive messaging is defined in the `application.properties`. The setting are using\nthe a kafka cluster named `dev`, we encourage to keep this name or you need to modify a lot of\nyaml files.\n* See the [COS tutorial README](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/cos-tutorial#readme) \nto run the producer application locally with docker compose or quarkus dev.\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-cos/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}