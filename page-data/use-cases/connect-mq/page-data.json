{"componentChunkName":"component---src-pages-use-cases-connect-mq-index-mdx","path":"/use-cases/connect-mq/","result":{"pageContext":{"frontmatter":{"title":"Kafka to IBM MQ with Kafka Connector","description":"Apache Kafka to IBM MQ with Kafka Connector labs"},"relativePagePath":"/use-cases/connect-mq/index.mdx","titleType":"append","MdxNode":{"id":"80de79b5-1f3d-5caa-a4bd-4ce12688931f","children":[],"parent":"dde4ad41-4f85-5292-bbf6-28908e3c6392","internal":{"content":"---\ntitle: Kafka to IBM MQ with Kafka Connector\ndescription: Apache Kafka to IBM MQ with Kafka Connector labs \n---\n\nThis extended scenario supports different labs going from simple to more complex and \naddresses how to integrate IBM MQ with Event Streams Kafka as part of Cloud Pak for Integration\nusing Kafka Connect with IBM MQ Kafka Connectors. \n\nTo run the same Lab with Kafka Confluent see the [ibm-cloud-architecture/eda-lab-mq-to-kafka readme file.](https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka)\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites to all labs</AnchorLink>\n<AnchorLink>Lab 1: MQ source to Event Streams Using the Admin Console</AnchorLink>\n<AnchorLink>Lab 2: MQ source to Event Streams using GitOps</AnchorLink>\n<AnchorLink>Lab 3: MQ Sink from Kafka</AnchorLink>\n<AnchorLink>Lab 4: MQ Connector with Kafka Confluent</AnchorLink>\n</AnchorLinks>\n\n## Audience\n\nWe assume readers have good knowledge of OpenShift to login, to naivgate into the Administrator\nconsole and use OC and GIT CLIs.\n\n## Pre-requisites to all labs\n\n* Access to an OpenShift Cluster and Console\n* Login to the OpenShift Console and get the `access token` to use `oc login`.\n* Access to [git cli](https://git-scm.com/) from your workstation\n* Clone Lab repository\n\n```sh\ngit clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka.git\n```    \n\n## Lab 1: MQ source to Event Streams Using the Admin Console\n\nIn this lab we assume Cloud Pak for Integration is deployed with Event Streams cluster created and at least one MQ Broker created.\nIf not see [this note to install Event Streams operator and defining one Kafka cluster](/technology/event-streams/es-cp4i#install-event-streams-using-openshift-console) using the OpenShift Console\nand [this note to install one MQ Operator and one MQ Broker](/technology/mq/#installation-with-cloud-pak-for-integration). \n\nThe following figure illustrates what we will deploy:\n\n![](./images/mq-kafka-lab1.png)\n\n* The Sell Store simulator is a separate application available in [the refarch-eda-store-simulator public github](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) and \nthe docker image is accessible from the [quay.io registry](https://quay.io/repository/ibmcase/eda-store-simulator).\n* The Event Streams cluster is named `dev`\n\n### Get setup\n\n* Get Cloud Pak for Integration `admin` user password\n\n   ```sh\n   oc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' -n ibm-common-services | base64 --decode && echo \"\"\n   ```\n* Access to the Event Streams console\n   \n   ```sh\n   oc get routes\n   # \n   oc get route dev-ibm-es-ui -o jsonpath='{.spec.host}' && echo \"\"\n   ```\n   \n   Use this URL to access to the Event Stream console:\n\n   ![](./images/es-console.png)\n\n* Using the Topic menu, create the `items` topic with 1 partition, use default retention time, and 3 replicas\n* Get [Internal Kafka bootstrap URL](/technology/event-streams/es-cp4i#get-kafka-bootstrap-url)\n\n   ![](./images/internal-url.png)\n\n   and generate TLS credentials with the  name `tls-mq-user` with the `Produce messages, consume messages and create topics and schemas` permissions, \n   \n   ![](./images/tls-mq-user.png)\n   \n   on `items` topic:\n\n   ![](./images/items-topic.png)\n\n   All consumer groups, as we may reuse this user for consumers.\n  \n   You can download the certificates, but in fact it is not necessary as we will use the created secrets to configure MQ Kafka connector.\n\n* Get MQ Console route from the namespace where MQ brokers run\n\n  ```sh \n  oc get routes -n cp4i | grep mq-web\n  ```\n\n* Access to MQ Broker Console > Manage to see the Broker configured \n\n   ![](./images/mq-manage.png)\n\n* Add the local `items` queue \n\n   ![](./images/create-queue.png)\n\n   ![](./images/create-queue-2.png)\n\n* Verify MQ App channels defined\n\n  ![](./images/mq-app-channel.png)\n\n* Create an OpenShift project named: `mq-es-demo` with the command: `oc new-project mq-es-demo`\n* Clone the git `ibm-cloud-architecture/eda-lab-mq-to-kafka` repository:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka\n ```\n\n* Update the configMap named `store-simulator-cm` in folder `kustomize/apps/store-simulator/overlays` for the store simulator: `Workloads > ConfigMaps > Create` and use \nthe parameters from MQ_HOST and MQ_CHANNEL.\n\n   ```yaml\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: store-simulator-cm\n    data:\n      APP_VERSION: 0.0.6\n      APP_TARGET_MESSAGING: IBMMQ\n      MQ_HOST: store-mq-ibm-mq.cp4i.svc\n      MQ_PORT: \"1414\"\n      MQ_QMGR: QM1 \n      MQ_QUEUE_NAME: items\n      MQ_CHANNEL: DEV.APP.SVRCONN\n   ```\n\n### Deploy Store simulator\n\n* Deploy to the OpenShift project\n\n   ```sh\n   oc project mq-es-demo\n   oc apply -k  kustomize/apps/store-simulator/overlays\n   ```\n\n* Access to the application web interface:\n\n  ```sh\n   oc get route store-simulator -o jsonpath='{.spec.host}' && echo \"\"\n  ```\n\n![](./images/)\n\n### Deploy Kafka Connect\n\nThe Event Streams MQ Source connector [product documentation](https://ibm.github.io/event-streams/connecting/mq/source/) describes\nwhat need to be done in details. Basically we have to do two things:\n\n* Build the connector image with the needed jars for all the connectos we want to use. For example we need\njar files for the MQ connector and jars for the MQ client api.\n* Start a specific connector with the parameters to connect to the external system and to kafka.\n\nTo make the MQ source connector immediatly useable for the demonstration, we have build a connector image at `quay.io/ibmcase/demomqconnect` and the kafkaconnect.yaml is\nin [this file](https://raw.githubusercontent.com/ibm-cloud-architecture/store-mq-gitops/main/environments/smq-dev/apps/services/kafkaconnect/kafka-connect.yaml).\n\nTo deploy the kafka connect framework do:\n\n```sh\noc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/kafka-connect.yaml\n```\n\nThen start the MQ source connector:\n\n```sh\noc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/mq-src-connector.yaml\n```\n\n\n### Demonstration scripts\n\n1. The solution has multiple stores as presented in the `stores` menu. \n\n  ![](./images/stores-view.png)\n\n1. In the `simulator` menu, select the IBMMQ backend, and send some messages randomly using the \nleft button after selecting the number of messages to send or run the \npredefined scenario with the right button:\n\n  ![](./images/demo-scenarios.png)\n\n1. In MQ Broker Console go to the `items` queue to see the messages generated from the simulator\n\n  ![](./images/mq-msgs.png)\n  \n1. In Event Streams console goes to the `items` topic to see the same messages produced by the Kafka MQ Source connector.\n\n## Lab 2: MQ source to Event Streams using GitOps\n\nThis labs uses GitOps approach with OpenShift GitOps product (ArgoCD) to deploy IBM Event Streams, MQ,\nthe Store Simulator app, and Kafka Connect in the minimum of work. It can be used\non a OpenShift Cluster witout any previously deployed Cloud Pak for Integration \nopersators. Basically as a SRE you will jumpstart the GitOps operator and then starts ArgoCD.\n\n* Clone the `store-mq-gitops` repo\n\n  ```shell\n  git clone https://github.com/ibm-cloud-architecture/store-mq-gitops\n  ```\n* Follow the instructions from the repository main Readme file.\n\n## Lab 3: MQ Sink from Kafka\n\nA may be less used Kafka connector will be to use the MQ Sink connector to get data\nfrom Kafka to MQ. This lab addresses such integration. The target deployment looks like in the following diagram:\n\n ![1](./images/all-ocp.png)\n\nKafka is runnning in its own namespace, and we are using Event Streams from Cloud Pack for Integration.\n\n### Setup MQ Channel\n\nOpen a shell on the remote container or use the user interface to define the communication channel to use for the Kafka Connection.\n\n* Using CLI: Change the name of the Q manager to reflect what you defined in MQ configuration.\n\n```shell\nrunmqsc EDAQMGR1\n# Define a app channel using server connection\nDEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\n# Set the channel authentication rules to accept connections requiring userid and password\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\n# Set identity of the client connections\nALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\n\nREFRESH SECURITY TYPE(CONNAUTH)\n# Define inventory queue\nDEFINE QLOCAL(INVENTORY)\n# Authorize the IBM MQ user ID to connect to and inquire the queue manager\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\n# Authorize the IBM MQ user ID to use the queue:\nSET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\n# done\nEND\n```\n\n* As an alternate you can use the MQ administration console.\n\nTBD\n\n### Setup Kafka Connect Cluster\n\nConnectors can be added to a Kafka Connect environment using OpenShift CLI commands and the source to image customer resource. We will use the Strimzi operator console to setup kafka connect environment. \n\nOnce you downloaded the zip file, which is a yaml manifest, define the configuration for a KafkaConnectS2I instance. The major configuration settings are the server certificate settings and the authentication using Mutual TLS authentication, something like:\n\n```yaml\nspec:\n  bootstrapServers: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n  tls:\n    trustedCertificates:\n    - secretName: sandbox-rp-cluster-ca-cert\n      certificate: ca.crt\n  authentication:\n    type: tls\n    certificate: user.crt\n    key: user.key\n    secretName: sandbox-rp-tls-cred\n```\n\nIf you change the name of the connect cluster in the metadata, modify also the name: `spec.template.pod.metadata.annotations. productChargedContainers` accordingly.\n\nThe secrets used above, need to be accessible from the project where the connector is deployed. The simple way to do so is to copy the source certificates from the Event streams project to your current project with the commands like:\n\n```shell\noc get secret  sandbox-rp-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f -\noc get secret  sandbox-rp-tls-cred  -n eventstreams --export -o yaml | oc apply -f -\n```\n\nIf you do not have a TLS client certificate from a TLS user, use [this note](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to create one.\n\n* Deploy the connector cluster\n\n```shell\noc apply -f kafka-connect-s2i.yaml \n```\n\nAn instance of this custom resource represents a Kafka Connect distributed worker cluster. In this mode, workload balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. Each connector is represented by another custom resource called KafkaConnector.\n\n```shell\noc describe kafkaconnects2i eda-connect-cluster\n```\n\n### Add the mq-sink connector  \n\nThe [product documentation](https://ibm.github.io/event-streams/connecting/mq/) details the available MQ connectors and the configuration process. Using the event streams console, the process is quite simple to get a connector configuration as json file. Here is an example of the final form to generate the json file:\n\n ![6](./images/es-mq-sink.png)\n\n* Once the json is downloaded, complete the settings\n\n```json\n{\n  \"name\": \"mq-sink\",\n  \"config\":\n  {\n      \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n      \"tasks.max\": \"1\",\n      \"topics\": \"inventory\",\n      \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"mq.queue.manager\": \"EDAQMGR1\",\n      \"mq.connection.name.list\": \"eda-mq-lab-ibm-mq(1414)\",\n      \"mq.user.name\": \"admin\",\n      \"mq.password\": \"passw0rd\",\n      \"mq.user.authentication.mqcsp\": true,\n      \"mq.channel.name\": \"KAFKA.CHANNEL\",\n      \"mq.queue\": \"INVENTORY\",\n      \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n  }\n}\n```\n\n* To run the connector within the cluster, we need to connector jar. You can download this jar file from the Event Stream adming console `> Toolkit > Add connector to your Kafka Connect environment > Add Connector > IBM MQ Connectors`,\n\n ![](./images/es-mq-sink-1.png)\n\nor as an alternate, we [cloned the mq-sink code](https://github.com/ibm-messaging/kafka-connect-mq-sink.git), so a `mvn package` command under `kafka-connect-mq-sink` folder will build the jar. Copy this jar under `cp4i/my-plugins` folder. \n* Build the connector with source to image component.\n\n\nWith the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured.  You will see signs of success in the container output (via oc logs, or in the UI):\n\n```shell\n+ curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json\n...\n{\"name\":\"mq-sink-connector\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"mq-service(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink-connector\"},\"tasks\":[{\"connector\":\"mq-sink-connector\",\"task\":0}],\"type\":\"sink\"}\n...\n[2020-06-23 04:26:26,054] INFO Creating task mq-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:419)\n...[2020-06-23 04:26:26,449] INFO Connection to MQ established (com.ibm.eventstreams.connect.mqsink.JMSWriter:229)\n[2020-06-23 04:26:26,449] INFO WorkerSinkTask{id=mq-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)\n```\n\nYou should now have the Kafka Connector MQ Sink running on OpenShift.\n\n\n## MQ Sink Connector on virtual or baremetal server, MQ and Event Streams on IBM Cloud\n\nIn this second option, we are using our own laptop for the baremetal dedployment, but the current solution will work the same on virtual server.\n\n ![2](./images/hybrid.png)\n\n### Pre-requisites\n\nWe assume that you have an instance of IBM Event Streams already running on IBM Cloud or on OpenShift with at least administrator credentials created. The credentials will be needed later on for configuring the Kafka Connect framework to be able to connect and work with your IBM Event Streams instance. Also, this scenario requires a topic called `inventory` in your IBM Event Streams instance. For gathering the credentials and creating the topic required for this scenario, please review the [Common pre-requisites](/use-cases/overview/pre-requisites/). **IMPORTANT:** if you are sharing your IBM Event Streams instance, append some unique identifier to the topic you create in IBM Event Streams.\n\n### Create Local IBM MQ Instance\n\nIn this section we are going to use Docker to create a local IBM MQ instance to simulate an IBM MQ instance somewhere in our datacenter.\n\n1. Create a data directory to mount onto the container.\n\n  ```shell\n  mkdir qm1data\n  ```\n\n1. Run the IBM MQ official Docker image by execting the following command.\n\n  ```shell\n  docker run                     \\\n    --name mq                    \\\n    --detach                     \\\n    --publish 1414:1414          \\\n    --publish 9443:9443          \\\n    --publish 9157:9157          \\\n    --volume qm1data:/mnt/mqm    \\\n    --env LICENSE=accept         \\\n    --env MQ_QMGR_NAME=QM1       \\\n    --env MQ_APP_PASSWORD=admin  \\\n    --env MQ_ENABLE_METRICS=true \\\n    ibmcom/mq\n  ```\n  where we can see that out container will be called `mq`, it will run in `detached` mode (i.e. in the background), it will expose the ports IBM MQ uses for communication and the image we are actually running `ibmcom/mq`.\n\n1. You could make sure your IBM MQ Docker image is running by listing the Docker containers in your workstation.\n\n  ```shell\n  $ docker ps\n  CONTAINER ID   IMAGE       COMMAND            CREATED         STATUS         PORTS                                                                    NAMES\n  a7b2a115a3c6   ibmcom/mq   \"runmqdevserver\"   6 minutes ago   Up 6 minutes   0.0.0.0:1414->1414/tcp, 0.0.0.0:9157->9157/tcp, 0.0.0.0:9443->9443/tcp   mq\n  ```\n\n1. You should also be able to log into the MQ server on port 9443 (<https://localhost:9443>) with default user `admin` and password `passw0rd`.\n\n  ![login](./images/login.png)\n\nNow, we need to configure our local IBM MQ instance Queue Manager in order to define a server connection (`KAFKA.CHANNEL`) with authentication (user `admin`, password `admin`) and the queue (`INVENTORY`) where our messages from Kafka will be sinked to. And we are going to so it using the IBM MQ CLI.\n\n1. Get into the IBM MQ Docker container we have started above by executing the following command that will give us a `bash` interactive terminal:\n\n  ```shell\n  docker exec -ti mq bash\n  ```\n\n1. Now that we are in the container, start the queue manager `QM1` by executing:\n\n  ```shell\n  strmqm QM1\n  ```\n\n1. Start the `runmqsc` tool to configure the queue manager by executing:\n\n  ```shell\n  runmqsc QM1\n  ```\n\n1. Create a server-connection channel called `KAFKA.CHANNEL` by executing:\n\n  ```shell\n  DEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\n  ```\n\n1. Set the channel authentication rules to accept connections requiring userid and password by executing:\n\n  ```shell\n  SET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\n  SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\n  SET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\n  ```\n\n1. Set the identity of the client connections based on the supplied context, the user ID by executing:\n\n  ```shell\n  ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\n  ```\n\n1. Refresh the connection authentication information by executing:\n\n  ```shell\n  REFRESH SECURITY TYPE(CONNAUTH)\n  ```\n\n1. Create the `INVENTORY` queue for the connector to use by executing:\n\n  ```shell\n  DEFINE QLOCAL(INVENTORY)\n  ```\n\n1. Authorize `admin` to connect to and inquire the queue manager by executing:\n\n  ```shell\n  SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\n  ```\n\n1. Finally authorize `admin` to use the queue by executing:\n\n  ```shell\n  SET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\n  ```\n\n1. End `runmqsc` by executing:\n\n  ```shell\n  END\n  ```\n\nYou should see the following output:\n\n```shell\n9 MQSC commands read.\nNo commands have a syntax error.\nOne valid MQSC command could not be processed.\n```\n\nExit the container by executing:\n\n```shell\nexit\n```\n\nIf you check your IBM MQ dashboard you should see your newly created `INVENTORY` queue:\n\n  ![inventory](./images/inventory_queue.png)\n\n### Create MQ Kafka Connector Sink\n\nThe MQ Connector Sink can be downloaded from this [Github](https://github.com/ibm-messaging/kafka-connect-mq-sink). The Github site includes exhaustive instructions for further detail on this connector and its usage.\n\n1. Clone the repository with the following command:\n\n  ```shell\n  git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git\n  ```\n\n1. Change directory into the `kafka-connect-mq-sink` directory:\n\n  ```shell\n  cd kafka-connect-mq-sink\n  ```\n\n1. Build the connector using Maven:\n\n  ```shell\n  mvn clean package\n  ```\n\n1. Create a directory (if it does not exist yet) to contain the Kafka Connect framework configuration and cd into it.\n\n  ```shell\n  mkdir config\n  cd config\n  ```\n\n1. Create a configuration file called `connect-distributed.properties` for the Kafka Connect framework with the following properties in it:\n\n  ```properties\n  # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\n  bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  ssl.enabled.protocols=TLSv1.2\n  ssl.protocol=TLS\n  ssl.truststore.location=/opt/kafka/config/es-cert.p12\n  ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n  ssl.truststore.type=PKCS12\n  security.protocol=SASL_SSL\n  sasl.mechanism=SCRAM-SHA-512\n  sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n  # Consumer side configuration\n  consumer.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  consumer.security.protocol=SASL_SSL\n  consumer.ssl.protocol=TLSv1.2\n  consumer.ssl.truststore.location=/opt/kafka/config/es-cert.p12\n  consumer.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n  consumer.ssl.truststore.type=PKCS12\n  consumer.sasl.mechanism=SCRAM-SHA-512\n  consumer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n  # Producer Side\n  producer.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  producer.security.protocol=SASL_SSL\n  producer.ssl.protocol=TLSv1.2\n  producer.ssl.truststore.location=/opt/kafka/config/es-cert.p12\n  producer.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n  producer.ssl.truststore.type=PKCS12\n  producer.sasl.mechanism=SCRAM-SHA-512\n  producer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n  plugin.path=/opt/kafka/libs\n\n  # unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\n  group.id=mq-sink-cluster-REPLACE_WITH_UNIQUE_IDENTIFIER\n\n  # The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n  # need to configure these based on the format they want their data in when loaded from or stored into Kafka\n  key.converter=org.apache.kafka.connect.json.JsonConverter\n  value.converter=org.apache.kafka.connect.json.JsonConverter\n  # Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n  # it to\n  key.converter.schemas.enable=true\n  value.converter.schemas.enable=true\n\n  # Topic to use for storing offsets.\n  offset.storage.topic=connect-offsets-REPLACE_WITH_UNIQUE_IDENTIFIER\n  offset.storage.replication.factor=3\n  #offset.storage.partitions=25\n\n  # Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic.\n  config.storage.topic=connect-configs-REPLACE_WITH_UNIQUE_IDENTIFIER\n  config.storage.replication.factor=3\n\n  # Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\n  status.storage.topic=connect-status-REPLACE_WITH_UNIQUE_IDENTIFIER\n  status.storage.replication.factor=3\n  status.storage.partitions=5\n\n  # Flush much faster than normal, which is useful for testing/debugging\n  offset.flush.interval.ms=10000\n  ```\n\n  **IMPORTANT:** You must replace **all occurences** of the following placeholders in the properties file above with the appropriate values for the Kafka Connect framework to work with your IBM Event Streams instance:\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_UNIQUE_IDENTIFIER`: A unique identifier so that the resources your kafka connect cluster will create on your IBM Event Streams instance don't collide with other users' resources _(Note: there are 4 placeholders of this type. Replace them all)._ \n    \n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n1. Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. **IMPORTANT:** download the PKCS12 certificate. How to get the certificate in the [Common pre-requisites](/use-cases/overview/pre-requisites/) section. \n\n1. Create a log4j configuration file named `connect-log4j.properties` based on the template below.\n\n  ```properties\n  log4j.rootLogger=DEBUG, stdout\n\n  log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n  log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n  log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n\n  log4j.logger.org.apache.kafka=INFO\n  ```\n\n1. Back out of the `config` directory to the `kafka-connect-mq-sink` directory:\n\n  ```shell\n  cd ..\n  ```\n\n2. Build the Docker image for your Kafka Connect framework that will contain the IBM MQ Sink connector and all the properties files you have created and tailored earlier so that your Kafka Connect framework can work with your IBM Event Streams instance we have set up previously in this exercise (_mind the dot at the end of the command. It is necessary_):\n\n  ```shell\n  docker build -t kafkaconnect-with-mq-sink:1.3.0 .\n  ```\n\n1. Run the Kafka Connect MQ Sink container.\n\n  ```\n  docker run                                 \\\n    --name mq-sink                           \\\n    --detach                                 \\\n    --volume $(pwd)/config:/opt/kafka/config \\\n    --publish 8083:8083                      \\\n    --link mq:mq                             \\\n    kafkaconnect-with-mq-sink:1.3.0\n  ```\n\n1. Now that we have a Kafka Connect framework running with the configuration to connect to our IBM Event Streams instance and the IBM MQ Sink Connector jar file in it, is to create a JSON file called `mq-sink.json` to startup an instance of the IBM MQ Sink connector in our Kafka Connect framework with the appropriate config to work read messages from the IBM Event Streams topic we desire and sink those to the IBM MQ instance we have running locally. **IMPORTANT:** If you are sharing your IBM Event Streams instance and followed the instructions on this readme, you should have appended a unique identifier to the name of the topic (`inventory`) that you are meant to create in IBM Event Streams. As a result, modify the line `\"topics\": \"inventory\"` in the following JSON object accordignly.\n\n   ```json\n   {\n      \"name\": \"mq-sink\",\n      \"config\":\n      {\n          \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n          \"tasks.max\": \"1\",\n          \"topics\": \"inventory\",\n\n          \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n          \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n          \"mq.queue.manager\": \"QM1\",\n          \"mq.connection.name.list\": \"mq(1414)\",\n          \"mq.user.name\": \"admin\",\n          \"mq.password\": \"passw0rd\",\n          \"mq.user.authentication.mqcsp\": true,\n          \"mq.channel.name\": \"KAFKA.CHANNEL\",\n          \"mq.queue\": \"INVENTORY\",\n          \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n      }\n   }\n   ```\n\n1. Last piece of the puzzle is to tell our Kafka Connect framework to create and startup the IBM MQ Sink connector based on the configuration we have in the previous json file. To do so, execute the following `POST` request.\n\n  ```Shell\n  curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./mq-sink.json\"\n\n  # The response returns the metadata about the connector\n  {\"name\":\"mq-sink\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"ibmmq(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink\"},\"tasks\":[{\"connector\":\"mq-sink\",\"task\":0}],\"type\":\"sink\"}\n  ```\n\n1. You can query your Kafka Connect framework to make sure of this. Execute the following command that should return all the connectors up and running in your Kafka Connect cluster.\n\n  ```shell\n  curl localhost:8083/connectors\n  [\"mq-sink\"]\n  ```\n\nYou should now have a working MQ Sink connector getting messages from your topic in IBM Event Streams and sending these to your local IBM MQ instance. In order to check that out working, we first need to send few messages to that IBM Event Streams topic and then check out our `INVENTORY` queue in our local IBM MQ instance.\n\n1. In order to send messages to IBM Event Streams, we are going to use the IBM Event Streams Starter application. You can find the instruction either in the IBM Event Streams official documentation [here] or on the IBM Event Streams dashboard:\n\n  ![starter](./images/starter-app.png)\n\n1. Follow the instructions to run the IBM Event Streams starter application from your workstation. **IMPORTANT:** When generating the properties for your IBM Event Streams starter application, please choose to connect to an existing topic and select the topic you created previously as part of this exercise so that the messages we send into IBM Event Streams end up in the appropriate topic that is being monitored by your IBM MQ Sink connector runnning on the Kafka Connect framework.\n\n  ![starter-config](./images/starter-config.png)\n\n1. Once you have your application running, open it up in your web browser, click on `Start producing`, let the application produce a couple of messages and then click on `Stop producing`\n\n  ![starter-messages](./images/starter-messages.png)\n\n1. Check out your those messages got into the Kafka topic\n\n  ![kafka-messages](./images/kafka-messages.png)\n\n1. Check out your messages in the Kafka topic have already reached your `INVENTORY` queue in your local IBM MQ instance\n\n  ![mq-messages](./images/mq-message.png)\n\n\n1. You could also inspect the logs of your Kafka Connect Docker container running on your workstation:\n\n  ```shell\n  docker logs mq-sink\n  ...\n  [2021-01-19 19:44:17,110] DEBUG Putting record for topic inventory, partition 0 and offset 0 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n  [2021-01-19 19:44:17,110] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n  [2021-01-19 19:44:18,292] DEBUG Flushing up to topic inventory, partition 0 and offset 1 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n  [2021-01-19 19:44:18,292] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 636: {inventory-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:346)\n  [2021-01-19 19:44:18,711] DEBUG Putting record for topic inventory, partition 0 and offset 1 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n  [2021-01-19 19:44:18,711] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n  [2021-01-19 19:44:20,674] DEBUG Putting record for topic inventory, partition 0 and offset 2 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n  [2021-01-19 19:44:20,675] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n  [2021-01-19 19:44:28,293] DEBUG Flushing up to topic inventory, partition 0 and offset 3 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n  [2021-01-19 19:44:28,293] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 637: {inventory-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:346)\n  [2021-01-19 19:44:38,296] DEBUG Flushing up to topic inventory, partition 0 and offset 3 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n  ```\n\nTo cleanup your environment, you can do\n\n1. Remove the connector from your Kafka Connect framework instance (this isn't really needed if you are going to stop and remove the Kafka Connect Docker container)\n\n  ```shell\n  curl -X DELETE http://localhost:8083/connectors/mq-sink\n  ```\n\n1. Stop both IBM MQ and Kakfa Connect Docker containers running on your workstation\n\n  ```shell\n  docker stop mq mq-sink\n  ```\n\n1. Remove both IBM MQ and Kakfa Connect Docker containers from your workstation\n\n  ```shell\n  docker rm mq mq-sink\n  ```\n\n\n## Lab 4: MQ Connector with  Kafka Confluent\n\n* Clone the git `ibm-cloud-architecture/eda-lab-mq-to-kafka` repository and follow the readme.\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka\n ```","type":"Mdx","contentDigest":"ee6feefee59fcc544f485a55a4b03607","owner":"gatsby-plugin-mdx","counter":955},"frontmatter":{"title":"Kafka to IBM MQ with Kafka Connector","description":"Apache Kafka to IBM MQ with Kafka Connector labs"},"exports":{},"rawBody":"---\ntitle: Kafka to IBM MQ with Kafka Connector\ndescription: Apache Kafka to IBM MQ with Kafka Connector labs \n---\n\nThis extended scenario supports different labs going from simple to more complex and \naddresses how to integrate IBM MQ with Event Streams Kafka as part of Cloud Pak for Integration\nusing Kafka Connect with IBM MQ Kafka Connectors. \n\nTo run the same Lab with Kafka Confluent see the [ibm-cloud-architecture/eda-lab-mq-to-kafka readme file.](https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka)\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites to all labs</AnchorLink>\n<AnchorLink>Lab 1: MQ source to Event Streams Using the Admin Console</AnchorLink>\n<AnchorLink>Lab 2: MQ source to Event Streams using GitOps</AnchorLink>\n<AnchorLink>Lab 3: MQ Sink from Kafka</AnchorLink>\n<AnchorLink>Lab 4: MQ Connector with Kafka Confluent</AnchorLink>\n</AnchorLinks>\n\n## Audience\n\nWe assume readers have good knowledge of OpenShift to login, to naivgate into the Administrator\nconsole and use OC and GIT CLIs.\n\n## Pre-requisites to all labs\n\n* Access to an OpenShift Cluster and Console\n* Login to the OpenShift Console and get the `access token` to use `oc login`.\n* Access to [git cli](https://git-scm.com/) from your workstation\n* Clone Lab repository\n\n```sh\ngit clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka.git\n```    \n\n## Lab 1: MQ source to Event Streams Using the Admin Console\n\nIn this lab we assume Cloud Pak for Integration is deployed with Event Streams cluster created and at least one MQ Broker created.\nIf not see [this note to install Event Streams operator and defining one Kafka cluster](/technology/event-streams/es-cp4i#install-event-streams-using-openshift-console) using the OpenShift Console\nand [this note to install one MQ Operator and one MQ Broker](/technology/mq/#installation-with-cloud-pak-for-integration). \n\nThe following figure illustrates what we will deploy:\n\n![](./images/mq-kafka-lab1.png)\n\n* The Sell Store simulator is a separate application available in [the refarch-eda-store-simulator public github](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) and \nthe docker image is accessible from the [quay.io registry](https://quay.io/repository/ibmcase/eda-store-simulator).\n* The Event Streams cluster is named `dev`\n\n### Get setup\n\n* Get Cloud Pak for Integration `admin` user password\n\n   ```sh\n   oc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' -n ibm-common-services | base64 --decode && echo \"\"\n   ```\n* Access to the Event Streams console\n   \n   ```sh\n   oc get routes\n   # \n   oc get route dev-ibm-es-ui -o jsonpath='{.spec.host}' && echo \"\"\n   ```\n   \n   Use this URL to access to the Event Stream console:\n\n   ![](./images/es-console.png)\n\n* Using the Topic menu, create the `items` topic with 1 partition, use default retention time, and 3 replicas\n* Get [Internal Kafka bootstrap URL](/technology/event-streams/es-cp4i#get-kafka-bootstrap-url)\n\n   ![](./images/internal-url.png)\n\n   and generate TLS credentials with the  name `tls-mq-user` with the `Produce messages, consume messages and create topics and schemas` permissions, \n   \n   ![](./images/tls-mq-user.png)\n   \n   on `items` topic:\n\n   ![](./images/items-topic.png)\n\n   All consumer groups, as we may reuse this user for consumers.\n  \n   You can download the certificates, but in fact it is not necessary as we will use the created secrets to configure MQ Kafka connector.\n\n* Get MQ Console route from the namespace where MQ brokers run\n\n  ```sh \n  oc get routes -n cp4i | grep mq-web\n  ```\n\n* Access to MQ Broker Console > Manage to see the Broker configured \n\n   ![](./images/mq-manage.png)\n\n* Add the local `items` queue \n\n   ![](./images/create-queue.png)\n\n   ![](./images/create-queue-2.png)\n\n* Verify MQ App channels defined\n\n  ![](./images/mq-app-channel.png)\n\n* Create an OpenShift project named: `mq-es-demo` with the command: `oc new-project mq-es-demo`\n* Clone the git `ibm-cloud-architecture/eda-lab-mq-to-kafka` repository:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka\n ```\n\n* Update the configMap named `store-simulator-cm` in folder `kustomize/apps/store-simulator/overlays` for the store simulator: `Workloads > ConfigMaps > Create` and use \nthe parameters from MQ_HOST and MQ_CHANNEL.\n\n   ```yaml\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: store-simulator-cm\n    data:\n      APP_VERSION: 0.0.6\n      APP_TARGET_MESSAGING: IBMMQ\n      MQ_HOST: store-mq-ibm-mq.cp4i.svc\n      MQ_PORT: \"1414\"\n      MQ_QMGR: QM1 \n      MQ_QUEUE_NAME: items\n      MQ_CHANNEL: DEV.APP.SVRCONN\n   ```\n\n### Deploy Store simulator\n\n* Deploy to the OpenShift project\n\n   ```sh\n   oc project mq-es-demo\n   oc apply -k  kustomize/apps/store-simulator/overlays\n   ```\n\n* Access to the application web interface:\n\n  ```sh\n   oc get route store-simulator -o jsonpath='{.spec.host}' && echo \"\"\n  ```\n\n![](./images/)\n\n### Deploy Kafka Connect\n\nThe Event Streams MQ Source connector [product documentation](https://ibm.github.io/event-streams/connecting/mq/source/) describes\nwhat need to be done in details. Basically we have to do two things:\n\n* Build the connector image with the needed jars for all the connectos we want to use. For example we need\njar files for the MQ connector and jars for the MQ client api.\n* Start a specific connector with the parameters to connect to the external system and to kafka.\n\nTo make the MQ source connector immediatly useable for the demonstration, we have build a connector image at `quay.io/ibmcase/demomqconnect` and the kafkaconnect.yaml is\nin [this file](https://raw.githubusercontent.com/ibm-cloud-architecture/store-mq-gitops/main/environments/smq-dev/apps/services/kafkaconnect/kafka-connect.yaml).\n\nTo deploy the kafka connect framework do:\n\n```sh\noc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/kafka-connect.yaml\n```\n\nThen start the MQ source connector:\n\n```sh\noc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/mq-src-connector.yaml\n```\n\n\n### Demonstration scripts\n\n1. The solution has multiple stores as presented in the `stores` menu. \n\n  ![](./images/stores-view.png)\n\n1. In the `simulator` menu, select the IBMMQ backend, and send some messages randomly using the \nleft button after selecting the number of messages to send or run the \npredefined scenario with the right button:\n\n  ![](./images/demo-scenarios.png)\n\n1. In MQ Broker Console go to the `items` queue to see the messages generated from the simulator\n\n  ![](./images/mq-msgs.png)\n  \n1. In Event Streams console goes to the `items` topic to see the same messages produced by the Kafka MQ Source connector.\n\n## Lab 2: MQ source to Event Streams using GitOps\n\nThis labs uses GitOps approach with OpenShift GitOps product (ArgoCD) to deploy IBM Event Streams, MQ,\nthe Store Simulator app, and Kafka Connect in the minimum of work. It can be used\non a OpenShift Cluster witout any previously deployed Cloud Pak for Integration \nopersators. Basically as a SRE you will jumpstart the GitOps operator and then starts ArgoCD.\n\n* Clone the `store-mq-gitops` repo\n\n  ```shell\n  git clone https://github.com/ibm-cloud-architecture/store-mq-gitops\n  ```\n* Follow the instructions from the repository main Readme file.\n\n## Lab 3: MQ Sink from Kafka\n\nA may be less used Kafka connector will be to use the MQ Sink connector to get data\nfrom Kafka to MQ. This lab addresses such integration. The target deployment looks like in the following diagram:\n\n ![1](./images/all-ocp.png)\n\nKafka is runnning in its own namespace, and we are using Event Streams from Cloud Pack for Integration.\n\n### Setup MQ Channel\n\nOpen a shell on the remote container or use the user interface to define the communication channel to use for the Kafka Connection.\n\n* Using CLI: Change the name of the Q manager to reflect what you defined in MQ configuration.\n\n```shell\nrunmqsc EDAQMGR1\n# Define a app channel using server connection\nDEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\n# Set the channel authentication rules to accept connections requiring userid and password\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\n# Set identity of the client connections\nALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\n\nREFRESH SECURITY TYPE(CONNAUTH)\n# Define inventory queue\nDEFINE QLOCAL(INVENTORY)\n# Authorize the IBM MQ user ID to connect to and inquire the queue manager\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\n# Authorize the IBM MQ user ID to use the queue:\nSET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\n# done\nEND\n```\n\n* As an alternate you can use the MQ administration console.\n\nTBD\n\n### Setup Kafka Connect Cluster\n\nConnectors can be added to a Kafka Connect environment using OpenShift CLI commands and the source to image customer resource. We will use the Strimzi operator console to setup kafka connect environment. \n\nOnce you downloaded the zip file, which is a yaml manifest, define the configuration for a KafkaConnectS2I instance. The major configuration settings are the server certificate settings and the authentication using Mutual TLS authentication, something like:\n\n```yaml\nspec:\n  bootstrapServers: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n  tls:\n    trustedCertificates:\n    - secretName: sandbox-rp-cluster-ca-cert\n      certificate: ca.crt\n  authentication:\n    type: tls\n    certificate: user.crt\n    key: user.key\n    secretName: sandbox-rp-tls-cred\n```\n\nIf you change the name of the connect cluster in the metadata, modify also the name: `spec.template.pod.metadata.annotations. productChargedContainers` accordingly.\n\nThe secrets used above, need to be accessible from the project where the connector is deployed. The simple way to do so is to copy the source certificates from the Event streams project to your current project with the commands like:\n\n```shell\noc get secret  sandbox-rp-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f -\noc get secret  sandbox-rp-tls-cred  -n eventstreams --export -o yaml | oc apply -f -\n```\n\nIf you do not have a TLS client certificate from a TLS user, use [this note](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to create one.\n\n* Deploy the connector cluster\n\n```shell\noc apply -f kafka-connect-s2i.yaml \n```\n\nAn instance of this custom resource represents a Kafka Connect distributed worker cluster. In this mode, workload balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. Each connector is represented by another custom resource called KafkaConnector.\n\n```shell\noc describe kafkaconnects2i eda-connect-cluster\n```\n\n### Add the mq-sink connector  \n\nThe [product documentation](https://ibm.github.io/event-streams/connecting/mq/) details the available MQ connectors and the configuration process. Using the event streams console, the process is quite simple to get a connector configuration as json file. Here is an example of the final form to generate the json file:\n\n ![6](./images/es-mq-sink.png)\n\n* Once the json is downloaded, complete the settings\n\n```json\n{\n  \"name\": \"mq-sink\",\n  \"config\":\n  {\n      \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n      \"tasks.max\": \"1\",\n      \"topics\": \"inventory\",\n      \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"mq.queue.manager\": \"EDAQMGR1\",\n      \"mq.connection.name.list\": \"eda-mq-lab-ibm-mq(1414)\",\n      \"mq.user.name\": \"admin\",\n      \"mq.password\": \"passw0rd\",\n      \"mq.user.authentication.mqcsp\": true,\n      \"mq.channel.name\": \"KAFKA.CHANNEL\",\n      \"mq.queue\": \"INVENTORY\",\n      \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n  }\n}\n```\n\n* To run the connector within the cluster, we need to connector jar. You can download this jar file from the Event Stream adming console `> Toolkit > Add connector to your Kafka Connect environment > Add Connector > IBM MQ Connectors`,\n\n ![](./images/es-mq-sink-1.png)\n\nor as an alternate, we [cloned the mq-sink code](https://github.com/ibm-messaging/kafka-connect-mq-sink.git), so a `mvn package` command under `kafka-connect-mq-sink` folder will build the jar. Copy this jar under `cp4i/my-plugins` folder. \n* Build the connector with source to image component.\n\n\nWith the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured.  You will see signs of success in the container output (via oc logs, or in the UI):\n\n```shell\n+ curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json\n...\n{\"name\":\"mq-sink-connector\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"mq-service(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink-connector\"},\"tasks\":[{\"connector\":\"mq-sink-connector\",\"task\":0}],\"type\":\"sink\"}\n...\n[2020-06-23 04:26:26,054] INFO Creating task mq-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:419)\n...[2020-06-23 04:26:26,449] INFO Connection to MQ established (com.ibm.eventstreams.connect.mqsink.JMSWriter:229)\n[2020-06-23 04:26:26,449] INFO WorkerSinkTask{id=mq-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)\n```\n\nYou should now have the Kafka Connector MQ Sink running on OpenShift.\n\n\n## MQ Sink Connector on virtual or baremetal server, MQ and Event Streams on IBM Cloud\n\nIn this second option, we are using our own laptop for the baremetal dedployment, but the current solution will work the same on virtual server.\n\n ![2](./images/hybrid.png)\n\n### Pre-requisites\n\nWe assume that you have an instance of IBM Event Streams already running on IBM Cloud or on OpenShift with at least administrator credentials created. The credentials will be needed later on for configuring the Kafka Connect framework to be able to connect and work with your IBM Event Streams instance. Also, this scenario requires a topic called `inventory` in your IBM Event Streams instance. For gathering the credentials and creating the topic required for this scenario, please review the [Common pre-requisites](/use-cases/overview/pre-requisites/). **IMPORTANT:** if you are sharing your IBM Event Streams instance, append some unique identifier to the topic you create in IBM Event Streams.\n\n### Create Local IBM MQ Instance\n\nIn this section we are going to use Docker to create a local IBM MQ instance to simulate an IBM MQ instance somewhere in our datacenter.\n\n1. Create a data directory to mount onto the container.\n\n  ```shell\n  mkdir qm1data\n  ```\n\n1. Run the IBM MQ official Docker image by execting the following command.\n\n  ```shell\n  docker run                     \\\n    --name mq                    \\\n    --detach                     \\\n    --publish 1414:1414          \\\n    --publish 9443:9443          \\\n    --publish 9157:9157          \\\n    --volume qm1data:/mnt/mqm    \\\n    --env LICENSE=accept         \\\n    --env MQ_QMGR_NAME=QM1       \\\n    --env MQ_APP_PASSWORD=admin  \\\n    --env MQ_ENABLE_METRICS=true \\\n    ibmcom/mq\n  ```\n  where we can see that out container will be called `mq`, it will run in `detached` mode (i.e. in the background), it will expose the ports IBM MQ uses for communication and the image we are actually running `ibmcom/mq`.\n\n1. You could make sure your IBM MQ Docker image is running by listing the Docker containers in your workstation.\n\n  ```shell\n  $ docker ps\n  CONTAINER ID   IMAGE       COMMAND            CREATED         STATUS         PORTS                                                                    NAMES\n  a7b2a115a3c6   ibmcom/mq   \"runmqdevserver\"   6 minutes ago   Up 6 minutes   0.0.0.0:1414->1414/tcp, 0.0.0.0:9157->9157/tcp, 0.0.0.0:9443->9443/tcp   mq\n  ```\n\n1. You should also be able to log into the MQ server on port 9443 (<https://localhost:9443>) with default user `admin` and password `passw0rd`.\n\n  ![login](./images/login.png)\n\nNow, we need to configure our local IBM MQ instance Queue Manager in order to define a server connection (`KAFKA.CHANNEL`) with authentication (user `admin`, password `admin`) and the queue (`INVENTORY`) where our messages from Kafka will be sinked to. And we are going to so it using the IBM MQ CLI.\n\n1. Get into the IBM MQ Docker container we have started above by executing the following command that will give us a `bash` interactive terminal:\n\n  ```shell\n  docker exec -ti mq bash\n  ```\n\n1. Now that we are in the container, start the queue manager `QM1` by executing:\n\n  ```shell\n  strmqm QM1\n  ```\n\n1. Start the `runmqsc` tool to configure the queue manager by executing:\n\n  ```shell\n  runmqsc QM1\n  ```\n\n1. Create a server-connection channel called `KAFKA.CHANNEL` by executing:\n\n  ```shell\n  DEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\n  ```\n\n1. Set the channel authentication rules to accept connections requiring userid and password by executing:\n\n  ```shell\n  SET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\n  SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\n  SET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\n  ```\n\n1. Set the identity of the client connections based on the supplied context, the user ID by executing:\n\n  ```shell\n  ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\n  ```\n\n1. Refresh the connection authentication information by executing:\n\n  ```shell\n  REFRESH SECURITY TYPE(CONNAUTH)\n  ```\n\n1. Create the `INVENTORY` queue for the connector to use by executing:\n\n  ```shell\n  DEFINE QLOCAL(INVENTORY)\n  ```\n\n1. Authorize `admin` to connect to and inquire the queue manager by executing:\n\n  ```shell\n  SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\n  ```\n\n1. Finally authorize `admin` to use the queue by executing:\n\n  ```shell\n  SET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\n  ```\n\n1. End `runmqsc` by executing:\n\n  ```shell\n  END\n  ```\n\nYou should see the following output:\n\n```shell\n9 MQSC commands read.\nNo commands have a syntax error.\nOne valid MQSC command could not be processed.\n```\n\nExit the container by executing:\n\n```shell\nexit\n```\n\nIf you check your IBM MQ dashboard you should see your newly created `INVENTORY` queue:\n\n  ![inventory](./images/inventory_queue.png)\n\n### Create MQ Kafka Connector Sink\n\nThe MQ Connector Sink can be downloaded from this [Github](https://github.com/ibm-messaging/kafka-connect-mq-sink). The Github site includes exhaustive instructions for further detail on this connector and its usage.\n\n1. Clone the repository with the following command:\n\n  ```shell\n  git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git\n  ```\n\n1. Change directory into the `kafka-connect-mq-sink` directory:\n\n  ```shell\n  cd kafka-connect-mq-sink\n  ```\n\n1. Build the connector using Maven:\n\n  ```shell\n  mvn clean package\n  ```\n\n1. Create a directory (if it does not exist yet) to contain the Kafka Connect framework configuration and cd into it.\n\n  ```shell\n  mkdir config\n  cd config\n  ```\n\n1. Create a configuration file called `connect-distributed.properties` for the Kafka Connect framework with the following properties in it:\n\n  ```properties\n  # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\n  bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  ssl.enabled.protocols=TLSv1.2\n  ssl.protocol=TLS\n  ssl.truststore.location=/opt/kafka/config/es-cert.p12\n  ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n  ssl.truststore.type=PKCS12\n  security.protocol=SASL_SSL\n  sasl.mechanism=SCRAM-SHA-512\n  sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n  # Consumer side configuration\n  consumer.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  consumer.security.protocol=SASL_SSL\n  consumer.ssl.protocol=TLSv1.2\n  consumer.ssl.truststore.location=/opt/kafka/config/es-cert.p12\n  consumer.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n  consumer.ssl.truststore.type=PKCS12\n  consumer.sasl.mechanism=SCRAM-SHA-512\n  consumer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n  # Producer Side\n  producer.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  producer.security.protocol=SASL_SSL\n  producer.ssl.protocol=TLSv1.2\n  producer.ssl.truststore.location=/opt/kafka/config/es-cert.p12\n  producer.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n  producer.ssl.truststore.type=PKCS12\n  producer.sasl.mechanism=SCRAM-SHA-512\n  producer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n  plugin.path=/opt/kafka/libs\n\n  # unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\n  group.id=mq-sink-cluster-REPLACE_WITH_UNIQUE_IDENTIFIER\n\n  # The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n  # need to configure these based on the format they want their data in when loaded from or stored into Kafka\n  key.converter=org.apache.kafka.connect.json.JsonConverter\n  value.converter=org.apache.kafka.connect.json.JsonConverter\n  # Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n  # it to\n  key.converter.schemas.enable=true\n  value.converter.schemas.enable=true\n\n  # Topic to use for storing offsets.\n  offset.storage.topic=connect-offsets-REPLACE_WITH_UNIQUE_IDENTIFIER\n  offset.storage.replication.factor=3\n  #offset.storage.partitions=25\n\n  # Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic.\n  config.storage.topic=connect-configs-REPLACE_WITH_UNIQUE_IDENTIFIER\n  config.storage.replication.factor=3\n\n  # Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\n  status.storage.topic=connect-status-REPLACE_WITH_UNIQUE_IDENTIFIER\n  status.storage.replication.factor=3\n  status.storage.partitions=5\n\n  # Flush much faster than normal, which is useful for testing/debugging\n  offset.flush.interval.ms=10000\n  ```\n\n  **IMPORTANT:** You must replace **all occurences** of the following placeholders in the properties file above with the appropriate values for the Kafka Connect framework to work with your IBM Event Streams instance:\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_UNIQUE_IDENTIFIER`: A unique identifier so that the resources your kafka connect cluster will create on your IBM Event Streams instance don't collide with other users' resources _(Note: there are 4 placeholders of this type. Replace them all)._ \n    \n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n1. Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. **IMPORTANT:** download the PKCS12 certificate. How to get the certificate in the [Common pre-requisites](/use-cases/overview/pre-requisites/) section. \n\n1. Create a log4j configuration file named `connect-log4j.properties` based on the template below.\n\n  ```properties\n  log4j.rootLogger=DEBUG, stdout\n\n  log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n  log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n  log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n\n  log4j.logger.org.apache.kafka=INFO\n  ```\n\n1. Back out of the `config` directory to the `kafka-connect-mq-sink` directory:\n\n  ```shell\n  cd ..\n  ```\n\n2. Build the Docker image for your Kafka Connect framework that will contain the IBM MQ Sink connector and all the properties files you have created and tailored earlier so that your Kafka Connect framework can work with your IBM Event Streams instance we have set up previously in this exercise (_mind the dot at the end of the command. It is necessary_):\n\n  ```shell\n  docker build -t kafkaconnect-with-mq-sink:1.3.0 .\n  ```\n\n1. Run the Kafka Connect MQ Sink container.\n\n  ```\n  docker run                                 \\\n    --name mq-sink                           \\\n    --detach                                 \\\n    --volume $(pwd)/config:/opt/kafka/config \\\n    --publish 8083:8083                      \\\n    --link mq:mq                             \\\n    kafkaconnect-with-mq-sink:1.3.0\n  ```\n\n1. Now that we have a Kafka Connect framework running with the configuration to connect to our IBM Event Streams instance and the IBM MQ Sink Connector jar file in it, is to create a JSON file called `mq-sink.json` to startup an instance of the IBM MQ Sink connector in our Kafka Connect framework with the appropriate config to work read messages from the IBM Event Streams topic we desire and sink those to the IBM MQ instance we have running locally. **IMPORTANT:** If you are sharing your IBM Event Streams instance and followed the instructions on this readme, you should have appended a unique identifier to the name of the topic (`inventory`) that you are meant to create in IBM Event Streams. As a result, modify the line `\"topics\": \"inventory\"` in the following JSON object accordignly.\n\n   ```json\n   {\n      \"name\": \"mq-sink\",\n      \"config\":\n      {\n          \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n          \"tasks.max\": \"1\",\n          \"topics\": \"inventory\",\n\n          \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n          \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n          \"mq.queue.manager\": \"QM1\",\n          \"mq.connection.name.list\": \"mq(1414)\",\n          \"mq.user.name\": \"admin\",\n          \"mq.password\": \"passw0rd\",\n          \"mq.user.authentication.mqcsp\": true,\n          \"mq.channel.name\": \"KAFKA.CHANNEL\",\n          \"mq.queue\": \"INVENTORY\",\n          \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n      }\n   }\n   ```\n\n1. Last piece of the puzzle is to tell our Kafka Connect framework to create and startup the IBM MQ Sink connector based on the configuration we have in the previous json file. To do so, execute the following `POST` request.\n\n  ```Shell\n  curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./mq-sink.json\"\n\n  # The response returns the metadata about the connector\n  {\"name\":\"mq-sink\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"ibmmq(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink\"},\"tasks\":[{\"connector\":\"mq-sink\",\"task\":0}],\"type\":\"sink\"}\n  ```\n\n1. You can query your Kafka Connect framework to make sure of this. Execute the following command that should return all the connectors up and running in your Kafka Connect cluster.\n\n  ```shell\n  curl localhost:8083/connectors\n  [\"mq-sink\"]\n  ```\n\nYou should now have a working MQ Sink connector getting messages from your topic in IBM Event Streams and sending these to your local IBM MQ instance. In order to check that out working, we first need to send few messages to that IBM Event Streams topic and then check out our `INVENTORY` queue in our local IBM MQ instance.\n\n1. In order to send messages to IBM Event Streams, we are going to use the IBM Event Streams Starter application. You can find the instruction either in the IBM Event Streams official documentation [here] or on the IBM Event Streams dashboard:\n\n  ![starter](./images/starter-app.png)\n\n1. Follow the instructions to run the IBM Event Streams starter application from your workstation. **IMPORTANT:** When generating the properties for your IBM Event Streams starter application, please choose to connect to an existing topic and select the topic you created previously as part of this exercise so that the messages we send into IBM Event Streams end up in the appropriate topic that is being monitored by your IBM MQ Sink connector runnning on the Kafka Connect framework.\n\n  ![starter-config](./images/starter-config.png)\n\n1. Once you have your application running, open it up in your web browser, click on `Start producing`, let the application produce a couple of messages and then click on `Stop producing`\n\n  ![starter-messages](./images/starter-messages.png)\n\n1. Check out your those messages got into the Kafka topic\n\n  ![kafka-messages](./images/kafka-messages.png)\n\n1. Check out your messages in the Kafka topic have already reached your `INVENTORY` queue in your local IBM MQ instance\n\n  ![mq-messages](./images/mq-message.png)\n\n\n1. You could also inspect the logs of your Kafka Connect Docker container running on your workstation:\n\n  ```shell\n  docker logs mq-sink\n  ...\n  [2021-01-19 19:44:17,110] DEBUG Putting record for topic inventory, partition 0 and offset 0 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n  [2021-01-19 19:44:17,110] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n  [2021-01-19 19:44:18,292] DEBUG Flushing up to topic inventory, partition 0 and offset 1 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n  [2021-01-19 19:44:18,292] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 636: {inventory-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:346)\n  [2021-01-19 19:44:18,711] DEBUG Putting record for topic inventory, partition 0 and offset 1 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n  [2021-01-19 19:44:18,711] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n  [2021-01-19 19:44:20,674] DEBUG Putting record for topic inventory, partition 0 and offset 2 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n  [2021-01-19 19:44:20,675] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n  [2021-01-19 19:44:28,293] DEBUG Flushing up to topic inventory, partition 0 and offset 3 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n  [2021-01-19 19:44:28,293] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 637: {inventory-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:346)\n  [2021-01-19 19:44:38,296] DEBUG Flushing up to topic inventory, partition 0 and offset 3 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n  ```\n\nTo cleanup your environment, you can do\n\n1. Remove the connector from your Kafka Connect framework instance (this isn't really needed if you are going to stop and remove the Kafka Connect Docker container)\n\n  ```shell\n  curl -X DELETE http://localhost:8083/connectors/mq-sink\n  ```\n\n1. Stop both IBM MQ and Kakfa Connect Docker containers running on your workstation\n\n  ```shell\n  docker stop mq mq-sink\n  ```\n\n1. Remove both IBM MQ and Kakfa Connect Docker containers from your workstation\n\n  ```shell\n  docker rm mq mq-sink\n  ```\n\n\n## Lab 4: MQ Connector with  Kafka Confluent\n\n* Clone the git `ibm-cloud-architecture/eda-lab-mq-to-kafka` repository and follow the readme.\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka\n ```","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-mq/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}