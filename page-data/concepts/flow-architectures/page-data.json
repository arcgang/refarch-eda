{"componentChunkName":"component---src-pages-concepts-flow-architectures-mdx","path":"/concepts/flow-architectures/","result":{"pageContext":{"frontmatter":{"title":"Flow architecture","description":"James Urquhaert's \"Flow architecture\" book summary"},"relativePagePath":"/concepts/flow-architectures.mdx","titleType":"append","MdxNode":{"id":"225646ae-dd70-5256-825a-273396eca092","children":[],"parent":"f3e97c03-d304-5dfd-994b-992353acecc3","internal":{"content":"---\ntitle: Flow architecture\ndescription: James Urquhaert's \"Flow architecture\" book summary\n---\n\n\n<InlineNotification kind=\"warning\">\n<strong>Added 1/21/2022</strong>\n</InlineNotification>\n\nFrom the [James Urquhart's book: Flow architecture](https://www.amazon.com/Flow-Architectures-Streaming-Event-Driven-Integration/dp/1492075892/ref=sr_1_1?keywords=flow+architectures&qid=1637675009&sr=8-1) and\npersonal studies.\n\nAs more and more of our businesses “go digital”, the groundwork is in place to fundamentally\nchange how real-time data is exchanged across organization boundaries. Data from different sources\ncan be combined to create a holistic view of a business situation.\n\n**Flow** is networked software integration that is event-driven, loosely coupled, and highly adaptable and extensible.\n\nValue is created by interacting with the flow, and not just the data movement.\n\nSince the beginning of IT as an industry, we are digitizing and automating the exchanges of value, and we spend\na lot of time and money to execute key transactions with less human intervention.\nHowever, most of the integrations we execute across organizational boundaries today are not\nin real time. Today, most, perhaps all—digital financial transactions in the world economy\nstill rely on batch processing at some point in the course of settlement.\n\nThere is no consistent and agreed-upon mechanism for exchanging signals for immediate action across companies or industries.\n\nIt is still extremely rare for a company to make real-time data available for \nunknown consumers to process at will.\n\nThis is why modern event-driven architecture (EDA) will enable profound changes in \nthe way companies integrate. EDAs are highly decoupled architectures, meaning there \nare very few mutual dependencies between the parties at both ends of the exchange.\n\n\n## 1- Flow characteristics\n\n* Consumer applications requests data streams through self-service interfaces, and get the data continuously.\n* Producers maintain control of relevant information to transmit and when to transmit.\n* **Event** packages information of data state changes, with timestamp and unique ID. \nThe context included with the transmitted data allows the consumer to better understand the nature of that data. [CloudEvent](https://cloudevents.io/) helps defining such context.\n* The transmission of a series of events between two parties is called an **event stream**.\n\nThe more streams there are from more sources, the more flow consumers will be drawn to \nthose streams and the more experimentation may be done. Over time, organizations will find \nnew ways to tie activities together to generate new value.\n\n**Composable architectures** allow the developer to assemble fine grained parts using \nconsistent mechanisms for both inputting data and consuming the output.\nIn **contextual architectures**, the environment provides specific contexts in \nwhich integration can happen. Developer must know a lot about the data that is available,\n the mechanism by which the data will be passed, the rules for coding and deploying \n the software.\n\nEDA provides a much more composable and evolutionary approach for building event and data streams.\n\n## 2- Business motivations\n\n* Do digital transformation to improve customer experiences. Customers expect their data to \nbe used in a way that is valuable to them, not just to the vendors. Sharing data between organizations\n can lead to new business opportunities. This is one of the pilard of Society 5.0. \n\n> The Japan government defined **Society 5.0** as \"A human-centered society that balances economic \nadvancement with the resolution of social problems by a system that highly integrates\ncyberspace and physical space\".\n\n* Improve process automation, to drive efficiencies and profitability. The most limiting constraint in the \nprocess hides any improvements made to other steps. Finding constraints is where [value stream mapping](https://tkmg.com/books/value-stream-mapping/) shines:\nit uses lead time (queue time) and actual time to do the work. EDA will help to get time stamp and data \nfor steps in the process that are not completely in scope of a business process: may be cross business boundaries.\n* Extract innovative value from data streams. Innovation as better solution for existing problem, or as new\nsolution to emerging problems.\n\nTo improve process time, software needs accurate data at the time to process the work. As business evolve,\nhaving a rigid protocol to get the data, impacts process time. A business will need to experiment with new data sources\n when they are available and potentially relevant to their business.\n\n**Stream processing improves interoperability (exchange data)**\n\nInnovation is not adaptation. Companies must adapt constantly just to survive, like adding features on a product\nto pace with competition. Digital transformation aimed at avoiding competitive disruption is not innovation.\n\nAs the number of stream options grows, more and more business capabilities will be \ndefined in terms of stream processing. This will drive developers to find easier ways \nto discover, connect to, and process streams.\n\n### Enabler for flow adoption\n\n* **Lowering the cost of stream processing**: Integration costs dominate modern IT budgets.\nFor many integrations, the cost of creating interaction between systems is simply too high for what little value is gained.\nWith common interfaces and protocols that enable flows, the integration cost will be lower\nand people will find new uses for streaming that will boost the overall demand for streaming technologies. The [Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox) at work\n* **Increasing the flexibility in composing data flows**: \"pipe\" data streams from one processing \nsystem to another through common interfaces and protocols.\n* **Creating and utilizing a rich market ecosystem around key streams**. The equities markets have all moved entirely to electronic forms of executing their marketplaces.\nHealth-care data streams for building services around patient data. Refrigerators streaming data to grocery\ndelivery services. \n\nFlow must be secure (producers maintain control over who can access their events), \nagile (change schema definitions), \ntimely (Data must arrive in a time frame that is appropriate for the context to which it is being applied), \nmanageable and retain a memory of its past. \n\nServerless, stream processing, machine learning, will create alternative to batch processing.\n\n## 3- Market\n\nSOA has brought challenges for adoption and scaling. Many applications have their own interfaces\nand even protocols to expose their functionality, so most integrations need protocol and \ndata model translations. \n\nThe adoption of queues and adaptors to do data and protocol translation was a scalable solution. \nExtending this central layer of adaptation was the Enterprise Service Bus, with intelligent\npipes / flows. \n\nMessage queues and ESBs are important to the development of streaming architectures but\nto support scalability and address complexity more decoupling is needed between \nproducers and consumers.\n\nFor IoT [MQTT](https://mqtt.org/) is the standard for messaging protocols in a lightweight pub/sub \ntransport protocol. MQTT supports 3 service levels: 0 - at most once, 1- at least once, 2 - exactly once.\nIt allows for messaging between device to cloud and cloud to device. It supports for persistent sessions\n reduces the time to reconnect the client with the broker.\nThe MQTT broker manages a list of topics, which enable it to identify groups of subscribers interested in a collection of messages.\n\nFor event processing three type of engines:\n\n* **Functions** (including low-code or no-code processors): WAS lambda, Knative eventing, Flink, Storm. Mendix and Vantiq have event-driven low code platform.\n* **log-based event streaming platforms**: Apache Kafka, Apache Pulsar, AWS Kinesis, and Microsoft Azure Event Hubs. Topic becomes a system of record, as event sourcing pattern implementation.\n* **real-time stateful systems**: *Digital twins* are software agents supporting the problem domain\nin a stateful manner. Behavior is supported by code or rules, and relationship between agents. \nAgents can monitor the overall system state. [Swim.ai](https://www.swim.ai/) builds its model dynamically from the event stream and provides built-in machine learning capabilities that enable both continuous learning and high performance model execution\n\nMainstream adoption of flow itself will be five to ten years from now (2020). Flow will have to prove that \nit meets security criteria for everything from electronic payments, to health-care data, to classified\n information. The CNCF’s [CloudEvents](https://cloudevents.io/) specification, for instance, strongly suggests payloads be encrypted.\nThere is no single approach to defining an event with encryption explicitly supported \nthat can be read by any event-consuming application (MQTT, AMQP, have different encryption and TLS add more for \nTCP connection).\n\nConsumers need assurances that the data they receive in an event is valid and accurate, a practice \nknown as **data provenance**.  \n\n> Data provenance is defined as “a record of the inputs, entities, \nsystems, and processes that influence data of interest, providing a historical record of the \ndata and its origins\"\n\nProvenance has to maintained by the producer as a checksum number created by parsing the event data, and encrypted\nby the producer's key. CloudEvent has metadata about the message. When sent to Kafka they are \nimmutable record. Now the traceability of the consumers in kafka world is a major challenge.\nBlockchain may also be used to track immutable record with network parties attest its accuracy.\n\n\nApplying the concept of data loose value over time, it is important to act on data as early\nas possible, close to creation time. After a period of time data becomes less valuable.\n\nTwo time factors are important in this data processing: **latency** (time to deliver data to consumers)\nand **retention** (time to keep data). For latency try to reduce the number of network segment between\nproducer and consumers. Considering edge computing as a way to bring event processing close to the source.\nThe event processing add time to the end to end latency. Considering constraining the processing time frame.\n\n*Retention* is a problem linked to the business requirements, and we need to assess for each topic how long\nan event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, \nprojection views... keeping for too long, increase the cost of storage, but also the time to rebuild data \nprojection. \n\nFinally, producers will want to trust that only authorized consumers are using the events they produce.\nAlso it may be possible to imagine a way to control the intellectual property of the data so producer can keep \nits ownership. Data consumption should be done via payment like we do with music subscription.\n\n## Flow patterns:\n\n### Collector pattern\n\nThe Collector pattern is a pattern in which a single consumer subscribes to topics from multiple producers. \n\n### Distributor pattern\n\nEach event in a stream is distributed to multiple consumers. It could be a hard problem to solve\nwhen doing it across geographically distributed systems. Edge computing can be used to distribute\n streaming endpoints closer to the consumers that need those streams. Alternate\n is to moving the event processing close to the source. For many Distributor use cases, \n partitioning the data by region is probably smart, and flow interfaces will need to take \n this into account.\n\n### Signal pattern\n\nThe Signal pattern is a general pattern that represents functions that exchange data between\n actors based on a distinct function or process, in can be seen as a traffic cop. \n It supports multiple producers and multiple consumers. The signal pattern is supported\n by multiple event processing each handling one aspect of the event processing.\n\nStream processing may route event streams between several distributed edge computing services as \nwell as core shared services, but then we need management layer to get global view of the systems.\nThey need to be integrated into observability tool. But the \"single pane of glass\" is often a lure\nas distributed systems require distributed decision-making. More local solutions are more agile, flexible\nand better address local problems for improved resilience.  \n\nOne of the challenge of complex adaptive systems is that any agent participating in \nthe system has difficulty seeing how the system as a whole operates,\nbecause of its limited connections to other neighbor agents.\n\n### Facilitator pattern\n\nA specialized form of Signal pattern, facilitator is a \"broker\" to match producers' events to\nconsumers' demands. It is like matching sellers with buyers.\n\n## 4- Identifying flow in your business\n\nThe classical usee case categories:\n\n* **Addressing and discovery**: In modern complex systems environments, multiple systems need to be informed of the new entity, be able to utilize to assign\nwork to it. Addressing and discovery happens across organizational boundaries (for example in real-time inventory SKU is used to identify item for both supplier and retailers).\nTo seek such use cases, look at tracking problems like who or what is involved in a problem domain that is difficult to scale.\nWith event stream centric approach, A&D is done via a registry service used by new agents to indicate their existence, and the service publishes an event to a topic\nto broadcast the information about the new agent. A second option is to use a discovery service to watch  \nspecific event stream for certain transmissions that indicate the presence of an agent. [Swim.ai](https://www.swim.ai/) continuously process and analyze \nstreaming data in concert with contextual data to inform business-critical, operational decisions. See also [SwimOS](https://github.com/swimos) or Apache Flink.\n\n* **Command and control**: sources are connected to key decision-making and action-taking services to complete a business task. So they are everywhere in any business.\nA typical example of such use case, is the Supervisory Control And Data Acquisition, used in manufacturing, or energy production.\nTry to ask: *where does the organization depend on timely responses to changes in state?*\nC&C can be supported by centralized control with events come from multiple sources to stateless or stateful services, to apply\nreal-time analysis and decision-making algorithms to those streams. Output events are published to sinks for future processing.\nScaling with a centralized control approach is not straightforward, as getting the right events to the right processing instances can be a challenge.\nAlso when we need the compute global aggregates by looking at the state of various agents in the systems is more complex,\nas it needs to integrate with stateful stream processing. Actions can be triggered by state changes, triggers that fire at specific times, or even API requests from other applications or services.\n\nAn alternate is to use distributed control, like applying the decision-making logic at the edge. \n\n* **Query and observability:** querying or monitoring individual agents or specific groups of agents. The problem is\nto locate the right agent target of the query, and get current state or history from that agent. \n* **Telemetry and analytics:** focuses on understanding systems behavior, and get real-time big data insights (e.g. Click streams). \nNeed to assess which insights require understanding the emerging behavior of a system of agents emitting vast amounts of data.\n\nInteresting presentations:\n\n* [Voxxed Athens 2018 - Eventing, Serverless, and the Extensible Enterprise by Clemens Vasters](https://www.youtube.com/watch?v=qCNXUUlhJJE)\n\n## 5- Model Flow\n\nUse Event storming to build a timeline of events that are required to complete a complex task, and to get\nand understanding of the people, systems, commands and policies that affect the event flow. \nThe Event Storming process is a highly interactive endeavor that :brings subject matter experts \ntogether to identify the flow of events in a process or system\n\n1. Identify the business activities that you would like to model in terms of event flow\n1. Begin by asking the group to identify the events that are interesting and/or required for that business activity\n1. Place events along a timeline from earliest action to latest action\n1. Capture:\n\n    * The real-world influences on the flow, such as the human users or external systems that produce or consume events\n    * What commands may initiate an event\n    * What policies are activated when an event takes place. A policy usually initiates a new command. Events always result in a policy action unless the event is generating output.\n    * What are the outputs from the event flow.\n\nWhen designing the solution assess:\n\n* When the event is simply to broadcast facts for consumption by interested parties. The producer contract is\n simply to promise to send events as soon as they are available.\n* If consumers can come and go, and experiment with the consumption of a stream with little risk of consequences if they choose not to continue\n* When event is part of an interaction around an intent, requiring a conversation with the consumer\n* Is the event a stand-alone communication, discrete, or is it only useful in a context that includes a series of events. \nSeries applications are where log-based queueing shines\n* Is the processing involve one simple action per event, or is there a group of related actions, a workflow, required to complete processing\n\nWhen building a flow-ready application for messaging, the “trunk-limb-branch-leaf” pattern is a \ncritical tool to consider: use edge computing to distribute decision-making close to the related \ngroups of agents, computing local aggregates, and propagate to larger more central flows. Using messaging\nmiddleware to manage interaction between agents, to isolate message distribution to just the needed\nservers and agents, and propagate aggregates to the trunk, greatly reducing traffic between the original agents and the core.\n\nAnother consideration is to assess if the consumers need to filter events from a unique topic before\ndoing its own processing, in this case the event payload may include metadata and URL to get the payload.\nIf the metadata indicates an action is required, the consumer can then call the data retrieval URL.\n\nWhether or not you include payload data depends a bit on the volume of events being published and the security and latency requirements of your application.\n\nLog-based queues can play the role of “system of record” for both event values and sequence, especially for systems that need both the most recent event and an understanding of the recent history of events received\n\nFor single action processing, serverless, knative eventing are technologies to consider. Solution\nneeds to route events to the appropriate processor. But if your event processing needs require \nmaintaining accurate state for the elements sending events then stateful streaming platform are better fit.\n\nFor workflow, modern solutions, simplify creating and managing process definitions independent of the actions taken in that process.\nIt supports for stepping an event through multiple interdependent actions. Workflow may require to\nwait for another related event occurs or a human completes his action.\n\n## 6- Today landscape\n\n\n* **Standards** are important for flow:  TLS, WebSockets, and HTTP from IETF, MQTT and AMQP from OASIS, \n*CloudEvents* and the Serverless Working Group from CNCF\n* **Open sources projects**: \n\n    * Apache Kafka and Apache Pulse for log-based queueing\n    * Apache Beam, Flink, Heron, Nifi, Samza, and Storm for stream processing\n    * Apache Druid as a “stream-native” database\n    * gRPC may play a key role in any future flow interface\n    * NATS.io, a cloud-native messaging platform\n    * Argo, a Kubernetes-based workflow manager that theoretically could act as the core of an event-driven process automation bus\n\n\n* Opportunities:\n\n    * Data provenance and security for payloads passed between disparate parties\n    * Tracking event data distribution across the world wide flow. Where does the data generated by an event end up being consumed or processed?\n    * Platforms for coordinating event processing, aggregation, and synchronization between core data center event processors, edge computing environments, and end-user or IoT devices\n    * Monetization mechanisms for all types of event and messaging streams\n\nThe adoption of a technology is not the delivery that makes it valuable, but the ecosystem that consumes it.\n\nLook at existing streams and determine how to add value for the consumers of that stream. \nCan you automate valuable insights and analytics in real time for customers with shared needs? \nWould it be possible to recast the stream in another format for an industry that is currently \nusing a different standard to consume that form of data? ","type":"Mdx","contentDigest":"3798c054e362966a3dee0b0073c36e4f","owner":"gatsby-plugin-mdx","counter":887},"frontmatter":{"title":"Flow architecture","description":"James Urquhaert's \"Flow architecture\" book summary"},"exports":{},"rawBody":"---\ntitle: Flow architecture\ndescription: James Urquhaert's \"Flow architecture\" book summary\n---\n\n\n<InlineNotification kind=\"warning\">\n<strong>Added 1/21/2022</strong>\n</InlineNotification>\n\nFrom the [James Urquhart's book: Flow architecture](https://www.amazon.com/Flow-Architectures-Streaming-Event-Driven-Integration/dp/1492075892/ref=sr_1_1?keywords=flow+architectures&qid=1637675009&sr=8-1) and\npersonal studies.\n\nAs more and more of our businesses “go digital”, the groundwork is in place to fundamentally\nchange how real-time data is exchanged across organization boundaries. Data from different sources\ncan be combined to create a holistic view of a business situation.\n\n**Flow** is networked software integration that is event-driven, loosely coupled, and highly adaptable and extensible.\n\nValue is created by interacting with the flow, and not just the data movement.\n\nSince the beginning of IT as an industry, we are digitizing and automating the exchanges of value, and we spend\na lot of time and money to execute key transactions with less human intervention.\nHowever, most of the integrations we execute across organizational boundaries today are not\nin real time. Today, most, perhaps all—digital financial transactions in the world economy\nstill rely on batch processing at some point in the course of settlement.\n\nThere is no consistent and agreed-upon mechanism for exchanging signals for immediate action across companies or industries.\n\nIt is still extremely rare for a company to make real-time data available for \nunknown consumers to process at will.\n\nThis is why modern event-driven architecture (EDA) will enable profound changes in \nthe way companies integrate. EDAs are highly decoupled architectures, meaning there \nare very few mutual dependencies between the parties at both ends of the exchange.\n\n\n## 1- Flow characteristics\n\n* Consumer applications requests data streams through self-service interfaces, and get the data continuously.\n* Producers maintain control of relevant information to transmit and when to transmit.\n* **Event** packages information of data state changes, with timestamp and unique ID. \nThe context included with the transmitted data allows the consumer to better understand the nature of that data. [CloudEvent](https://cloudevents.io/) helps defining such context.\n* The transmission of a series of events between two parties is called an **event stream**.\n\nThe more streams there are from more sources, the more flow consumers will be drawn to \nthose streams and the more experimentation may be done. Over time, organizations will find \nnew ways to tie activities together to generate new value.\n\n**Composable architectures** allow the developer to assemble fine grained parts using \nconsistent mechanisms for both inputting data and consuming the output.\nIn **contextual architectures**, the environment provides specific contexts in \nwhich integration can happen. Developer must know a lot about the data that is available,\n the mechanism by which the data will be passed, the rules for coding and deploying \n the software.\n\nEDA provides a much more composable and evolutionary approach for building event and data streams.\n\n## 2- Business motivations\n\n* Do digital transformation to improve customer experiences. Customers expect their data to \nbe used in a way that is valuable to them, not just to the vendors. Sharing data between organizations\n can lead to new business opportunities. This is one of the pilard of Society 5.0. \n\n> The Japan government defined **Society 5.0** as \"A human-centered society that balances economic \nadvancement with the resolution of social problems by a system that highly integrates\ncyberspace and physical space\".\n\n* Improve process automation, to drive efficiencies and profitability. The most limiting constraint in the \nprocess hides any improvements made to other steps. Finding constraints is where [value stream mapping](https://tkmg.com/books/value-stream-mapping/) shines:\nit uses lead time (queue time) and actual time to do the work. EDA will help to get time stamp and data \nfor steps in the process that are not completely in scope of a business process: may be cross business boundaries.\n* Extract innovative value from data streams. Innovation as better solution for existing problem, or as new\nsolution to emerging problems.\n\nTo improve process time, software needs accurate data at the time to process the work. As business evolve,\nhaving a rigid protocol to get the data, impacts process time. A business will need to experiment with new data sources\n when they are available and potentially relevant to their business.\n\n**Stream processing improves interoperability (exchange data)**\n\nInnovation is not adaptation. Companies must adapt constantly just to survive, like adding features on a product\nto pace with competition. Digital transformation aimed at avoiding competitive disruption is not innovation.\n\nAs the number of stream options grows, more and more business capabilities will be \ndefined in terms of stream processing. This will drive developers to find easier ways \nto discover, connect to, and process streams.\n\n### Enabler for flow adoption\n\n* **Lowering the cost of stream processing**: Integration costs dominate modern IT budgets.\nFor many integrations, the cost of creating interaction between systems is simply too high for what little value is gained.\nWith common interfaces and protocols that enable flows, the integration cost will be lower\nand people will find new uses for streaming that will boost the overall demand for streaming technologies. The [Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox) at work\n* **Increasing the flexibility in composing data flows**: \"pipe\" data streams from one processing \nsystem to another through common interfaces and protocols.\n* **Creating and utilizing a rich market ecosystem around key streams**. The equities markets have all moved entirely to electronic forms of executing their marketplaces.\nHealth-care data streams for building services around patient data. Refrigerators streaming data to grocery\ndelivery services. \n\nFlow must be secure (producers maintain control over who can access their events), \nagile (change schema definitions), \ntimely (Data must arrive in a time frame that is appropriate for the context to which it is being applied), \nmanageable and retain a memory of its past. \n\nServerless, stream processing, machine learning, will create alternative to batch processing.\n\n## 3- Market\n\nSOA has brought challenges for adoption and scaling. Many applications have their own interfaces\nand even protocols to expose their functionality, so most integrations need protocol and \ndata model translations. \n\nThe adoption of queues and adaptors to do data and protocol translation was a scalable solution. \nExtending this central layer of adaptation was the Enterprise Service Bus, with intelligent\npipes / flows. \n\nMessage queues and ESBs are important to the development of streaming architectures but\nto support scalability and address complexity more decoupling is needed between \nproducers and consumers.\n\nFor IoT [MQTT](https://mqtt.org/) is the standard for messaging protocols in a lightweight pub/sub \ntransport protocol. MQTT supports 3 service levels: 0 - at most once, 1- at least once, 2 - exactly once.\nIt allows for messaging between device to cloud and cloud to device. It supports for persistent sessions\n reduces the time to reconnect the client with the broker.\nThe MQTT broker manages a list of topics, which enable it to identify groups of subscribers interested in a collection of messages.\n\nFor event processing three type of engines:\n\n* **Functions** (including low-code or no-code processors): WAS lambda, Knative eventing, Flink, Storm. Mendix and Vantiq have event-driven low code platform.\n* **log-based event streaming platforms**: Apache Kafka, Apache Pulsar, AWS Kinesis, and Microsoft Azure Event Hubs. Topic becomes a system of record, as event sourcing pattern implementation.\n* **real-time stateful systems**: *Digital twins* are software agents supporting the problem domain\nin a stateful manner. Behavior is supported by code or rules, and relationship between agents. \nAgents can monitor the overall system state. [Swim.ai](https://www.swim.ai/) builds its model dynamically from the event stream and provides built-in machine learning capabilities that enable both continuous learning and high performance model execution\n\nMainstream adoption of flow itself will be five to ten years from now (2020). Flow will have to prove that \nit meets security criteria for everything from electronic payments, to health-care data, to classified\n information. The CNCF’s [CloudEvents](https://cloudevents.io/) specification, for instance, strongly suggests payloads be encrypted.\nThere is no single approach to defining an event with encryption explicitly supported \nthat can be read by any event-consuming application (MQTT, AMQP, have different encryption and TLS add more for \nTCP connection).\n\nConsumers need assurances that the data they receive in an event is valid and accurate, a practice \nknown as **data provenance**.  \n\n> Data provenance is defined as “a record of the inputs, entities, \nsystems, and processes that influence data of interest, providing a historical record of the \ndata and its origins\"\n\nProvenance has to maintained by the producer as a checksum number created by parsing the event data, and encrypted\nby the producer's key. CloudEvent has metadata about the message. When sent to Kafka they are \nimmutable record. Now the traceability of the consumers in kafka world is a major challenge.\nBlockchain may also be used to track immutable record with network parties attest its accuracy.\n\n\nApplying the concept of data loose value over time, it is important to act on data as early\nas possible, close to creation time. After a period of time data becomes less valuable.\n\nTwo time factors are important in this data processing: **latency** (time to deliver data to consumers)\nand **retention** (time to keep data). For latency try to reduce the number of network segment between\nproducer and consumers. Considering edge computing as a way to bring event processing close to the source.\nThe event processing add time to the end to end latency. Considering constraining the processing time frame.\n\n*Retention* is a problem linked to the business requirements, and we need to assess for each topic how long\nan event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state, \nprojection views... keeping for too long, increase the cost of storage, but also the time to rebuild data \nprojection. \n\nFinally, producers will want to trust that only authorized consumers are using the events they produce.\nAlso it may be possible to imagine a way to control the intellectual property of the data so producer can keep \nits ownership. Data consumption should be done via payment like we do with music subscription.\n\n## Flow patterns:\n\n### Collector pattern\n\nThe Collector pattern is a pattern in which a single consumer subscribes to topics from multiple producers. \n\n### Distributor pattern\n\nEach event in a stream is distributed to multiple consumers. It could be a hard problem to solve\nwhen doing it across geographically distributed systems. Edge computing can be used to distribute\n streaming endpoints closer to the consumers that need those streams. Alternate\n is to moving the event processing close to the source. For many Distributor use cases, \n partitioning the data by region is probably smart, and flow interfaces will need to take \n this into account.\n\n### Signal pattern\n\nThe Signal pattern is a general pattern that represents functions that exchange data between\n actors based on a distinct function or process, in can be seen as a traffic cop. \n It supports multiple producers and multiple consumers. The signal pattern is supported\n by multiple event processing each handling one aspect of the event processing.\n\nStream processing may route event streams between several distributed edge computing services as \nwell as core shared services, but then we need management layer to get global view of the systems.\nThey need to be integrated into observability tool. But the \"single pane of glass\" is often a lure\nas distributed systems require distributed decision-making. More local solutions are more agile, flexible\nand better address local problems for improved resilience.  \n\nOne of the challenge of complex adaptive systems is that any agent participating in \nthe system has difficulty seeing how the system as a whole operates,\nbecause of its limited connections to other neighbor agents.\n\n### Facilitator pattern\n\nA specialized form of Signal pattern, facilitator is a \"broker\" to match producers' events to\nconsumers' demands. It is like matching sellers with buyers.\n\n## 4- Identifying flow in your business\n\nThe classical usee case categories:\n\n* **Addressing and discovery**: In modern complex systems environments, multiple systems need to be informed of the new entity, be able to utilize to assign\nwork to it. Addressing and discovery happens across organizational boundaries (for example in real-time inventory SKU is used to identify item for both supplier and retailers).\nTo seek such use cases, look at tracking problems like who or what is involved in a problem domain that is difficult to scale.\nWith event stream centric approach, A&D is done via a registry service used by new agents to indicate their existence, and the service publishes an event to a topic\nto broadcast the information about the new agent. A second option is to use a discovery service to watch  \nspecific event stream for certain transmissions that indicate the presence of an agent. [Swim.ai](https://www.swim.ai/) continuously process and analyze \nstreaming data in concert with contextual data to inform business-critical, operational decisions. See also [SwimOS](https://github.com/swimos) or Apache Flink.\n\n* **Command and control**: sources are connected to key decision-making and action-taking services to complete a business task. So they are everywhere in any business.\nA typical example of such use case, is the Supervisory Control And Data Acquisition, used in manufacturing, or energy production.\nTry to ask: *where does the organization depend on timely responses to changes in state?*\nC&C can be supported by centralized control with events come from multiple sources to stateless or stateful services, to apply\nreal-time analysis and decision-making algorithms to those streams. Output events are published to sinks for future processing.\nScaling with a centralized control approach is not straightforward, as getting the right events to the right processing instances can be a challenge.\nAlso when we need the compute global aggregates by looking at the state of various agents in the systems is more complex,\nas it needs to integrate with stateful stream processing. Actions can be triggered by state changes, triggers that fire at specific times, or even API requests from other applications or services.\n\nAn alternate is to use distributed control, like applying the decision-making logic at the edge. \n\n* **Query and observability:** querying or monitoring individual agents or specific groups of agents. The problem is\nto locate the right agent target of the query, and get current state or history from that agent. \n* **Telemetry and analytics:** focuses on understanding systems behavior, and get real-time big data insights (e.g. Click streams). \nNeed to assess which insights require understanding the emerging behavior of a system of agents emitting vast amounts of data.\n\nInteresting presentations:\n\n* [Voxxed Athens 2018 - Eventing, Serverless, and the Extensible Enterprise by Clemens Vasters](https://www.youtube.com/watch?v=qCNXUUlhJJE)\n\n## 5- Model Flow\n\nUse Event storming to build a timeline of events that are required to complete a complex task, and to get\nand understanding of the people, systems, commands and policies that affect the event flow. \nThe Event Storming process is a highly interactive endeavor that :brings subject matter experts \ntogether to identify the flow of events in a process or system\n\n1. Identify the business activities that you would like to model in terms of event flow\n1. Begin by asking the group to identify the events that are interesting and/or required for that business activity\n1. Place events along a timeline from earliest action to latest action\n1. Capture:\n\n    * The real-world influences on the flow, such as the human users or external systems that produce or consume events\n    * What commands may initiate an event\n    * What policies are activated when an event takes place. A policy usually initiates a new command. Events always result in a policy action unless the event is generating output.\n    * What are the outputs from the event flow.\n\nWhen designing the solution assess:\n\n* When the event is simply to broadcast facts for consumption by interested parties. The producer contract is\n simply to promise to send events as soon as they are available.\n* If consumers can come and go, and experiment with the consumption of a stream with little risk of consequences if they choose not to continue\n* When event is part of an interaction around an intent, requiring a conversation with the consumer\n* Is the event a stand-alone communication, discrete, or is it only useful in a context that includes a series of events. \nSeries applications are where log-based queueing shines\n* Is the processing involve one simple action per event, or is there a group of related actions, a workflow, required to complete processing\n\nWhen building a flow-ready application for messaging, the “trunk-limb-branch-leaf” pattern is a \ncritical tool to consider: use edge computing to distribute decision-making close to the related \ngroups of agents, computing local aggregates, and propagate to larger more central flows. Using messaging\nmiddleware to manage interaction between agents, to isolate message distribution to just the needed\nservers and agents, and propagate aggregates to the trunk, greatly reducing traffic between the original agents and the core.\n\nAnother consideration is to assess if the consumers need to filter events from a unique topic before\ndoing its own processing, in this case the event payload may include metadata and URL to get the payload.\nIf the metadata indicates an action is required, the consumer can then call the data retrieval URL.\n\nWhether or not you include payload data depends a bit on the volume of events being published and the security and latency requirements of your application.\n\nLog-based queues can play the role of “system of record” for both event values and sequence, especially for systems that need both the most recent event and an understanding of the recent history of events received\n\nFor single action processing, serverless, knative eventing are technologies to consider. Solution\nneeds to route events to the appropriate processor. But if your event processing needs require \nmaintaining accurate state for the elements sending events then stateful streaming platform are better fit.\n\nFor workflow, modern solutions, simplify creating and managing process definitions independent of the actions taken in that process.\nIt supports for stepping an event through multiple interdependent actions. Workflow may require to\nwait for another related event occurs or a human completes his action.\n\n## 6- Today landscape\n\n\n* **Standards** are important for flow:  TLS, WebSockets, and HTTP from IETF, MQTT and AMQP from OASIS, \n*CloudEvents* and the Serverless Working Group from CNCF\n* **Open sources projects**: \n\n    * Apache Kafka and Apache Pulse for log-based queueing\n    * Apache Beam, Flink, Heron, Nifi, Samza, and Storm for stream processing\n    * Apache Druid as a “stream-native” database\n    * gRPC may play a key role in any future flow interface\n    * NATS.io, a cloud-native messaging platform\n    * Argo, a Kubernetes-based workflow manager that theoretically could act as the core of an event-driven process automation bus\n\n\n* Opportunities:\n\n    * Data provenance and security for payloads passed between disparate parties\n    * Tracking event data distribution across the world wide flow. Where does the data generated by an event end up being consumed or processed?\n    * Platforms for coordinating event processing, aggregation, and synchronization between core data center event processors, edge computing environments, and end-user or IoT devices\n    * Monetization mechanisms for all types of event and messaging streams\n\nThe adoption of a technology is not the delivery that makes it valuable, but the ecosystem that consumes it.\n\nLook at existing streams and determine how to add value for the consumers of that stream. \nCan you automate valuable insights and analytics in real time for customers with shared needs? \nWould it be possible to recast the stream in another format for an industry that is currently \nusing a different standard to consume that form of data? ","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/concepts/flow-architectures.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}