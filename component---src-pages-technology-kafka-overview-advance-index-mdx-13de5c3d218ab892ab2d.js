(window.webpackJsonp=window.webpackJsonp||[]).push([[90,73,123],{"013z":function(e,t,a){"use strict";var n=a("q1tI"),o=a.n(n),r=a("NmYn"),i=a.n(r),s=a("Wbzz"),l=a("Xrax"),c=a("k4MR"),d=a("TSYQ"),p=a.n(d),b=a("QH2O"),u=a.n(b),h=a("qKvR"),m=function(e){var t,a=e.title,n=e.theme,o=e.tabs,r=void 0===o?[]:o;return Object(h.b)("div",{className:p()(u.a.pageHeader,(t={},t[u.a.withTabs]=r.length,t[u.a.darkMode]="dark"===n,t))},Object(h.b)("div",{className:"bx--grid"},Object(h.b)("div",{className:"bx--row"},Object(h.b)("div",{className:"bx--col-lg-12"},Object(h.b)("h1",{id:"page-title",className:u.a.text},a)))))},g=a("BAC9"),f=function(e){var t=e.relativePagePath,a=e.repository,n=Object(s.useStaticQuery)("1364590287").site.siteMetadata.repository,o=a||n,r=o.baseUrl,i=o.subDirectory,l=r+"/edit/"+o.branch+i+"/src/pages"+t;return r?Object(h.b)("div",{className:"bx--row "+g.row},Object(h.b)("div",{className:"bx--col"},Object(h.b)("a",{className:g.link,href:l},"Edit this page on GitHub"))):null},y=a("FCXl"),k=a("dI71"),w=a("I8xM"),v=function(e){function t(){return e.apply(this,arguments)||this}return Object(k.a)(t,e),t.prototype.render=function(){var e=this.props,t=e.title,a=e.tabs,n=e.slug,o=n.split("/").filter(Boolean).slice(-1)[0],r=a.map((function(e){var t,a=i()(e,{lower:!0,strict:!0}),r=a===o,l=new RegExp(o+"/?(#.*)?$"),c=n.replace(l,a);return Object(h.b)("li",{key:e,className:p()((t={},t[w.selectedItem]=r,t),w.listItem)},Object(h.b)(s.Link,{className:w.link,to:""+c},e))}));return Object(h.b)("div",{className:w.tabsContainer},Object(h.b)("div",{className:"bx--grid"},Object(h.b)("div",{className:"bx--row"},Object(h.b)("div",{className:"bx--col-lg-12 bx--col-no-gutter"},Object(h.b)("nav",{"aria-label":t},Object(h.b)("ul",{className:w.list},r))))))},t}(o.a.Component),O=a("MjG9"),A=a("CzIb"),j=a("Asxa"),N=a("OIbQ"),x=a.n(N),T=function(e){var t=e.date,a=new Date(t);return t?Object(h.b)(j.c,{className:x.a.row},Object(h.b)(j.a,null,Object(h.b)("div",{className:x.a.text},"Page last updated: ",a.toLocaleDateString("en-GB",{day:"2-digit",year:"numeric",month:"long"})))):null};t.a=function(e){var t=e.pageContext,a=e.children,n=e.location,o=e.Title,r=t.frontmatter,d=void 0===r?{}:r,p=t.relativePagePath,b=t.titleType,u=d.tabs,g=d.title,k=d.theme,w=d.description,j=d.keywords,N=d.date,x=Object(A.a)().interiorTheme,I=Object(s.useStaticQuery)("2456312558").site.pathPrefix,z=I?n.pathname.replace(I,""):n.pathname,C=u?z.split("/").filter(Boolean).slice(-1)[0]||i()(u[0],{lower:!0}):"",K=k||x;return Object(h.b)(c.a,{tabs:u,homepage:!1,theme:K,pageTitle:g,pageDescription:w,pageKeywords:j,titleType:b},Object(h.b)(m,{title:o?Object(h.b)(o,null):g,label:"label",tabs:u,theme:K}),u&&Object(h.b)(v,{title:g,slug:z,tabs:u,currentTab:C}),Object(h.b)(O.a,{padded:!0},a,Object(h.b)(f,{relativePagePath:p}),Object(h.b)(T,{date:N})),Object(h.b)(y.a,{pageContext:t,location:n,slug:z,tabs:u,currentTab:C}),Object(h.b)(l.a,null))}},BAC9:function(e,t,a){e.exports={bxTextTruncateEnd:"EditLink-module--bx--text-truncate--end--2pqje",bxTextTruncateFront:"EditLink-module--bx--text-truncate--front--3_lIE",link:"EditLink-module--link--1qzW3",row:"EditLink-module--row--1B9Gk"}},I8xM:function(e,t,a){e.exports={bxTextTruncateEnd:"PageTabs-module--bx--text-truncate--end--267NA",bxTextTruncateFront:"PageTabs-module--bx--text-truncate--front--3xEQF",tabsContainer:"PageTabs-module--tabs-container--8N4k0",list:"PageTabs-module--list--3eFQc",listItem:"PageTabs-module--list-item--nUmtD",link:"PageTabs-module--link--1mDJ1",selectedItem:"PageTabs-module--selected-item--YPVr3"}},OIbQ:function(e,t,a){e.exports={bxTextTruncateEnd:"last-modified-date-module--bx--text-truncate--end--123zi",bxTextTruncateFront:"last-modified-date-module--bx--text-truncate--front--3xeKz",text:"last-modified-date-module--text--24m-4",row:"last-modified-date-module--row--2BquN"}},QH2O:function(e,t,a){e.exports={bxTextTruncateEnd:"PageHeader-module--bx--text-truncate--end--mZWeX",bxTextTruncateFront:"PageHeader-module--bx--text-truncate--front--3zvrI",pageHeader:"PageHeader-module--page-header--3hIan",darkMode:"PageHeader-module--dark-mode--hBrwL",withTabs:"PageHeader-module--with-tabs--3nKxA",text:"PageHeader-module--text--o9LFq"}},z4kH:function(e,t,a){"use strict";a.r(t),a.d(t,"_frontmatter",(function(){return s})),a.d(t,"default",(function(){return h}));var n=a("wx14"),o=a("zLVn"),r=(a("q1tI"),a("7ljp")),i=a("013z"),s=(a("qKvR"),{}),l=function(e){return function(t){return console.warn("Component "+e+" was not imported, exported, or provided by MDXProvider as global scope"),Object(r.b)("div",t)}},c=l("InlineNotification"),d=l("AnchorLinks"),p=l("AnchorLink"),b={_frontmatter:s},u=i.a;function h(e){var t=e.components,a=Object(o.a)(e,["components"]);return Object(r.b)(u,Object(n.a)({},b,a,{components:t,mdxType:"MDXLayout"}),Object(r.b)(c,{kind:"warning",mdxType:"InlineNotification"},Object(r.b)("strong",null,"Updated 10/13/2021")),Object(r.b)(d,{mdxType:"AnchorLinks"},Object(r.b)(p,{mdxType:"AnchorLink"},"High Availability"),Object(r.b)(p,{mdxType:"AnchorLink"},"Performance Considerations"),Object(r.b)(p,{mdxType:"AnchorLink"},"Disaster Recovery"),Object(r.b)(p,{mdxType:"AnchorLink"},"Solution Considerations")),Object(r.b)("h1",null,"High Availability"),Object(r.b)("p",null,"As a distributed cluster, Kafka brokers ensure high availability to process records.\nTopic has replication factor to support not loosing data in case of broker failure.\nYou need at least 3 brokers to ensure availability and a replication factor set to 3 for each\ntopic, so no data should be lost.\nBut for production deployment, it is strongly recommended to use 5 brokers cluster to ensure the quorum is always set,\neven when doing product upgrade. Replica factor can still be set to 3."),Object(r.b)("p",null,"The brokers need to run on separate physical machines, and when cluster extends over\nmultiple availability zones, a ",Object(r.b)("a",{parentName:"p",href:"https://strimzi.io/docs/operators/latest/using.html#type-Rack-reference"},"rack awareness"),"\nconfiguration can be defined. The ",Object(r.b)("strong",{parentName:"p"},"rack")," concept can represent an availability zone, data center,\nor an actual rack in your data center. Enabling rack awareness is done in a cluster definition (see Strimzi):"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  kafka:\n    rack:\n      topologyKey: topology.kubernetes.io/zone\n\n")),Object(r.b)("p",null,"For Zookeeper 3,5,7 nodes are needed depending of the number of objects to manage. You can\nstart with 3 and add nodes overtime."),Object(r.b)("h3",null,"Replication and partition leadership"),Object(r.b)("p",null,"Partition enables data locality, elasticity, scalability, high performance, parallelism,\nand fault tolerance. Each partition is replicated at least 3 times and allocated in different\nbrokers. One replicas is the ",Object(r.b)("strong",{parentName:"p"},"leader"),". In the case of broker failure (broker 1 in figure below),\none of the existing replica in the remaining running brokers will take the ",Object(r.b)("strong",{parentName:"p"},"leader")," role as soon\nas the broker is identified as not responding:"),Object(r.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1152px"}},"\n      ",Object(r.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"40.97222222222222%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABwklEQVQoz02STWsUQRCG9x+Kf0DIwT+g+BcEQ/Dixyl4EvSmXgTRBPHiQQJZFJHEsNnP2Z3Pnpmenu7pmX6s3TVoQdHV3dRbb9VbI8RCCHSux1tPmnVcLxw/ryzfLyxx0pGklk3c7jxJrNw70tT9jR1KdbjW47ue0RZw6AMqbagKxe+p5d6TioOHiruPSq7mDl0HygIqhSQHNlJkszFEUStuKJTfkWl1twcMQ0ArS5nVRCvDt/Oar2cV4x+VsOj5cHLG42evOXr6ik+fx2TpwGKmuPi1ZjmvyYSpKS22EcDGGIy26KIlT0sSaSuPG5K1JtvUVBU8f/GW2wcPuHXnPscv36PlLV0qppcR+bqmyJ2wc/8YDsKwyoV6WrFZG5aLRio3REu9a6vIelazksVUUeY9l9OWoxPL4ann8GPLYm1xtcOI7wB7P+xbVjW5VEtizeQ6FSBDknmKwjGZxMymKUriRex5c95w/CXj3diSFvsZboUZbRX+35oWtPFkSmPdQCGCNDZQlIaytvIvAulA8F5a14R+wNgtxh5ndLM2N96L4s4L6wFcF5BpkBWaVayYRxl142REYDuQLZFTcoY92Db/DxPbWL/Q+S65AAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(r.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"Replication and partition leadership",title:"Replication and partition leadership",src:"/refarch-eda/static/ef8599c41d43dc9578ff3e2c3719453d/3cbba/kafka-ha.png",srcSet:["/refarch-eda/static/ef8599c41d43dc9578ff3e2c3719453d/7fc1e/kafka-ha.png 288w","/refarch-eda/static/ef8599c41d43dc9578ff3e2c3719453d/a5df1/kafka-ha.png 576w","/refarch-eda/static/ef8599c41d43dc9578ff3e2c3719453d/3cbba/kafka-ha.png 1152w","/refarch-eda/static/ef8599c41d43dc9578ff3e2c3719453d/31986/kafka-ha.png 1338w"],sizes:"(max-width: 1152px) 100vw, 1152px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    "),Object(r.b)("p",null,"The record key in each record sent by producer, determines the partition allocation in ",Object(r.b)("strong",{parentName:"p"},"Kafka"),":\nthe records with the same key (hashcode of the key to be exact) will be in the same partition\n(It is possible to use more complex behavior via configuration and even code)."),Object(r.b)("p",null,"As Kafka is keeping its cluster states in ",Object(r.b)("a",{parentName:"p",href:"http://zookeeper.apache.org/"},"Apache Zookeeper"),", you also\nneed to have at least a three node cluster for zookeeper.\nFrom version 2.8, it is possible to do not use Zookeeper, but not yet for production deployment:\nthe brokers are using a Raft quorum algorithm, and an internal topic to keep state and metadata\n(See the ",Object(r.b)("a",{parentName:"p",href:"https://kafka.apache.org/documentation/#brokerconfigs_process.roles"},"process.roles  property")," from the broker configuration)."),Object(r.b)("p",null,"Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster topology itself.\nAssuming you are using the most recent Kafka version, it is possible to have a unique zookeeper cluster for multiple\nkafka clusters. But the latency between Kafka and Zookeeper needs to be under few milliseconds (< 15ms) anyway."),Object(r.b)("p",null,"Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated\non different racks and blades."),Object(r.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1152px"}},"\n      ",Object(r.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"56.59722222222222%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACkklEQVQoz1WT2U8aURTG/T/7F7Rpm/jQBx/60MZE+6LRVo0LCoobbtS9aWvc6gIj28jmVktcUAdhGGaGYYBfL0iTepIv3zk3ud/9Ts65TTQiW9BJ3qqk9SqKUeE2a5HO2Vw96qjFKjmziirwqNso+SIZrYRq2Pwf1WqVpt3jNB2zx3j2U7zt8vKi5TPtq5c4z1UGEin6AhoDh2UGpRoq9B9a9O3pfPll4gkXMe0qWsGmYFYwytAUu1RY2EryI3JF11KY9skdHNvXTEUVJo5vcfofGPdrTBwJSJrIVcF5Rn0qy5E8hbzF452OIdyaNUF/8p6hlQRLR1e8d6zz5pOT7u8ppi/yuJIpRsJ53MEy4yFboMzIocbwgcrgfh5vpECpCgXRvi7U6g4TfzIs7pzyU76hc1GmbWqPwe07JmMq7miaYSmDWzIYDwgc1dhkQuSjksmSbJDLWWTSOgWjIXh2rbIZumFDtNfS95XXrf10r58zfZ5hNHHGSDDDmN9gTIi5j0yGdhWGdu7p31aYD2SfOay3vBW6onXUx+z+BR2LIT6ObTKwk2ZCOHTJaZzikidi45FLgp8wI/KpsM1a3CKbLaI0HNYF/41cVXP4AiFCchSfJBEMhpEjUYEIJ/EYyUSchOB4LEqyxgKXlxf1uyWjhFVsDKVcqdQPlfs07SNzvOqZ52Wvl+H5dXo8qzQ7VmgeWsEh6g+uWr7Mu+FV3HPf2NjYIyumnFEsNBPE5tBUW8Za6AUNSU7gWPHjXJOQ40n8kTgDiz5coo4mT9j0H9O7cIB3K0QsmuD89Dcl4Uy5e0R5yGCVSk8tVxqiplWhbTxI54z89EixTOtYgB5vtF7XBtjce8DaYerZDzGLJpqmYds2fwFEsAHOXnnpfwAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(r.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"Dual network",title:"Dual network",src:"/refarch-eda/static/e00e16b21482eed07c17f1595ea6fab9/3cbba/ha-comp.png",srcSet:["/refarch-eda/static/e00e16b21482eed07c17f1595ea6fab9/7fc1e/ha-comp.png 288w","/refarch-eda/static/e00e16b21482eed07c17f1595ea6fab9/a5df1/ha-comp.png 576w","/refarch-eda/static/e00e16b21482eed07c17f1595ea6fab9/3cbba/ha-comp.png 1152w","/refarch-eda/static/e00e16b21482eed07c17f1595ea6fab9/3a43e/ha-comp.png 1262w"],sizes:"(max-width: 1152px) 100vw, 1152px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    "),Object(r.b)("p",null,"To contact the cluster, consumers and producers are using a list of bootstrap server names (also named ",Object(r.b)("inlineCode",{parentName:"p"},"advertiser.listeners"),").\nThe list is used for cluster discovery, it does not need to keep the full set of server names or IP addresses.\nA Kafka cluster has exactly one broker that acts as the controller."),Object(r.b)("p",null,"Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks.\nWith multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property\nto assign kafka broker using rack awareness. (See ",Object(r.b)("a",{parentName:"p",href:"https://kafka.apache.org/documentation/#brokerconfigs"},"the broker configuration")," from the product documentation)."),Object(r.b)("p",null,"As introduced on the ",Object(r.b)("a",{parentName:"p",href:"/refarch-eda/technology/kafka-overview/#topics"},"topic introduction section"),", data are replicated between brokers. The following diagram illustrates\nthe best case scenario where followers fetch data from the partition leader,\nand acknowledge the replications:"),Object(r.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"829px"}},"\n      ",Object(r.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"40.625%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABs0lEQVQoz12QT2/TQBDF/ZW5ceHKt+DQS7lwRAKhIAQCRFFaEyKVqkVp2rhu7PhPbK/Xu2tvfh07nFhpNPbbmffevuBs1fHhV82f2LDaWd6HFR8XFbPfNWFkeDcvmS1r5hvD2x85ny4blNZcLL9y9XjG7PKEzzen1CbHWU+wqy2rRLOOc27uYtaJYr3TRGlN2TpuY8V92pErxypquM86ijJncfGFZTzj1bdnvD5/QWW2+AECYwx3jwUHoLM9Xj6u44qs6TAe8rZnkxSCHzA9NFbTmJbx/M1/cvL9OW/Cl0KYMIyEtdKinE0D3nsBPddRSVJUyD1pZUgLNd1vM0VcVsTpljwraG1NpiJxHzP4HtcPBK73RLuGvnfozkxOb7cNTtg6YynFqRZnI54Umm1VoZ05Av+dI6FsRqJct4ZW3rQXx1erCNHBuJ6mcxghHyQL7Q4TZqzFiJhzDmuNEI3diqGO4JjNUS7fK/a1Ikl3HA5HbJDWj8GKJedHYmhbzWKxIAzDqebz8+l/3AvGxWleFsaMlNgZ+UcH1rrpGVaq/9etOHRSSZKyiR54iGOyvJB4zDT7BJ0yXeROvAhVAAAAAElFTkSuQmCC')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(r.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"topic replica seq",title:"topic replica seq",src:"/refarch-eda/static/6e849a31c60f7b84399bc9dec56cefe1/ebaef/topic-replica-seq.png",srcSet:["/refarch-eda/static/6e849a31c60f7b84399bc9dec56cefe1/7fc1e/topic-replica-seq.png 288w","/refarch-eda/static/6e849a31c60f7b84399bc9dec56cefe1/a5df1/topic-replica-seq.png 576w","/refarch-eda/static/6e849a31c60f7b84399bc9dec56cefe1/ebaef/topic-replica-seq.png 829w"],sizes:"(max-width: 829px) 100vw, 829px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    "),Object(r.b)("p",null,"Usually replicas is done in-sync, and the configuration settings specify the number of ",Object(r.b)("inlineCode",{parentName:"p"},"in-sync")," replicas needed:\nfor example, a replica 3 can have a minimum in-sync of 2,\nto tolerate 1 (= 3-2) out of sync replica (1 broker outage)."),Object(r.b)("p",null,"The leader maintains a set of in-sync-replicas (ISR) broker list: all the nodes which are up-to-date with the leader’s\nlog, and actively acknowledging new writes.\nEvery write goes through the leader and is propagated to every node in the In Sync Replica set."),Object(r.b)("p",null,"Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log.\nHaving the followers pull from the leader has the nice property of allowing the follower to naturally batch together\nlog entries they are applying to their log."),Object(r.b)("p",null,"Once all nodes in the ISR have acknowledged the request, the partition leader broker considers it committed,\nand can acknowledge to the producer."),Object(r.b)("p",null,"A message is considered committed when all in-sync replicas for that partition have applied this message to their log."),Object(r.b)("p",null,"If a leader fails, followers elect a new one. The partition leadership is dynamic and changes as servers come and go.\nApplications do not need to take specific actions to handle the change in the leadership of a partition.\nThe Kafka client library automatically reconnects to the new leader, although you will see\nincreased latency while the cluster settles: later  we will see this problem with partition rebalancing.\nAny replica in the ISR is eligible to be elected leader."),Object(r.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"794px"}},"\n      ",Object(r.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"46.18055555555556%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB4klEQVQoz3WRz27TQBDG86C8AFcOvAEXJM5cEI8AKOKChGhAJIektJHTOA1xnMSJmzqxG9tre+399zHehh6QGGns3Z3Z334z0xlvOH64BfyjIG/Qu8nx02UYzAu48wgXvRWGwwATP8WXqxSjVQVeV5jMRlglDgbeJzjhN9SCQwiFzu7UwAkYFuEJ/j7F1TLB1+Eci7sc0X2KiXOHpRcjPBS49ph9OD5GGPS7GO0+4u335/jgvII0HFoBHa0kgu0SRVWClTWMKjBdblFwgdaCJMcpO0JrDaklnShUvLaxX+FnvO+/QHf6BrUsoZRBp6xq3Hq3dEHCUJIWKS5nPmLWoL0+/r3BIVpagLdaoCpiNELb/am6R5jP4G5GJGpDDAIaoviBT/ULUkBAemm3Cuy6/dz4Oyrds4A1VcLLxAJNe/Efk5J6qIjqrxe0EU8BQdJhiCgaTL2A4nNUjYG39lCyGLxRFmgoR5/9CdjKnHk+UlYizjhqaiwjdi0fFUxJ4TZcIOcGeb6nkxz/MwtsF2lpIEnVNsrAuEJJChoKtmcPGQOvMtSNxjGJqL8bXIz3NPWMhiAtRKpHFy2QpSHKxG3HYV8xNE1Frs/e2vmHw6nCy3eXePa6j27PQfZA06dC/ua3bfgDMUunrcI3uy0AAAAASUVORK5CYII=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(r.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"topic replica fail",title:"topic replica fail",src:"/refarch-eda/static/52c56509429fcc28cdb34454e9838a95/d4b82/topic-replica-fail.png",srcSet:["/refarch-eda/static/52c56509429fcc28cdb34454e9838a95/7fc1e/topic-replica-fail.png 288w","/refarch-eda/static/52c56509429fcc28cdb34454e9838a95/a5df1/topic-replica-fail.png 576w","/refarch-eda/static/52c56509429fcc28cdb34454e9838a95/d4b82/topic-replica-fail.png 794w"],sizes:"(max-width: 794px) 100vw, 794px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    "),Object(r.b)("p",null,"When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost.\nBut there is a risk of having the single broker separated from the Zookeeper cluster when network partition occurs.\nTo tolerate f failures, both the majority\nvote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message."),Object(r.b)("p",null,"Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and will\nimpact throughput as data is sent 1+4 times over the network. Get acknowledgement will take a little bit more time too."),Object(r.b)("p",null,"Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact. \nKafka protocol by allowing a replica to rejoin the ISR, ensures that before rejoining, it must fully re-sync again even if\nit lost unflushed data in its crash."),Object(r.b)("p",null,"When a producer sends message, it can control how to get the response from the committed message:\nwait for all replicas to succeed, wait for one acknowledge, fire and forget.\nConsumers receive only committed messages."),Object(r.b)("p",null,"Always assess the latency requirements and consumers needs. Throughput is linked to the number\nof partitions within a topic and having more consumers running in parallel.\nConsumers and producers should better run on separate servers than the brokers nodes.\nRunning in parallel, also means the order of event arrivals will be lost.\nMost of the time, consumers are processing events from a unique partition and Kafka records\nto partition assignment will guarantee that records with the same key hashcode will\nbe in the same partition. So orders are preserved within a partition.\nBut if consumer needs to read from multiple partitions and if ordered records is needed,\nthe consumer needs to rebuild the order by implementing adhoc logic, based on time stamp."),Object(r.b)("p",null,"For high availability, assess any potential single point of failure, such as server,\nrack, network, power supply… We recommend reading\n",Object(r.b)("a",{parentName:"p",href:"https://ibm.github.io/event-streams/installing/planning/"},"this event stream article")," for planning your kafka on Kubernetes installation."),Object(r.b)("p",null,"For the consumers code maintenance, the re-creation of the consumer instance within the consumer\ngroup will trigger a partition rebalancing. This includes all the state of the aggregated data\ncalculations that were persisted on disk. Until this process is finished real-time events are\nnot processed. It is possible to limit this impact by setting the ",Object(r.b)("inlineCode",{parentName:"p"},"group.initial.rebalance.delay.ms"),"\nto delay the rebalancing process once one instance of the consumer dies.\nNevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer\ngroup. "),Object(r.b)("p",null,"When consumers are stream processing using Kafka streams, it is important to note that\nduring the rollover the downstream processing will see a lag in event arrival:\nthe time for the consumer to reread from the last committed offset. So if the end-to-end timing\nis becoming important, we need to setup a standby consumer group (group B).\nThis consumer group has different name, but does the same processing logic, and is consuming\nthe same events from the same topic as the active consumer group (group A).\nThe difference is that they do not send events to the downstream topic until they are set\nup active. So the process is to set group B active while cluster A is set inactive.\nThe downstream processing will not be impacted.\nFinally to be exhaustive, the control of the segment size for the change log topic, may be considered to\navoid having the stream processing doing a lot of computation to reload its state when it restarts."),Object(r.b)("p",null,"To add new broker, you can deploy the runtime to a new server / rack / blade, and give a\nunique broker ID. Broker will process new topic, but it is possible to use thr ",Object(r.b)("a",{parentName:"p",href:"https://access.redhat.com/documentation/en-us/red_hat_amq/7.4/html/using_amq_streams_on_red_hat_enterprise_linux_rhel/scaling_clusters"},"kafka-reassign-partitions ")," tool to migrate some existing\ntopic/ partitions to the new server. The tool is used to reassign partitions across brokers.\nAn ideal partition distribution would ensure even data load and partition sizes across all brokers."),Object(r.b)("h3",null,"High Availability in the context of Kubernetes deployment"),Object(r.b)("p",null,"The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve.\nKubernetes workloads prefer to be stateless,\nKafka is a stateful platform and manages its own brokers, and replications across known servers.\nIt knows the underlying infrastructure.\nIn Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker\ndirectly once they get the connection metadata.\nHaving a kubernetes service object which will round robin across all brokers in the cluster will not work with Kafka."),Object(r.b)("p",null,"The figure below illustrates a Kubernetes deployment, where Zookeeper and kafka brokers are allocated to 3 worker nodes,\nwith some event driven microservices deployed\nin separate worker nodes. Those microservices are consumers and producers of events from one\nto many topics."),Object(r.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1136px"}},"\n      ",Object(r.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"56.59722222222222%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACqklEQVQozz2RW28aVxCA+bOR2n/Qlyp5qtq0jRJFqpoordU+FBqb1oAJ2Aab2y7LAstlb9x2l5vBxkCIG2wMjb8eardHGh19mqOZOfP52sMFanNKxZlR8WbI5pBcrYtij3GHU+zOAG96Q2dyjeVd0HTP6M5XtAWb7jktbyT4lvb5R1a3G3xK/Qyp1KfqfaA5XbMvd+heLim0L5Grnsi1SasNEkmFuNxE0RwyBZvjE1Vwg7zgtGKSOC3yYbnGV3cnGO6YYmOA3Z8SUVzOV1By5pyWTILHEVLlBmV9xIFUIZI6RKo6lIwR4UyRg/QRedFY1YdcLVf49N4crbtArrvk6x32pRaV7py6u+C4nuNJ9DP8xd8p9noE5CMeRz5ntxxCFbyT2uPL8COClSiK57Ba3+HLFG1SksahpIuv6ISkNrLYn2yIqZ0We1oUv7JLpHrCiWnxZy1MQPUT1VMcG3X+qIbwF/zEjRy3a/CVG0M000FrDRhfTokXPbyzGYXmBe3BHNN5Lxa+wR6tqLYn6J33NMYbjMH1PTv/8fJeSr7eJavanJYdTmWd3UwLpz9BblxQtnoiZ6AY/X9FxHKmkNAQslxyxSZHeQv5gbNC3F83QkrR7COpFqo1ICMeHSiOsPyRvgjN6hJLZollFaLiDiZKJDN53qUkDjMFQidlEg8cTysslrdbKTPM/pzeYkN7tqHQGAN3bI9k63y1/5xn0Zf8kPiZH+P7fBd+yffR5+xkAuwkYzwNveDZuxf8lP6N65WYsGx4lOwBnemKzuUNvYur/wtqnRGBbI2gEPY2ZxBWbILbteR0cVtE1SZ7IreNt1mD4eQK33rzSej+mxtRfdthW+zT3X3BmDrgizca3wZqfO3XeR2xePJrjW/8IgI6rx74aaDO418qJLUh/wAkKfObKkkjjgAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(r.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"kubernetes deployment",title:"kubernetes deployment",src:"/refarch-eda/static/269dfdf6fa078e1e472bc39191e89eac/0fd57/k8s-deploy.png",srcSet:["/refarch-eda/static/269dfdf6fa078e1e472bc39191e89eac/7fc1e/k8s-deploy.png 288w","/refarch-eda/static/269dfdf6fa078e1e472bc39191e89eac/a5df1/k8s-deploy.png 576w","/refarch-eda/static/269dfdf6fa078e1e472bc39191e89eac/0fd57/k8s-deploy.png 1136w"],sizes:"(max-width: 1136px) 100vw, 1136px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    "),Object(r.b)("p",null,"The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of\nstateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration.\nServices add a logical name to access brokers for any deployed workload within the cluster.\nThe virtual network also enables transparent TLS communication between components."),Object(r.b)("p",null,"For any Kubernetes deployment real high availability is constrained by the application / workload\ndeployed on it. The Kubernetes platform supports high availability by having at least the following configuration:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm."),Object(r.b)("li",{parentName:"ul"},"At least three worker nodes, but with Zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have Zookeeper and Kafka brokers\nsharing the same host with other pods if the Kakfa traffic is supposed to grow."),Object(r.b)("li",{parentName:"ul"},"Externalize the management stack to three manager nodes"),Object(r.b)("li",{parentName:"ul"},"Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kakfa broker file systems)."),Object(r.b)("li",{parentName:"ul"},"Use ",Object(r.b)("inlineCode",{parentName:"li"},"etcd")," cluster: See recommendations ",Object(r.b)("a",{parentName:"li",href:"https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md"},"from this article"),".\nThe virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster.\nIt leverages ",Object(r.b)("inlineCode",{parentName:"li"},"etcd")," for storing information, so it is important that ",Object(r.b)("inlineCode",{parentName:"li"},"etcd")," is high available too and connected to low latency network below 10ms.")),Object(r.b)("p",null,"Traditionally disaster recovery and high availability were always consider separated subjects.\nNow active/active deployment where workloads are deployed in different data centers,\nis becoming a common IT’s request."),Object(r.b)("p",null,"For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running\nat least five nodes (you can tolerate the loss of one server during the planned maintenance of another server).\nOne Zookeeper server acts as a lead and the two others as stand-by."),Object(r.b)("p",null,"The diagram above illustrates a simple deployment where Zookeeper servers and Kafka brokers are running in pods, in different worker nodes.\nIt is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from ",Object(r.b)("strong",{parentName:"p"},"Kafka")," nodes\nto limit the risk of failover, as Zookeeper keeps state of the ",Object(r.b)("strong",{parentName:"p"},"Kafka")," cluster topology and metadata. You will limit to have both the Zookeeper leader and\none Kafka broker dying at the same time. We use Kubernetes ",Object(r.b)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity"},"anti-affinity"),"\nto ensure they are scheduled onto separate worker nodes that the ones used by Zookeeper. It uses the labels on pods with a rule like:\n",Object(r.b)("inlineCode",{parentName:"p"},"**Kafka** pod should not run on same node as zookeeper pods"),"."),Object(r.b)("p",null,"Here is an example of such spec:"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            labelSelector:\n            matchExpressions:\n            - key: name\n              operator: In\n              values:\n              - gc-zookeeper\n          topologyKey: kubernetes.io/hostname\n")),Object(r.b)("p",null,"We recommend reading the ",Object(r.b)("a",{parentName:"p",href:"https://kubernetes.io/docs/tutorials/stateful-application/zookeeper"},"“running zookeeper in k8s tutorial”")," for understanding such configuration."),Object(r.b)("p",null,"For optimum performance, provision a ",Object(r.b)("strong",{parentName:"p"},"fast storage class")," for persistence volume."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Kafka")," uses the ",Object(r.b)("inlineCode",{parentName:"p"},"log.dirs")," property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support ",Object(r.b)("inlineCode",{parentName:"p"},"log.dirs"),"."),Object(r.b)("p",null,"Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one ",Object(r.b)("strong",{parentName:"p"},"Kafka")," cluster only."),Object(r.b)("p",null,"In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster."),Object(r.b)("p",null,"For ",Object(r.b)("strong",{parentName:"p"},"Kafka")," streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers."),Object(r.b)("li",{parentName:"ul"},"communications between Zookeepers and cluster nodes are redundant and safe for data losses"),Object(r.b)("li",{parentName:"ul"},"consumers ensure idempotence… They have to tolerate data duplication and manage data integrity in their persistence layer.")),Object(r.b)("p",null,"Within Kafka’s boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Kafka")," configuration is an art and you need to tune the parameters by use case:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Partition replication for at least 3 replicas. Recall that in case of node failure,  coordination of partition re-assignments is provided with ZooKeeper."),Object(r.b)("li",{parentName:"ul"},"End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas."),Object(r.b)("li",{parentName:"ul"},"Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties."),Object(r.b)("li",{parentName:"ul"},"Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost."),Object(r.b)("li",{parentName:"ul"},"Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense."),Object(r.b)("li",{parentName:"ul"},"Control the maximum message size the server can receive.")),Object(r.b)("p",null,"Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot."),Object(r.b)("h1",null,"Performance Considerations"),Object(r.b)("p",null,"Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may be shared with other pods.\nIt is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput.",Object(r.b)("br",{parentName:"p"}),"\n","Performance will always depend on\nnumerous factors including message throughput, message size, hardware, configuration settings, …"),Object(r.b)("p",null,"Performance may be linked to different focuses:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Resilience: ensuring replication and not loosing data"),Object(r.b)("li",{parentName:"ul"},"Throughput: ensuring message processing performance"),Object(r.b)("li",{parentName:"ul"},"Payload size: support larger message")),Object(r.b)("h2",null,"Resilience"),Object(r.b)("p",null,"When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. (",Object(r.b)("inlineCode",{parentName:"p"},"min.insync.replicas"),")."),Object(r.b)("p",null,"The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network."),Object(r.b)("h2",null,"Throughput"),Object(r.b)("p",null,"To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures."),Object(r.b)("p",null,"The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code.\nWith 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second."),Object(r.b)("h2",null,"Payload size"),Object(r.b)("p",null,"From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg)."),Object(r.b)("p",null,"To do performance test the ",Object(r.b)("a",{parentName:"p",href:"https://github.com/IBM/event-streams-sample-producer"},"event-streams-sample-producer")," github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: ",Object(r.b)("inlineCode",{parentName:"p"},"ProducerPerformance.java")," in the jar:"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>org.apache.kafka</groupId>\n  <artifactId>kafka-tools</artifactId>\n</dependency>\n")),Object(r.b)("h2",null,"Parameter considerations"),Object(r.b)("p",null,"There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers\nthreading level (",Object(r.b)("inlineCode",{parentName:"p"},"num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads")," ) and the\npod resources constraints. See ",Object(r.b)("a",{parentName:"p",href:"https://Kafka.apache.org/documentation/#configuration"},"the configuration documentation"),"."),Object(r.b)("h2",null,"Openshift specifics"),Object(r.b)("p",null,"When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS\ncertificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman\nbetween Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by\nclient needs to be sized and in case of the router needs to be scaled up, and even isolate the routing\nby adding a separate router for the Kafka routes."),Object(r.b)("h1",null,"Disaster Recovery"),Object(r.b)("p",null,"With the current implementation it is recommended to have one cluster per data center / availability zone.\nConsumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the\ndata replicated in both data centers, you need to assess what kind of data can be aggregated, and if Kafka\nmirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces\nto a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved."),Object(r.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"1152px"}},"\n      ",Object(r.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"51.041666666666664%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACWUlEQVQoz02SW08TURSF+zd98UUTJeEX+GaMkahBCYrEhGgCAtqgFkESw7VIaKlcLExLoRc6hTLtTJne5tJ2znyetoieZM1Jzuy9svZeK+D7Pv/ADf4/esOh7rTR6w4tz7t5F90e+vXimiPQ/Qjho5U0isUCplnpFduSYCF6znzkHKPu4nqCqtVmKVYkuJnHqDnYjSpa8QJD17q0vb5Al0z4HntZjZV4gViuQqLkcGL4fN4ps3Vs0u54dGS93fZJFarMRzPs5q7QLI9CxaJmyymqLeyWR+Cv/GBU5fbD99x7PsfonsnIfoI3cZNjo/+/JSSupw2nytwfmpKYZjRa4q1yKntKRFVB4LRYIXOps5U2mdzIMRnO8fW4wWL6ilDKIpauoCSTJLMqbdHdl8dOrsbEaoZ3q2k+HlZZyNQIJhocXUqFalnuwazJ8fLcffSOweE5xg+qvNpXGD80WNq75PvyGuFfhz3C7lk+0hgYmmTw6TRjMYOJZIbXB0Vi51JhsqCjnF0QyVuElI6ETShh8yMrmE+22FQuWVz9yXrsiE6XUCrcVRt8iOjMRErM/q4TOm7KWyrUpEK37fUc+rR9xq0H49x5Miv3UWVkT2F4t8SJND2eUhkO7t/EZSVRZmD4C4MvQ7yIGIzFMzzbuWA7L/qmCF9It+rkioZ0scxZ1SGtXqHqLvFsg5mNPLNhFdfxcO0WiTODhfUDvklsyl3vZMvsy2SULPo59ITAajk4bZdcIY9lNfqxku9RpczjWYWptTyO7WE3XSpmvZdTS5I3601wfUxdxxcd/gC5JM/QMPdoLwAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(r.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"High availability cross data centers",title:"High availability cross data centers",src:"/refarch-eda/static/43765bc7d00d03c71420945abebacd51/3cbba/ha-dc1.png",srcSet:["/refarch-eda/static/43765bc7d00d03c71420945abebacd51/7fc1e/ha-dc1.png 288w","/refarch-eda/static/43765bc7d00d03c71420945abebacd51/a5df1/ha-dc1.png 576w","/refarch-eda/static/43765bc7d00d03c71420945abebacd51/3cbba/ha-dc1.png 1152w","/refarch-eda/static/43765bc7d00d03c71420945abebacd51/7ce68/ha-dc1.png 1188w"],sizes:"(max-width: 1152px) 100vw, 1152px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    "),Object(r.b)("p",null,"The above diagram is using Kafka MirrorMaker 2 with a master to slave deployment. Within the data center 2,\nthe brokers are here to manage the topics and events. When there is no consumer running, nothing happen.\nConsumers and producers can be started when DC1 fails. This is the active/passive model. In fact,\nwe could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory\ntheir projection view, as presented in the ",Object(r.b)("a",{parentName:"p",href:"../../patterns/cqrs/"},"CQRS pattern"),"."),Object(r.b)("p",null,"The second solution is to use one MirrorMaker 2 in each site, for each topic. This is an active - active\ntopology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention\nfor the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic."),Object(r.b)("span",{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"761px"}},"\n      ",Object(r.b)("span",{parentName:"span",className:"gatsby-resp-image-background-image",style:{paddingBottom:"50.34722222222222%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACSElEQVQoz0WSW08TYRCG+bVeqldeGRPCDQlCbOLhwnKwDQGCppIoiIlaTaFYKEJbCrVtSk+Lbbdb93zu49cuwUnm4su888zszjuDCNd1GY1GqKqGHwSMx0zTC0JUy8VwPBTDEbVwIsf3A4bDIbIsT3snMRYNk5yZPAzT4LpRF6LBpHSbkchyfUw3wPYCQvEOwwDT0FEUmcD3hCi8005iChwZJkeVDie1HpWeQUcNaCkemhOJnWA8HWG5Dt2hRkvWkEYaYha2WND1wykwDENmzuoSpY7CbCLNvbllFg8uSF7/4VXOoiAFU6DtE4EFfzF1ysOlbeb36sQvJJ7lJI5akW66oef5yLrNarrB8w8Fkvk+qZrGVskiW5EpX5apd3q0xVYtsd165ppY6oSVwxu2KwPWSwq5pk2r2aTZ7kafrDo+cxsHPFjYIvalwZtal0TFYi8v8X53n1ypRrOv0pB1XuzkeLSUJPa5yobQrRY6fCsO2U9n+Hp4EgH/2h6xnTyzr/eIf2+TaphsFE1+XPTJHucpN7pUJYXfkszOucpapkfieMS7K5v1U430+YBMLk/2tHgLFNZ4uVtkXvzH5WybzVqLzYpBZRBd3BZHMcRQ2fR4+jbL/fkVFj5ViV+2iOVbHNZcwsDFdtwIaAkvOeJKnuhXFBNL9TFGLpbm4hoevb7FbLLI47VfnF3d8LPYJVeW6Pd1dFE3de//UcKpJcYEwtCe8JVtCrhIS4+Atj4xvUN/YGFoDqXqkCeJAvGPVRwB88TWvuPfefEfKSnZG6e+FsAAAAAASUVORK5CYII=')",backgroundSize:"cover",display:"block"}}),"\n  ",Object(r.b)("img",{parentName:"span",className:"gatsby-resp-image-image",alt:"High availability cross data centers",title:"High availability cross data centers",src:"/refarch-eda/static/715fdb2c82679231921d4960e51cbbfd/05d44/ha-dc2.png",srcSet:["/refarch-eda/static/715fdb2c82679231921d4960e51cbbfd/7fc1e/ha-dc2.png 288w","/refarch-eda/static/715fdb2c82679231921d4960e51cbbfd/a5df1/ha-dc2.png 576w","/refarch-eda/static/715fdb2c82679231921d4960e51cbbfd/05d44/ha-dc2.png 761w"],sizes:"(max-width: 761px) 100vw, 761px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"}),"\n    "),Object(r.b)("p",null,"When you want to deploy solution that spreads over multiple regions to support global streaming, you need\nto address the following challenges:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"How do you make data available to applications across multiple data centers?"),Object(r.b)("li",{parentName:"ul"},"How to serve data closer to the geography?"),Object(r.b)("li",{parentName:"ul"},"How to be compliant on regulations, like GDPR?"),Object(r.b)("li",{parentName:"ul"},"How to address no duplication of records?")),Object(r.b)("p",null,Object(r.b)("a",{parentName:"p",href:"https://www.confluent.io/blog/apache-kafka-2-4-latest-version-updates/"},"Kafka 2.4")," introduces the capability\nfor a consumer to read messages from the closest replica using some rack-id and specific algorithm.\nThis capability will help to extend the cluster to multiple data center and avoid having consumers going over\nWAN communication."),Object(r.b)("h1",null,"Solution Considerations"),Object(r.b)("p",null,"There are a set of design considerations to assess for each ",Object(r.b)("strong",{parentName:"p"},"Kafka")," solution:"),Object(r.b)("h2",null,"Topics"),Object(r.b)("p",null,"Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node."),Object(r.b)("p",null,"What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types?\nLet define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic."),Object(r.b)("p",null,"The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions."),Object(r.b)("p",null,"When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together."),Object(r.b)("p",null,"Other best practices:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"When event order is important use the same topic and use the entity unique identifier as partition key."),Object(r.b)("li",{parentName:"ul"},"When two entities are related together by containment relationship then they can be in the same topic."),Object(r.b)("li",{parentName:"ul"},"Different entities are separated to different topics."),Object(r.b)("li",{parentName:"ul"},"It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics."),Object(r.b)("li",{parentName:"ul"},"Clearly define the partition key as it could be an compound key based on multiple entities.")),Object(r.b)("p",null,"With ",Object(r.b)("strong",{parentName:"p"},"Kafka")," stream, state store or KTable, you should separate the changelog topic from the others."),Object(r.b)("h2",null,"Producers"),Object(r.b)("p",null,"When developing a record producer you need to assess the following:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size."),Object(r.b)("li",{parentName:"ul"},"Can the producer batch events together to send them in batch over one send operation?"),Object(r.b)("li",{parentName:"ul"},"Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size"),Object(r.b)("li",{parentName:"ul"},"Assess ",Object(r.b)("em",{parentName:"li"},"once to exactly once")," delivery requirement. Look at idempotent producer.")),Object(r.b)("p",null,"See ",Object(r.b)("a",{parentName:"p",href:"../../kafka-producers/"},"implementation considerations discussion")),Object(r.b)("h2",null,"Consumers"),Object(r.b)("p",null,"From the consumer point of view a set of items need to be addressed during design phase:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Do you need to group consumers for parallel consumption of events?"),Object(r.b)("li",{parentName:"ul"},"What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?."),Object(r.b)("li",{parentName:"ul"},"How to persist consumer committed position? (the last offset that has been stored securely)"),Object(r.b)("li",{parentName:"ul"},"Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code."),Object(r.b)("li",{parentName:"ul"},"Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records?"),Object(r.b)("li",{parentName:"ul"},"Do the consumer needs to perform joins, aggregations between multiple partitions?")),Object(r.b)("p",null,"See ",Object(r.b)("a",{parentName:"p",href:"../../kafka-consumers/"},"implementation considerations discussion")))}h.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-technology-kafka-overview-advance-index-mdx-13de5c3d218ab892ab2d.js.map